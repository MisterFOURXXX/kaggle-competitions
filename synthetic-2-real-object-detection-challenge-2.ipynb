{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99173,"databundleVersionId":11843845,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition Overview","metadata":{}},{"cell_type":"markdown","source":"**Overview: The Synthetic-to-Real Object Detection Challenge 2**\n\nThis challenge, a follow-up to the first competition, tasks participants with training an object detection model using exclusively synthetic data and applying it to real-world images. The goal is to detect soup cans in previously unseen photos, an exercise that simulates real-world AI applications where models trained in controlled, virtual environments must perform in unpredictable, physical ones.","metadata":{}},{"cell_type":"markdown","source":"**Challenge, Objective, and Process**\n\nThe primary objective is to achieve the highest possible Mean Average Precision at IoU 0.5 (mAP@50) score. Participants will train a model using synthetic images of soup cans generated by Falcon, a digital twin simulation software. The trained model will then be used to predict the bounding boxes of soup cans in a set of real-world test images.\n\nParticipants are provided with a starter dataset on Kaggle but can significantly boost their performance by leveraging additional datasets available on FalconCloud or by generating their own custom synthetic data using the Falcon Editor tool. The competition encourages participants to manipulate and combine these datasets, exploring the benefits of synthetic data to close the \"Sim2Real gap\"—the discrepancy in performance between models trained on synthetic data and those deployed in reality.","metadata":{}},{"cell_type":"markdown","source":"**Submission and Evaluation**\n\nSubmissions must be a single .csv file with predictions in YOLO format. Each row should contain the image_id and a prediction_string that includes the class, confidence, and normalized coordinates of the detected bounding box.\n\nThe final score is determined by mAP@50, which measures the accuracy of the predicted bounding boxes against the ground truth labels. A detection is considered correct if its Intersection over Union (IoU) with the true bounding box is at least 0.5. The competition features a public leaderboard for real-time ranking and a private leaderboard for final scoring. To maintain competition integrity, top performers are subject to a verification check.","metadata":{}},{"cell_type":"markdown","source":"**Getting Started and Improving Your Score**\n\nParticipants can choose from three paths to improve their performance:\n\n- **Use Provided Data:** Train and tune a model using only the data available on Kaggle.\n- **Use Supplemental Datasets:** Download additional, more comprehensive datasets from the Falcon website to fine-tune the model.\n- **Generate Your Own Data:** For the most significant performance boost, participants are highly encouraged to use the Falcon Editor to create custom synthetic datasets that target specific edge cases, such as varied lighting, occlusions, and camera viewpoints.\n\nParticipants can join the Discord community for support and can explore tutorials and documentation provided by Duality AI to learn how to use the Falcon tools effectively.","metadata":{}},{"cell_type":"markdown","source":"**Participants Benefits**\n\n- **Prizes:** Cash awards, digital certificates, and public recognition on platforms like LinkedIn and Hugging Face.\n- **Skill Development:** Hands-on experience with synthetic data, a skill highly valued in AI fields like robotics, industrial automation, and computer vision.\n- **Portfolio Enhancement:** A chance to showcase real-world AI and machine learning skills to recruiters and peers.\n- **Free Access to Falcon:** Gain hands-on experience with a professional synthetic data generation tool.","metadata":{}},{"cell_type":"markdown","source":"### [Competition Link](https://www.kaggle.com/competitions/synthetic-2-real-object-detection-challenge-2/overview)","metadata":{}},{"cell_type":"markdown","source":"## Data Description","metadata":{}},{"cell_type":"markdown","source":"The Synthetic-to-Real Object Detection Challenge 2 provides a dataset for training and validating an object detection model exclusively on synthetic images and then testing its performance on real-world images. The dataset is organized into three main directories: train, val, and test, each containing an images and a labels subdirectory.","metadata":{}},{"cell_type":"markdown","source":"<pre>\n/data\n    /train\n        /images/ # Synthetic images\n        /labels/ # YOLO format annotations\n    /val\n        /images/ # Synthetic validation images\n        /labels/ # YOLO format annotations\n    /test\n        /images/ # Real-world images (ones you'll be judged on)\n</pre>","metadata":{}},{"cell_type":"markdown","source":"The training and validation data consist of synthetic images, all with a resolution of 1920x1080 pixels. The corresponding labels are in YOLO format, stored in separate .txt files. These files contain normalized coordinates (x_center, y_center, width, height) and a class_id, which is always 0 for the single object class, \"soup can.\"\n\nThe test data consists of real-world images, also 1920x1080 pixels, for which participants must predict the bounding boxes. A crucial note is that external data is allowed only if it is generated using the Falcon tool, which is provided for free to participants. The competition encourages participants to create their own custom datasets with varied environments, lighting, and occlusions to improve model generalization and performance on the real-world test set.","metadata":{}},{"cell_type":"markdown","source":"## Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"This data dictionary describes the file structure and format of the datasets provided for the competition.","metadata":{}},{"cell_type":"markdown","source":"**Main Datasets**","metadata":{}},{"cell_type":"markdown","source":"<pre>\n/data\n    /train\n        /images/ # Synthetic images\n        /labels/ # YOLO format annotations\n    /val\n        /images/ # Synthetic validation images\n        /labels/ # YOLO format annotations\n    /test\n        /images/ # Real-world images (ones you'll be judged on)\n</pre>","metadata":{}},{"cell_type":"markdown","source":"**Label File (.txt) Data Dictionary**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Field Name</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">class_id</td>\n    <td class=\"tg-7zrl\">int</td>\n    <td class=\"tg-0lax\">The object class ID. In this competition, it is always 0, representing a soup can.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">x_center</td>\n    <td class=\"tg-7zrl\">float</td>\n    <td class=\"tg-0lax\">The normalized x-coordinate of the center of the bounding box (from 0 to 1).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">y_center</td>\n    <td class=\"tg-7zrl\">float</td>\n    <td class=\"tg-0lax\">The normalized y-coordinate of the center of the bounding box (from 0 to 1).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">width</td>\n    <td class=\"tg-7zrl\">float</td>\n    <td class=\"tg-0lax\">The normalized width of the bounding box (from 0 to 1).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">height</td>\n    <td class=\"tg-7zrl\">float</td>\n    <td class=\"tg-0lax\">The normalized height of the bounding box (from 0 to 1).</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**Submission File (.csv) Data Dictionary**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column Name</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">image_id</td>\n    <td class=\"tg-7zrl\">object</td>\n    <td class=\"tg-0lax\">The filename of the test image without the extension (e.g., 0002).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">prediction_string</td>\n    <td class=\"tg-7zrl\">object</td>\n    <td class=\"tg-0lax\">A string containing all the predicted bounding boxes for an image. <br>Each prediction is a space-separated sequence: class confidence x_center y_center width height.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"## Addional Integration","metadata":{}},{"cell_type":"markdown","source":"**What is Falcon?**","metadata":{}},{"cell_type":"markdown","source":"Falcon is a digital twin simulation software developed by Duality AI. Its purpose is to generate synthetic (computer-generated) images for training object detection models. Falcon is the tool used to create the synthetic data of soup cans. Participants can use the provided datasets generated by Falcon or create their own custom datasets using the Falcon Editor to improve their model's ability to perform in real-world scenarios.\n\nFalcon's purpose is to help users generate high-quality training data that can bridge the \"Sim2Real gap\", which is the difference in performance between models trained on simulated data and their performance in real-world scenarios. The tools are a key resource for participants to improve their model's ability to generalize.","metadata":{}},{"cell_type":"markdown","source":"There are two key parts of the Falcon ecosystem:\n\n- **Falcon (the core software):** This is described as a digital twin simulation software used to generate the synthetic soup can images for the competition's training and validation datasets. It creates a controlled virtual environment where data can be generated easily and at scale.\n- **Falcon Editor:** This is a more advanced tool that allows users to create their own unique supplemental datasets. It enables the user to customize scene parameters such as lighting, occlusions, camera angles, and backgrounds to simulate complex real-world scenarios and create targeted data to improve model performance on \"edge cases\" or difficult situations.\n\nThe competition encourages participants to use these tools to bridge the \"Sim2Real gap,\" which refers to the challenge of a model generalizing from a virtual, simulated environment to the unpredictable complexity of the real world.","metadata":{}},{"cell_type":"markdown","source":"**Albumentations**","metadata":{}},{"cell_type":"markdown","source":"Albumentations is a Python library for fast and flexible image augmentations that can be used for deep learning models, particularly in computer vision tasks. It's designed to be highly performant and offers a wide range of transformation techniques to artificially increase the size and diversity of a training dataset.","metadata":{}},{"cell_type":"markdown","source":"**Key Features and Purpose**\n\n- **Data Augmentation:** Its primary function is to apply various transformations to images, such as resizing, flipping, rotating, and adjusting brightness or contrast. This helps a model generalize better to unseen data and become more robust to real-world variations.\n- **Performance:** The library is optimized for speed, which is crucial for training large-scale deep learning models efficiently.\n- **Flexibility:** It can handle various data types beyond just images, including bounding boxes, segmentation masks, and keypoints, ensuring that all annotations are correctly transformed along with the image. This is particularly useful in object detection and semantic segmentation tasks.\n\n**Why It's Used in Deep Learning**\n\nIn deep learning, models can easily overfit to the training data. Data augmentation with Albumentations helps prevent this by creating new, slightly modified versions of existing images. This process makes the model less sensitive to specific features of the training set and more capable of recognizing objects under different conditions, a concept known as regularization.\n\n**Albumentations Integration**\n\nAlbumentations is integrated into the code to perform data augmentation for the Faster R-CNN model. It's used to apply various image transformations to the training and validation datasets, which helps the model generalize better.","metadata":{}},{"cell_type":"markdown","source":"### [Insapiration Notebook](https://www.kaggle.com/code/mohanapavanbezawada/synthetic-2-real-object-detection)","metadata":{}},{"cell_type":"markdown","source":"# Pipeline Overview","metadata":{}},{"cell_type":"markdown","source":"**Pipeline Overview and Theoretical Foundation**\n\nThis pipeline implements a robust object detection pipeline for the Synthetic-to-Real Object Detection Challenge. The core strategy is to train two distinct deep learning models—YOLOv8 and Faster R-CNN—on synthetic data and then combine their predictions on real-world test images using an ensembling technique called Weighted Boxes Fusion (WBF). The final, fused predictions are then formatted for submission.\n\nThis approach is designed to improve overall accuracy and generalization, a common strategy in machine learning to mitigate the weaknesses of a single model by leveraging the strengths of multiple models.\n\n**How the Pipeline Works?**\n\nThe pipeline is structured in four main phases: data preparation, model training, prediction, and ensembling.\n\n**1. Data Preparation and Configuration**\n\n- **Data Paths and data.yaml:** The code first defines the file paths for the synthetic training, validation, and real-world test data. It creates a data.yaml file, a configuration standard for YOLOv8, which specifies the data locations and the single class name ('object').\n- **Custom Dataset:** A SoupCanDataset class is created to handle the data for the Faster R-CNN model. It reads images and their corresponding YOLO-formatted labels, converting the normalized coordinates into a format (Pascal VOC) compatible with Faster R-CNN.\n- **Data Augmentation:** The code uses the Albumentations library to apply data augmentation techniques like horizontal flipping, resizing, and color jitter to the training data. This process helps the models generalize better to unseen variations in the real-world test images.\n\n**2. Model Training**\n\n- **YOLOv8 Training:** The pipeline trains a YOLOv8x model, which is pre-trained on a large dataset like COCO. It's fine-tuned on the provided synthetic training data for a specified number of epochs. The training process uses a cosine learning rate scheduler to smoothly decrease the learning rate over time and an SGD optimizer.\n- **Faster R-CNN Training:** Simultaneously, a Faster R-CNN model with a ResNet50-FPN backbone is loaded. This model is also pre-trained, and its final layer is replaced to adapt to the single-object \"soup can\" class. It's trained on the same data with a similar setup to the YOLOv8 model, including early stopping to prevent overfitting.\n\n**3. Prediction and Ensembling**\n\n- **Inference:** After training, both the YOLOv8 and Faster R-CNN models are loaded. They are then used to predict bounding boxes and confidence scores on the real-world test images.\n- **Weighted Boxes Fusion (WBF):** This is the crucial step where the predictions from the two models are combined. The code collects all bounding boxes from both models for each image. It assigns weights to each model's predictions (1.0 for YOLOv8 and 0.8 for Faster R-CNN, indicating a slightly higher trust in YOLOv8's predictions). WBF then merges overlapping boxes from different models into a single, refined bounding box, using a weighted average of their coordinates and confidence scores. This ensemble method is more effective than traditional methods like Non-Maximum Suppression (NMS) at preserving all true positives.\n- **Final Output:** The fused bounding boxes are then converted back into the YOLO format (class confidence x_center y_center width height) and saved to a .txt file for each image.\n\n**4. Submission**\n\n- **CSV Conversion:** Finally, the script iterates through all the prediction .txt files and compiles them into the required submission.csv format, including a \"no boxes\" entry for any images without detections.","metadata":{}},{"cell_type":"markdown","source":"**Theoretical Background**\n\n**Object Detection Models: YOLO and Faster R-CNN**\n\nBoth YOLO (You Only Look Once) and Faster R-CNN are seminal architectures in object detection, but they approach the problem differently.\n\nYOLOv8 is a one-stage detector. It treats object detection as a single regression problem, simultaneously predicting bounding boxes and class probabilities directly from an image. This makes it incredibly fast and efficient, ideal for real-time applications.\n\nFaster R-CNN is a two-stage detector. The first stage, the Region Proposal Network (RPN), proposes potential object regions. The second stage then classifies these proposals and refines their bounding boxes. This multi-step process makes Faster R-CNN more accurate, though generally slower, than single-stage models.\n\n**The Ensemble Strategy:** \n\nBy training both a fast, efficient model (YOLOv8) and a slower, more accurate model (Faster R-CNN), the pipeline aims to combine their strengths. YOLOv8 might capture objects missed by Faster R-CNN, and Faster R-CNN might provide more precise bounding boxes. The ensemble of these models often leads to better performance than either model could achieve alone.\n\n**Weighted Boxes Fusion (WBF)**\n\nWBF is an advanced ensembling technique that addresses a key limitation of standard methods like NMS.\n\n- **The Problem with NMS:** Non-Maximum Suppression (NMS) is a post-processing algorithm that suppresses duplicate bounding boxes. It selects the box with the highest confidence and discards other boxes that significantly overlap with it. However, if multiple high-confidence models predict slightly different, but correct, boxes, NMS might discard valuable predictions.\n\n- **The Solution of WBF:** WBF is an alternative to NMS. Instead of eliminating boxes, it clusters and fuses them. It calculates a weighted average of the coordinates and confidence scores of all overlapping boxes to create a single, more refined bounding box. This approach is more robust because it leverages consensus among models, and it can even improve the accuracy of the final bounding box. The weights assigned to each model's predictions allow you to specify which model's output you trust more. This is particularly useful in this pipeline, where you might have a hypothesis about one model's performance over the other.","metadata":{}},{"cell_type":"markdown","source":"# Install necessary packages and import necessary libraries","metadata":{}},{"cell_type":"code","source":"# Example: Download YOLOv8 weights (adjust for YOLOv11 if available)\n!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt -O /kaggle/working/yolo8x.pt\n!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11x.pt -O /kaggle/working/yolo11x.pt\n\n# Install necessary packages\n!pip install ultralytics\n!pip install ensemble_boxes\n!pip install -U ultralytics\n!pip install optuna\n\n# Cache clearing\n!pip install --no-cache-dir torch torchvision\n!pip install --no-cache-dir ultralytics\n!pip install --no-cache-dir albumentations\n!pip install --no-cache-dir ensemble-boxes\n!pip install --no-cache-dir pycocotools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:21:39.739264Z","iopub.execute_input":"2025-09-16T11:21:39.739843Z","iopub.status.idle":"2025-09-16T11:23:35.605078Z","shell.execute_reply.started":"2025-09-16T11:21:39.739815Z","shell.execute_reply":"2025-09-16T11:23:35.604117Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--2025-09-16 11:21:39--  https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://release-assets.githubusercontent.com/github-production-release-asset/521807533/c13b916e-8b1f-47ab-a613-7022adfa73c6?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-16T12%3A19%3A47Z&rscd=attachment%3B+filename%3Dyolov8x.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-16T11%3A19%3A00Z&ske=2025-09-16T12%3A19%3A47Z&sks=b&skv=2018-11-09&sig=bqvZ9pWDIKXvbp8RJiSD2Ci50iwKZ0HEJ0MFExh4y2M%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1ODAyMjAwMCwibmJmIjoxNzU4MDIxNzAwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.Gd9dhOY-xkD8MfLIcnqXnDjKf83yphYrcHY7tCFBw2E&response-content-disposition=attachment%3B%20filename%3Dyolov8x.pt&response-content-type=application%2Foctet-stream [following]\n--2025-09-16 11:21:40--  https://release-assets.githubusercontent.com/github-production-release-asset/521807533/c13b916e-8b1f-47ab-a613-7022adfa73c6?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-16T12%3A19%3A47Z&rscd=attachment%3B+filename%3Dyolov8x.pt&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-16T11%3A19%3A00Z&ske=2025-09-16T12%3A19%3A47Z&sks=b&skv=2018-11-09&sig=bqvZ9pWDIKXvbp8RJiSD2Ci50iwKZ0HEJ0MFExh4y2M%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1ODAyMjAwMCwibmJmIjoxNzU4MDIxNzAwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.Gd9dhOY-xkD8MfLIcnqXnDjKf83yphYrcHY7tCFBw2E&response-content-disposition=attachment%3B%20filename%3Dyolov8x.pt&response-content-type=application%2Foctet-stream\nResolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 136867539 (131M) [application/octet-stream]\nSaving to: ‘/kaggle/working/yolo8x.pt’\n\n/kaggle/working/yol 100%[===================>] 130.53M   222MB/s    in 0.6s    \n\n2025-09-16 11:21:40 (222 MB/s) - ‘/kaggle/working/yolo8x.pt’ saved [136867539/136867539]\n\n--2025-09-16 11:21:40--  https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov11x.pt\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2025-09-16 11:21:41 ERROR 404: Not Found.\n\nCollecting ultralytics\n  Downloading ultralytics-8.3.200-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.21.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.200-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.200 ultralytics-thop-2.0.17\nCollecting ensemble_boxes\n  Downloading ensemble_boxes-1.0.9-py3-none-any.whl.metadata (728 bytes)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ensemble_boxes) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ensemble_boxes) (2.2.3)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from ensemble_boxes) (0.60.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->ensemble_boxes) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble_boxes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble_boxes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble_boxes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble_boxes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble_boxes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble_boxes) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ensemble_boxes) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ensemble_boxes) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ensemble_boxes) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ensemble_boxes) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ensemble_boxes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ensemble_boxes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->ensemble_boxes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->ensemble_boxes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->ensemble_boxes) (2024.2.0)\nDownloading ensemble_boxes-1.0.9-py3-none-any.whl (23 kB)\nInstalling collected packages: ensemble_boxes\nSuccessfully installed ensemble_boxes-1.0.9\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.200)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.21.0)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.17)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.200)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.21.0)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.17)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\nRequirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\nRequirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\nRequirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.4.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.14.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\nRequirement already satisfied: ensemble-boxes in /usr/local/lib/python3.11/dist-packages (1.0.9)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ensemble-boxes) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ensemble-boxes) (2.2.3)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from ensemble-boxes) (0.60.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->ensemble-boxes) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble-boxes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble-boxes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble-boxes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble-boxes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble-boxes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->ensemble-boxes) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ensemble-boxes) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ensemble-boxes) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ensemble-boxes) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ensemble-boxes) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ensemble-boxes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->ensemble-boxes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->ensemble-boxes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->ensemble-boxes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->ensemble-boxes) (2024.2.0)\nRequirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom pathlib import Path\nimport csv\nimport os\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torch.utils.data import Dataset, DataLoader\nimport csv\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom ultralytics import YOLO\nfrom ensemble_boxes import weighted_boxes_fusion\nfrom tqdm import tqdm\nimport warnings\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom ultralytics import YOLO\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.ops import box_iou, nms\nfrom tqdm import tqdm\nimport torchvision.transforms.v2 as T\nimport pandas as pd\nimport csv\nfrom ensemble_boxes import weighted_boxes_fusion\nfrom torchvision.ops import box_iou, nms\nimport optuna\n\nwarnings.filterwarnings('ignore')\n\n# Verify installations\ntry:\n    from ultralytics import YOLO\n    print(\"[notice] All dependencies installed successfully\")\n    print(f\"[notice] PyTorch version: {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\nexcept ImportError as e:\n    print(f\"[error] Failed to import dependencies: {e}\")\n    print(\"[error] Please restart the kernel and re-run the script.\")\n    raise\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:24:33.541101Z","iopub.execute_input":"2025-09-16T11:24:33.541339Z","iopub.status.idle":"2025-09-16T11:24:33.550095Z","shell.execute_reply.started":"2025-09-16T11:24:33.541314Z","shell.execute_reply":"2025-09-16T11:24:33.549437Z"}},"outputs":[{"name":"stdout","text":"[notice] All dependencies installed successfully\n[notice] PyTorch version: 2.6.0+cu124, CUDA available: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"**Theoretical Concepts**\n\n**Data Augmentation**\n\nData augmentation is a powerful regularization technique used to increase the diversity of a dataset without collecting new data. By applying various transformations to the training images, the model is exposed to a wider range of scenarios (different lighting, rotations, sizes). This helps the model become more robust and less likely to overfit to the specific examples in the original dataset. For a Sim2Real challenge, data augmentation is critical for bridging the gap between clean synthetic data and messy real-world images. It simulates the noise and variability of the real world, forcing the model to learn more generalizable features.\n\n**Normalization**\n\nNormalization is a preprocessing step where pixel values are scaled to a standard range, typically a mean of 0 and a standard deviation of 1.  This is done to prevent certain features (e.g., bright pixels) from dominating the learning process. It helps to stabilize and speed up the training of neural networks by ensuring that all input features are on a similar scale. The mean and std values used are common to models pre-trained on the ImageNet dataset, a practice that ensures consistency and better performance when using a pre-trained backbone.\n\n**Data Loaders**\n\nA data loader is an iterator in PyTorch that provides an efficient way to load and prepare data in batches. It handles key tasks like shuffling the data, parallelizing data loading using multiple worker processes (num_workers), and batching the data. This is essential for modern deep learning, as models are trained in batches rather than on one image at a time, which is much more computationally efficient. The collate_fn is a crucial part of the data loader for object detection, as it handles the \"ragged\" data structure where each image can have a different number of objects (and thus a different number of bounding boxes).","metadata":{}},{"cell_type":"markdown","source":"## Dataset configuration and Hyperparameters Configuration","metadata":{}},{"cell_type":"markdown","source":"**Dataset Configuration and Hyperparameters Configuration:** \n\nThe data.yaml file is created, which defines the paths for the training, validation, and testing images and labels. It also specifies the single class, 'object', to be detected.","metadata":{}},{"cell_type":"markdown","source":"**Hyperparameter Configuration**\n\nThe following parameters were configured for the training and evaluation of the object detection model:\n\n**Dataset Configuration**\n\n- **data_yaml:** A dictionary-like configuration that defines the dataset's structure.\n- **path:** The root directory of the dataset.\n- **train:** The relative path to the training images directory.\n- **val:** The relative path to the validation images directory, used for tuning hyperparameters and evaluating the model during training.\n- **test:** The relative path to the test images directory, used for the final, independent evaluation of the trained model's performance.\n- **nc:** The number of classes in the dataset. In this case, 1 indicates a single class.\n- **names:** A list of class names, with ['object'] specifying the name for the single class.\n\n**Training Hyperparameters**\n\n- **TRAIN_EPOCHS:** The total number of times the model will iterate through the entire training dataset. An epoch represents one complete pass of the training data through the algorithm.\n- **IMG_SIZE:** The size to which all images will be resized before being fed into the model. A size of 640×640 pixels is a standard input dimension for many modern object detection models.\n- **PATIENCE:** The number of epochs to wait for the validation loss to improve before stopping the training. This technique, known as early stopping, prevents overfitting and saves computational resources. A patience of 20 means the training will halt if the model's performance on the validation set does not improve for 20 consecutive epochs.\n- **BATCH_SIZE:** The number of training examples utilized in one iteration. A batch size of 8 means the model processes 8 images at a time before updating its internal parameters.\n- **CONF_THRESHOLD:** The confidence score threshold used to filter object detections. Only bounding boxes with a confidence score greater than 0.25 will be considered as valid detections.\n- **IOU_THRESHOLD:** The Intersection over Union (IoU) threshold used for Non-Maximum Suppression (NMS). This value determines how much overlap is allowed between predicted bounding boxes for the same object. An IoU threshold of 0.5 means any predicted bounding box with an IoU of 0.5 or more with a higher-confidence box will be suppressed.\n- **device:** Specifies the computing device to be used for training. The code automatically selects a GPU (cuda) if available, otherwise, it defaults to the CPU (cpu) for training.\n- **base_path:** The base directory for the dataset, providing the full file path for the model to access the data.","metadata":{}},{"cell_type":"code","source":"# Dataset configuration\ndata_yaml = \"\"\"\npath: /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2\ntrain: train/images\nval: val/images\ntest: testImages/images\nnc: 1\nnames: ['object']\n\"\"\"\n\n# Hyperparameters Configuration\nTRAIN_EPOCHS = 100\nIMG_SIZE = 640\nPATIENCE = 10\nBATCH_SIZE = 8\nCONF_THRESHOLD = 0.25\nIOU_THRESHOLD = 0.5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbase_path = \"/kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:24:33.550870Z","iopub.execute_input":"2025-09-16T11:24:33.551139Z","iopub.status.idle":"2025-09-16T11:24:33.580994Z","shell.execute_reply.started":"2025-09-16T11:24:33.551106Z","shell.execute_reply":"2025-09-16T11:24:33.580401Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**File Saving**\n\nThe provided code snippet is responsible for saving the data.yaml string to a file named data.yaml in the /kaggle/working directory. This is a crucial step to make the dataset configuration accessible to the training script, which will typically read its data paths and class information from this file. The os.makedirs('/kaggle/working', exist_ok=True) command ensures that the directory exists before the file is written, preventing a FileNotFoundError.","metadata":{}},{"cell_type":"code","source":"# Save data.yaml\nos.makedirs('/kaggle/working', exist_ok=True)\nwith open('/kaggle/working/data.yaml', 'w') as file:\n    file.write(data_yaml)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:24:33.583731Z","iopub.execute_input":"2025-09-16T11:24:33.583917Z","iopub.status.idle":"2025-09-16T11:24:33.610057Z","shell.execute_reply.started":"2025-09-16T11:24:33.583902Z","shell.execute_reply":"2025-09-16T11:24:33.609376Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"**Data Augmentation (Albumentations):**\n\nThis step sets up the data processing and loading for an object detection project, using data augmentation to prepare the images for model training. The goal is to get the data from its raw format into a state that is both usable by deep learning models and robust enough to handle real-world variations.\n\nThe albumentations library is used to apply various data augmentation techniques. train_transforms includes a series of random changes like horizontal flips, brightness/contrast adjustments, and color jitter to help the model generalize better. val_transforms only includes resizing and normalization to ensure consistent evaluation.","metadata":{}},{"cell_type":"markdown","source":"**How this step Works?**\n\nThis section of the code focuses on transforming and loading the dataset. It defines a set of image transformations and then creates a dataset and a data loader for both the training and validation sets.\n\n**Image Transformations:** The albumentations library is used to define image transformations.\n- **train_transforms:** This is a composition of several transformations applied during training.\n  - **A.Resize(IMG_SIZE, IMG_SIZE):** Resizes all images to a consistent 640x640 resolution, which is a requirement for many neural networks.\n  - **A.HorizontalFlip(p=0.5):** Randomly flips the image horizontally with a 50% probability, a common technique for data augmentation.\n  - **A.RandomBrightnessContrast(p=0.2):** Randomly adjusts the brightness and contrast of the image, simulating different lighting conditions.\n  - **A.ColorJitter(p=0.2):** Randomly changes the color properties (e.g., hue, saturation), making the model less sensitive to color variations.\n  - **A.Normalize(...):** Normalizes the pixel values using the mean and standard deviation of the ImageNet dataset. This is a standard practice that helps the model converge faster and more effectively.\n  - **ToTensorV2():** Converts the image from a NumPy array to a PyTorch tensor.\n- **val_transforms:** This is a simpler set of transformations for validation. It only includes Resize and Normalize because the goal of validation is to evaluate the model's performance on unseen data without introducing random variations that could obscure its true performance.","metadata":{}},{"cell_type":"code","source":"# Data transforms\ntrain_transforms = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n    A.ColorJitter(p=0.2),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\nval_transforms = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:24:34.348194Z","iopub.execute_input":"2025-09-16T11:24:34.348537Z","iopub.status.idle":"2025-09-16T11:24:34.358664Z","shell.execute_reply.started":"2025-09-16T11:24:34.348516Z","shell.execute_reply":"2025-09-16T11:24:34.358046Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Prepared Dataset","metadata":{}},{"cell_type":"markdown","source":"**Custom Dataset and Data Loaders**\n\nThis section details the implementation of a custom dataset class and data loaders, which are crucial for preparing the data in a format suitable for the object detection model.\n\n**The SoupCanDataset Class**\n\nThe SoupCanDataset is a custom class that inherits from PyTorch's torch.utils.data.Dataset. This is a standard practice for handling datasets that are not directly supported by pre-built PyTorch utilities. The class is a crucial bridge between the raw image and label files and the data format required by the training pipeline.\n\n- **Initialization (__init__):** The constructor takes the directories for images and labels as input. It then finds and stores the paths to all image files (.png, .jpg, .jpeg).\n\n- **Length (__len__):** This method returns the total number of images in the dataset, allowing DataLoader to know how many samples to iterate over.\n\n- **Item Retrieval (__getitem__):** This is the most important method. When called with an index idx, it performs the following steps:\n\n  - Loads the image file at the specified index using the Pillow library (Image.open).\n  - Reads the corresponding label file. The labels are expected to be in the YOLO format, which uses normalized coordinates (x_center, y_center, width, height).\n  - Coordinate Conversion Theory: Object detection models often require bounding box coordinates in a specific format. The SoupCanDataset converts the normalized YOLO coordinates to the absolute pixel coordinates (x1, y1, x2, y2), where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner of the bounding box. This conversion is necessary for compatibility with many object detection frameworks, such as Faster R-CNN, which expect this format.\n  - It populates a target dictionary with the bounding boxes, class labels, and other metadata required by the model, such as image_id, area, and iscrowd.\n  - It applies any specified data augmentations or transformations (self.transforms) to the image. It also scales the bounding box coordinates to match the new image size if a transformation is applied.\n\n**Data Loaders**\n\nThe DataLoader utility from PyTorch is used to create iterable data loaders from the custom SoupCanDataset. Data loaders provide an efficient way to feed data to the training loop by handling batching, shuffling, and multi-processing.\n\n- **collate_fn:** This custom function is a critical part of the data loading process for object detection. Because each image can have a variable number of bounding boxes, a standard DataLoader cannot create a single tensor from a batch of images and their targets. The collate_fn function takes a list of samples and combines them into a single batch, ensuring that the bounding boxes and labels are handled correctly, preventing errors during training.\n- **train_loader:** This loader is configured with shuffle=True to randomize the order of the training data in each epoch. Shuffling is a best practice to prevent the model from learning the order of the data, which could lead to overfitting.\n- **val_loader:** The validation loader is set with shuffle=False because the order of evaluation data does not affect the model's performance metrics.\n- **num_workers:** This parameter enables multi-process data loading, which helps to speed up the training process by loading data in the background while the GPU is busy with computations.","metadata":{}},{"cell_type":"code","source":"# --- Custom Dataset ---\nclass SoupCanDataset(Dataset):\n    def __init__(self, image_dir, label_dir, transforms=None):\n        self.image_dir = Path(image_dir)\n        self.label_dir = Path(label_dir)\n        self.transforms = transforms\n        self.images = [p for p in self.image_dir.glob(\"*\") if p.suffix.lower() in ['.png', '.jpg', '.jpeg']]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n\n        img = Image.open(img_path).convert(\"RGB\")\n        img_width, img_height = img.size\n\n        boxes = []\n        labels = []\n        if label_path.exists():\n            with open(label_path, 'r') as f:\n                for line in f:\n                    try:\n                        cls_id, x_center, y_center, width, height = map(float, line.strip().split())\n                        x1 = (x_center - width / 2) * img_width\n                        y1 = (y_center - height / 2) * img_height\n                        x2 = (x_center + width / 2) * img_width\n                        y2 = (y_center + height / 2) * img_height\n                        boxes.append([x1, y1, x2, y2])\n                        labels.append(int(cls_id))\n                    except ValueError:\n                        print(f\"[warning] Invalid label in {label_path}: {line.strip()}\")\n\n        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx]),\n            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.empty((0,)),\n            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n        }\n\n        if self.transforms:\n            img = self.transforms(img)\n            if len(boxes) > 0:\n                scale = IMG_SIZE / max(img_width, img_height)\n                boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n                boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n                target[\"boxes\"] = boxes.clamp(min=0, max=IMG_SIZE-1)\n\n        return img, target\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Create datasets and dataloaders\ntrain_dataset = SoupCanDataset(f\"{base_path}/train/images\", f\"{base_path}/train/labels\", transforms=train_transforms)\nval_dataset = SoupCanDataset(f\"{base_path}/val/images\", f\"{base_path}/val/labels\", transforms=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:24:37.398866Z","iopub.execute_input":"2025-09-16T11:24:37.399753Z","iopub.status.idle":"2025-09-16T11:24:37.416041Z","shell.execute_reply.started":"2025-09-16T11:24:37.399722Z","shell.execute_reply":"2025-09-16T11:24:37.415458Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"This step trains and combines the predictions of two powerful models, YOLOv8 and Faster R-CNN, to achieve more robust results. This process is particularly effective for bridging the gap between synthetic and real-world data, as each model can learn different features from the training set.\n\n**Theoretical Concepts**\n\n- **YOLO (You Only Look Once):** A single-stage object detector that frames object detection as a single regression problem. It predicts bounding boxes and class probabilities simultaneously, making it exceptionally fast.\n- **Faster R-CNN (ResNet50):** A two-stage object detector. The first stage, a Region Proposal Network (RPN), proposes regions of interest, and the second stage refines these proposals into final detections. This two-step process generally yields higher localization accuracy than single-stage models.\n- **Model Ensemble:** The practice of combining multiple machine learning models to improve overall performance. Since YOLOv8 and Faster R-CNN have different architectural strengths (speed vs. accuracy), an ensemble can leverage the best of both worlds.\n- **Transfer Learning:** The use of a pre-trained model as a starting point. This is crucial when the target dataset is small or a significant domain gap exists (as in Sim2Real challenges).\n- **Weighted Boxes Fusion (WBF):** A sophisticated ensemble technique for bounding box predictions. Unlike NMS, which simply discards redundant boxes, WBF merges overlapping boxes, preserving valuable information from multiple models to generate a more accurate final prediction.\n\n**Model Training Overview**\n\nThis phase focuses on fine-tuning two distinct object detection models.\n\n**YOLOv8 Training:**\n\n- **Initialization and Model Loading:** The pipeline begins by loading pre-trained versions of the YOLOv8 models using the file paths \"yolov8x.pt\" and \"yolov8n.pt\". The 'x' variant is the largest and most computationally intensive, while 'n' is a smaller, more compact variant. This approach leverages transfer learning - a fundamental concept in deep learning where a model pre-trained on a massive dataset (like COCO) is used as a starting point. This significantly accelerates training and enhances performance on the new custom dataset.\n- **Taining Execution:** The core of the code is the .train() function call, which manages the entire training process. It takes numerous arguments, known as hyperparameters, that control the training procedure, from the number of training cycles to various data augmentation techniques. The models are trained on the data specified by the data parameter.\n\n**Faster R-CNN Training:**\n\n- **Model Loading:** The code loads a pre-trained Faster R-CNN with a ResNet50-FPN backbone. This is a classic two-stage architecture known for its high accuracy.\n- **Head Replacement:** The final classification head of the model is replaced with a new one that is specifically configured for the single object class in the dataset (plus a background class). This is a standard practice in transfer learning to adapt a pre-trained model for a new task.\n- **Custom Training Loop:** Unlike YOLOv8, Faster R-CNN is trained using a manual, explicit training loop. This loop iterates through epochs, calculates the combined loss (from box regression and classification), and updates the model's weights using an optimizer and a learning rate scheduler.\n- **Early Stopping:** The loop incorporates a crucial early stopping mechanism. It monitors the model's performance on a validation set and saves the best-performing weights. If the model's performance doesn't improve for a certain number of epochs (PATIENCE), training is halted to prevent overfitting.\n\n**Ensemble Inference and Validation**\n\nThe final stage involves combining the trained models to create a more robust ensemble system.\n\n- **Ensemble Inference:** The run_ensemble_inference function takes an image, runs it through the trained YOLOv8x, YOLOv8n, and Faster R-CNN models, and combines their predicted bounding boxes.\n- **Non-Maximum Suppression (NMS):** After combining the predictions from all three models, NMS is applied to filter out redundant and overlapping bounding boxes. This process ensures that for each object, only the most confident and representative bounding box remains.\n- **Validation:** The validate_ensemble function evaluates the combined model's performance on the validation dataset. It calculates two critical metrics:\n  - **Precision:** Measures the accuracy of the positive predictions. It is the ratio of true positives to the total number of predicted positives (TP / (TP + FP)).\n  - **Recall:** Measures the model's ability to find all relevant instances. It is the ratio of true positives to the total number of actual positives (TP / (TP + FN)).","metadata":{}},{"cell_type":"markdown","source":"## Fine-Tune YOLOv8 (YOLO8n and YOLO8x)","metadata":{}},{"cell_type":"markdown","source":"**YOLOv8n (Nano)**\n\nYOLOv8n is the smallest and fastest model in the YOLOv8 series. The 'n' stands for 'nano,' reflecting its compact size. It has the fewest parameters and the lowest computational requirements.\n\n- **Size:** It has a significantly smaller number of parameters, making its model file size very small. This is ideal for deployment on devices with limited memory, such as mobile phones, embedded systems, and drones.\n- **Speed:** Due to its smaller size, it performs inference much faster. This makes it a great choice for real-time applications where a high frame rate is critical, even if it means sacrificing some accuracy.\n- **Accuracy:** It is the least accurate of the YOLOv8 models. While still a powerful object detector, it may not perform as well on complex scenes or with small, hard-to-detect objects compared to its larger counterparts.\n\n**YOLOv8x (XLarge)**\n\nYOLOv8x is the largest and most accurate model in the YOLOv8 family. The 'x' stands for 'extra-large.' It has the highest number of parameters, making it computationally intensive.\n\n- **Size:** It has a large number of parameters, resulting in a much larger model file. This requires more storage and memory on the device where it's deployed.\n- **Speed:** It is the slowest of the YOLOv8 models. Its size and complexity mean that it takes more time to process each image, which can be a limiting factor for real-time applications.\n- **Accuracy:** It offers the highest accuracy among the YOLOv8 models. Its deeper and wider network architecture allows it to learn more complex features, leading to better performance in detecting objects, especially in challenging conditions.","metadata":{}},{"cell_type":"markdown","source":"**Model Fine-Tuning Methodology**\n\n- **Initialization and Error Handling:** The pipeline begins with a notice that YOLOv8 training is starting. A try...except block is used to ensure the program can gracefully handle potential failures during the training process, such as issues with file paths or hardware.\n- **Model Loading:** The lines yolo8x_model = YOLO(\"yolov8x.pt\") and yolo8n_model = YOLO(\"yolov8n.pt\") initialize the models. The .pt files refer to pre-trained versions of the YOLOv8 models. The 'x' variant is the largest and most computationally intensive, while 'n' is a smaller, more compact variant. By loading these files, the pipeline is leveraging transfer learning, which means it is starting with a model that has already learned to detect a wide range of objects from a massive dataset (typically COCO). This significantly speeds up training and improves performance on the new dataset.\n- **Training Execution:** The core of the code is the .train(...) function call. This method executes the entire training process. It takes numerous arguments, known as hyperparameters, which control the training procedure, from the number of training cycles to various data augmentation techniques. The model is trained on the data specified by the data parameter.","metadata":{}},{"cell_type":"markdown","source":"**Hyperparameters:**\n\nThe training process is controlled by a set of hyperparameters that define how the model learns.\n\n- **epochs:** The number of times the entire dataset is passed forward and backward through the neural network during training.\n- **imgsz:** The size of the input image for the model.\n- **patience:** The number of epochs with no improvement in the validation loss after which training will be stopped. This is a form of early stopping.\n- **cos_lr:** A boolean that enables a cosine annealing learning rate scheduler.\n- **dropout:** The probability of randomly dropping neurons during training to prevent overfitting.\n- **mosaic:** The probability of using mosaic data augmentation, which stitches four images together to create a single training image.\n- **lr0:** The initial learning rate for the optimizer.\n- **optimizer:** The optimization algorithm used to update the model's weights (e.g., SGD, Adam).\n- **momentum:** A parameter for SGD that helps accelerate the descent in the right direction.\n- **weight_decay:** A regularization term that penalizes large weights, preventing the model from becoming too complex and overfitting.\n- **single_cls:** A boolean to indicate if the dataset contains only a single object class.\n- **plots:** A boolean to enable the generation of plots of training and validation metrics.\n- **cache:** A boolean to cache the dataset in memory or on disk for faster loading during training.\n- **flipud:** The probability of performing a random vertical flip on the images for data augmentation.\n- **scale:** The probability of performing a random scaling augmentation on the images.\n- **name:** The name for the training run, which determines the directory where results are saved.\n- **verbose:** A boolean to display detailed training output.","metadata":{}},{"cell_type":"markdown","source":"**Theories and Concepts**\n\n**YOLO (You Only Look Once):** YOLO is a prominent single-stage object detection model. Unlike older, two-stage detectors, YOLO processes an entire image in one pass to simultaneously predict all bounding boxes and their class probabilities. This makes it exceptionally fast, suitable for real-time applications. The YOLOv8 architecture improves upon previous versions by incorporating new techniques to enhance accuracy while maintaining high speed.\n**Stochastic Gradient Descent (SGD) and Adam:** The optimizer parameter specifies the algorithm used to update the model's weights during training.\n\n- **SGD** works by calculating the gradient of the loss function with respect to the model's weights for a small batch of data. It then updates the weights in the opposite direction of the gradient to minimize the loss.\n- **Adam** is a more advanced optimizer that adapts the learning rate for each weight, often leading to faster convergence. The use of different optimizers for the YOLOv8x and YOLOv8n models allows for experimentation to find the best configuration for each model.\n\n**Learning Rate Schedulers:** The cos_lr=True parameter activates a cosine annealing learning rate scheduler. A learning rate is the step size the optimizer takes. This scheduler is a technique that adjusts the learning rate throughout training, starting high to make fast progress and gradually decreasing it in a cosine-like curve. This helps the model to settle into an optimal configuration without getting stuck in local minima.\n\n**Data Augmentation:** Techniques like mosaic, flipud, and scale are forms of data augmentation. These methods artificially expand the diversity of the training dataset by applying random transformations to the images. This prevents the model from overfitting to the specific examples it has seen, making it more robust and better at generalizing to new, unseen data.\n\n**Regularization:** Hyperparameters such as dropout and weight_decay are forms of regularization. They are designed to prevent the model from overfitting.\n\n- **Dropout** randomly \"drops\" a percentage of neuron connections during training, forcing the network to learn more robust features.\n- **Weight Decay** adds a penalty to the loss function for large weights, discouraging the model from relying too heavily on any single feature.","metadata":{}},{"cell_type":"markdown","source":"### Fine-Tune YOLO8n","metadata":{}},{"cell_type":"code","source":"print(\"[notice] Training YOLO8n...\")\ntry:\n    yolo8n_model = YOLO(\"yolov8n.pt\")\n    yolo8n_results = yolo8n_model.train(\n        data=\"/kaggle/working/data.yaml\",\n        epochs=TRAIN_EPOCHS,\n        imgsz=IMG_SIZE,\n        patience=PATIENCE,\n        cos_lr=True,\n        dropout=0.2,\n        mosaic=0.5,\n        lr0=0.001,\n        optimizer=\"Adam\",\n        momentum=0.9,\n        weight_decay=0.0005,\n        single_cls=True,\n        plots=True,\n        cache=True,\n        flipud=0.5,\n        scale=0.8,\n        name=\"yolo8n_trained\",\n        verbose=True\n    )\nexcept Exception as e:\n    print(f\"[error] YOLO8n training failed: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:24:41.859153Z","iopub.execute_input":"2025-09-16T11:24:41.859791Z","iopub.status.idle":"2025-09-16T11:28:19.295691Z","shell.execute_reply.started":"2025-09-16T11:24:41.859765Z","shell.execute_reply":"2025-09-16T11:28:19.294677Z"}},"outputs":[{"name":"stdout","text":"[notice] Training YOLO8n...\nUltralytics 8.3.200 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.2, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.9, mosaic=0.5, multi_scale=False, name=yolo8n_trained2, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/yolo8n_trained2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.8, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding class names with single class.\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.5 ms, read: 1007.5±79.7 MB/s, size: 4298.7 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train/labels... 201 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 201/201 298.1it/s 0.7s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 201/201 30.1it/s 6.7s0.1s\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.6 ms, read: 1187.3±366.7 MB/s, size: 3973.2 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val/labels... 163 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 163/163 205.5it/s 0.8s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 163/163 30.9it/s 5.3s0.1s\nPlotting labels to /kaggle/working/runs/detect/yolo8n_trained2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/detect/yolo8n_trained2\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      1/100      2.09G      0.516      2.674     0.8603         16        640: 100% ━━━━━━━━━━━━ 13/13 3.5it/s 3.8s0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 3.9it/s 1.6s0.2ss\n                   all        163        163    0.00777          1      0.826       0.63\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      2/100      2.21G     0.6437     0.9025     0.9005         15        640: 100% ━━━━━━━━━━━━ 13/13 6.7it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.2it/s 0.7s0.2s\n                   all        163        163    0.00921      0.994      0.891      0.716\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      3/100      2.24G     0.5622     0.6826     0.8737         12        640: 100% ━━━━━━━━━━━━ 13/13 6.6it/s 2.0s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.1it/s 0.7s0.1s\n                   all        163        163      0.899      0.988      0.983      0.832\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      4/100      2.24G     0.5607     0.6207     0.8538         17        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.8it/s 0.7s0.1s\n                   all        163        163      0.748          1      0.989       0.87\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      5/100      2.27G     0.5603     0.6141     0.8635         14        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.5it/s 0.7s0.2s\n                   all        163        163      0.508          1       0.98      0.859\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      6/100      2.28G      0.565     0.5682     0.8774         23        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.869      0.734      0.886      0.771\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      7/100       2.3G     0.5151     0.5601     0.8715         22        640: 100% ━━━━━━━━━━━━ 13/13 7.3it/s 1.8s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.4it/s 0.7s0.2s\n                   all        163        163      0.762      0.689      0.771      0.642\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      8/100      2.32G     0.5503     0.5816     0.8929         11        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163      0.914      0.848      0.928      0.843\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      9/100      2.32G     0.5009     0.5229     0.8524         17        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.3it/s 0.7s0.2s\n                   all        163        163      0.987      0.949      0.983      0.897\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     10/100      2.35G     0.4966     0.4997     0.8674         13        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.1it/s 0.7s0.1s\n                   all        163        163      0.996      0.994      0.995      0.911\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     11/100      2.37G     0.4858     0.4683     0.8592         15        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163      0.993      0.994      0.995      0.876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     12/100      2.38G     0.4789      0.501     0.8445         14        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.4it/s 0.7s0.2s\n                   all        163        163      0.993      0.994      0.995      0.878\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     13/100      2.39G     0.5047     0.4901     0.8576         14        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.973      0.933      0.976      0.846\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     14/100      2.42G     0.4523     0.4557     0.8485         12        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.2it/s 0.7s0.2s\n                   all        163        163          1      0.938       0.98      0.921\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     15/100      2.44G      0.522     0.4623     0.8908         12        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.4it/s 0.7s0.2s\n                   all        163        163      0.999      0.963      0.994      0.937\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     16/100      2.46G     0.4726     0.4373     0.8436         18        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.2s\n                   all        163        163       0.97       0.99      0.994      0.919\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     17/100      2.46G     0.4566     0.4292     0.8476         12        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.6it/s 0.7s0.2s\n                   all        163        163      0.975      0.977      0.993      0.934\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     18/100      2.49G     0.4717     0.4459     0.8427         14        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.0it/s 0.8s0.2s\n                   all        163        163      0.976      0.986      0.994      0.924\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     19/100      2.51G     0.4744     0.4313     0.8678         10        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.5it/s 0.7s0.2s\n                   all        163        163          1      0.994      0.995       0.93\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     20/100      2.52G     0.4776     0.4379     0.8722         15        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.6it/s 0.7s0.2s\n                   all        163        163          1      0.993      0.995      0.944\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     21/100      2.53G     0.4725     0.4081     0.8503         18        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.1it/s 0.7s0.2s\n                   all        163        163      0.994      0.988      0.994      0.945\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     22/100      2.56G     0.4941     0.4305     0.8488         16        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.8it/s 0.7s0.2s\n                   all        163        163      0.983      0.969      0.994      0.941\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     23/100      2.57G     0.4332      0.406     0.8422         13        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.4it/s 0.7s0.2s\n                   all        163        163      0.975      0.988      0.994      0.933\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     24/100      2.59G     0.4635     0.4344     0.8711         14        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.5it/s 0.7s0.2s\n                   all        163        163      0.988      0.982      0.995      0.943\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     25/100       2.6G     0.4474     0.4161     0.8379         15        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.5it/s 0.7s0.2s\n                   all        163        163      0.992      0.994      0.995       0.96\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     26/100      2.62G     0.4916     0.4297     0.8515         13        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.994      0.992      0.995      0.947\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     27/100      2.64G      0.459     0.4262     0.8507         17        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163      0.988      0.992      0.995      0.954\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     28/100      2.66G     0.4447     0.4031     0.8395         16        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.974          1      0.994      0.944\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     29/100      2.67G     0.4525     0.4129     0.8866         16        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.1it/s 0.7s0.1s\n                   all        163        163      0.981      0.988      0.995      0.953\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     30/100      2.69G     0.4407     0.3978     0.8518         14        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.5it/s 0.7s0.2s\n                   all        163        163      0.998      0.994      0.995      0.952\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     31/100      2.71G     0.4371     0.3933     0.8519         19        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163      0.994          1      0.995       0.96\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     32/100      2.73G     0.4274      0.387     0.8545         18        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.2it/s 0.7s0.1s\n                   all        163        163          1      0.993      0.995      0.949\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     33/100      2.73G     0.3975     0.3631     0.8263         12        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.2it/s 0.7s0.1s\n                   all        163        163      0.999      0.994      0.995      0.939\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     34/100      2.76G     0.4254     0.3741     0.8502         16        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.999      0.994      0.995      0.964\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     35/100      2.78G     0.3898     0.3593     0.8331         15        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163      0.994          1      0.995      0.968\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     36/100      2.79G     0.3957     0.3477     0.8371         17        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163      0.999          1      0.995      0.969\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     37/100       2.8G     0.3777     0.3326     0.8319         12        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163      0.999          1      0.995      0.971\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     38/100      2.83G     0.3852     0.3343     0.8455         15        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163      0.999          1      0.995      0.969\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     39/100      2.85G      0.384     0.3473     0.8386          9        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.993          1      0.995      0.969\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     40/100      2.86G     0.4076     0.3431     0.8554         17        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.994          1      0.995      0.976\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     41/100      2.87G     0.3767     0.3215     0.8256         25        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.993          1      0.995      0.967\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     42/100       2.9G     0.3768     0.3152     0.8299         16        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163      0.994          1      0.995       0.97\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     43/100      2.91G     0.3744     0.3166     0.8274         13        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.988          1      0.995      0.973\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     44/100      2.93G     0.4004      0.333     0.8349         13        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163      0.998          1      0.995      0.972\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     45/100      2.94G     0.3822     0.3272      0.847         14        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163          1          1      0.995      0.963\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     46/100      2.96G     0.3924     0.3319     0.8474         15        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.8it/s 0.7s0.2s\n                   all        163        163      0.999          1      0.995      0.968\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     47/100      2.98G     0.3777     0.3241     0.8466         18        640: 100% ━━━━━━━━━━━━ 13/13 7.2it/s 1.8s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.3it/s 0.7s0.1s\n                   all        163        163      0.993          1      0.995       0.98\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     48/100         3G     0.3773     0.3293     0.8394         12        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163      0.993          1      0.995      0.971\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     49/100      3.01G      0.409     0.3351     0.8191         24        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.5it/s 0.6s0.1s\n                   all        163        163      0.999      0.994      0.995      0.981\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     50/100      3.03G     0.3336      0.302     0.8165         20        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.8it/s 0.7s0.2s\n                   all        163        163          1      0.988      0.995      0.976\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     51/100      3.05G     0.3701     0.3121     0.8349         12        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.5it/s 0.7s0.2s\n                   all        163        163      0.997      0.994      0.995      0.984\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     52/100      3.07G     0.3479     0.3022     0.8058          9        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.2it/s 0.7s0.1s\n                   all        163        163      0.999      0.994      0.995      0.984\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     53/100      3.08G     0.3312      0.296     0.8219         13        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.1it/s 0.7s0.1s\n                   all        163        163      0.993          1      0.995      0.988\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     54/100       3.1G     0.3177     0.2887     0.8174         20        640: 100% ━━━━━━━━━━━━ 13/13 7.2it/s 1.8s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.1s\n                   all        163        163      0.993          1      0.995      0.981\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     55/100      3.12G     0.3379     0.2895     0.8135         12        640: 100% ━━━━━━━━━━━━ 13/13 7.1it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.7it/s 0.7s0.2s\n                   all        163        163      0.999      0.994      0.995      0.986\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     56/100      3.14G     0.3444     0.3128     0.8297         16        640: 100% ━━━━━━━━━━━━ 13/13 6.7it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163      0.999      0.994      0.995      0.989\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     57/100      3.14G     0.3463     0.2923      0.818          9        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.8s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163          1          1      0.995      0.988\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     58/100      3.17G     0.3497     0.2943      0.834         21        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163          1          1      0.995      0.983\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     59/100      3.19G     0.3444     0.2845     0.8214          9        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.9it/s 0.7s0.1s\n                   all        163        163          1          1      0.995      0.987\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     60/100       3.2G      0.305     0.2659     0.8169         14        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163          1          1      0.995      0.987\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     61/100      3.21G     0.3243     0.2752     0.8096         17        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163      0.999          1      0.995      0.985\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     62/100      3.24G     0.3445     0.2788     0.8233         10        640: 100% ━━━━━━━━━━━━ 13/13 7.0it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.6it/s 0.7s0.2s\n                   all        163        163      0.999          1      0.995      0.987\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     63/100      3.26G     0.3041     0.2634     0.8245         14        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.3s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 8.8it/s 0.7s0.1s\n                   all        163        163      0.999          1      0.995      0.988\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     64/100      3.27G     0.3217     0.2662     0.8242         12        640: 100% ━━━━━━━━━━━━ 13/13 6.9it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.0it/s 0.7s0.1s\n                   all        163        163          1          1      0.995      0.985\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     65/100      3.28G     0.3306     0.2685     0.8412         12        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163          1          1      0.995      0.989\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     66/100      3.31G     0.2975     0.2639     0.8097         18        640: 100% ━━━━━━━━━━━━ 13/13 6.8it/s 1.9s0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 9.3it/s 0.6s0.1s\n                   all        163        163          1          1      0.995      0.989\n\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 56, best model saved as best.pt.\nTo update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n\n66 epochs completed in 0.052 hours.\nOptimizer stripped from /kaggle/working/runs/detect/yolo8n_trained2/weights/last.pt, 6.2MB\nOptimizer stripped from /kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt, 6.2MB\n\nValidating /kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt...\nUltralytics 8.3.200 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 5.1it/s 1.2s0.2s\n                   all        163        163      0.999      0.994      0.995      0.988\nSpeed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 2.1ms postprocess per image\nResults saved to \u001b[1m/kaggle/working/runs/detect/yolo8n_trained2\u001b[0m\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Fine-Tune YOLO8x","metadata":{}},{"cell_type":"code","source":"# --- Train YOLO Models ---\nprint(\"[notice] Training YOLO8x...\")\ntry:\n    yolo8x_model = YOLO(\"yolov8x.pt\")\n    yolo8x_results = yolo8x_model.train(\n        data=\"/kaggle/working/data.yaml\",\n        epochs=TRAIN_EPOCHS,\n        imgsz=IMG_SIZE,\n        patience=PATIENCE,\n        cos_lr=True,\n        dropout=0.4,\n        mosaic=0.2,\n        lr0=0.0001,\n        optimizer=\"SGD\",\n        momentum=0.975,\n        weight_decay=0.0001,\n        single_cls=True,\n        plots=True,\n        cache=True,\n        flipud=0.25,\n        scale=1.0,\n        name=\"yolo8x_trained\",\n        verbose=True\n    )\nexcept Exception as e:\n    print(f\"[error] YOLO8x training failed: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:28:19.298083Z","iopub.execute_input":"2025-09-16T11:28:19.298616Z","iopub.status.idle":"2025-09-16T11:37:32.152390Z","shell.execute_reply.started":"2025-09-16T11:28:19.298583Z","shell.execute_reply":"2025-09-16T11:37:32.151588Z"}},"outputs":[{"name":"stdout","text":"[notice] Training YOLO8x...\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt': 100% ━━━━━━━━━━━━ 130.5MB 254.4MB/s 0.5s 0.5s<0.0s\nUltralytics 8.3.200 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.4, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.25, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.975, mosaic=0.2, multi_scale=False, name=yolo8x_trained, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/yolo8x_trained, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=1.0, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0001, workers=8, workspace=None\nOverriding class names with single class.\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n 22        [15, 18, 21]  1   8718931  ultralytics.nn.modules.head.Detect           [1, [320, 640, 640]]          \nModel summary: 209 layers, 68,153,571 parameters, 68,153,555 gradients, 258.1 GFLOPs\n\nTransferred 589/595 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.5 ms, read: 1970.4±110.5 MB/s, size: 4236.3 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train/labels... 201 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 201/201 300.2it/s 0.7s0.0s\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 201/201 33.9it/s 5.9s0.1s\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.5 ms, read: 804.2±533.1 MB/s, size: 3817.3 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val/labels... 163 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 163/163 169.9it/s 1.0s0.0s\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 163/163 33.6it/s 4.9s0.0s\nPlotting labels to /kaggle/working/runs/detect/yolo8x_trained/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0001, momentum=0.975) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0001), 103 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/detect/yolo8x_trained\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      1/100      11.7G     0.3587      4.217     0.8467          8        640: 100% ━━━━━━━━━━━━ 13/13 0.9it/s 15.2s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.0s0.7ss\n                   all        163        163      0.592      0.442      0.545       0.53\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      2/100      12.3G     0.3715      3.853     0.8797         11        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 15.4s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.0s0.7ss\n                   all        163        163      0.629      0.577      0.639      0.618\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      3/100      12.4G     0.3352      3.437     0.8366         17        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 15.5s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.0s0.7ss\n                   all        163        163      0.764      0.753      0.788      0.768\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      4/100      12.2G     0.3495      2.678     0.8735         10        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 15.8s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.861      0.828      0.898      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      5/100        12G     0.2915      1.961     0.8271         15        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 15.8s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.928      0.865      0.956      0.937\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      6/100      12.1G      0.309      1.558     0.8471         11        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.0s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.929      0.969      0.978      0.973\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      7/100      12.2G     0.2751      1.175     0.8325          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.1s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.947      0.979      0.989      0.986\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      8/100        12G     0.2538     0.9764     0.8038         16        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.976      0.981      0.993      0.988\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K      9/100      12.2G     0.2642     0.8993     0.8232          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163       0.98      0.988      0.994       0.99\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     10/100      12.4G     0.2375     0.8257     0.8093          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.0s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.991      0.988      0.995       0.99\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     11/100      12.1G     0.2413     0.6606      0.799         11        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.0s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.988      0.999      0.995       0.99\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     12/100      12.1G     0.2767     0.5882     0.8316         12        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.1s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.987      0.994      0.995      0.991\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     13/100      12.2G     0.2556     0.5575     0.8101         10        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.0s1.0s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.987      0.994      0.995      0.991\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     14/100      12.1G     0.2526     0.5369     0.7961          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.1s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.994      0.991      0.995       0.99\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     15/100      12.1G      0.238     0.4582     0.8052          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.994      0.993      0.995      0.992\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     16/100      12.1G     0.2344     0.4126     0.8105         12        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.3s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163          1      0.993      0.995      0.993\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     17/100      12.2G     0.2364      0.379     0.8032         10        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.999      0.994      0.995      0.992\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     18/100      12.1G     0.2453     0.3473     0.8237         12        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.1s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.999      0.994      0.995      0.991\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     19/100      12.4G     0.2389     0.3194     0.8319         11        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.1s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163          1      0.999      0.995       0.99\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     20/100      12.1G     0.2262     0.3081      0.807          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.9it/s 3.1s0.7ss\n                   all        163        163      0.999      0.994      0.995       0.99\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     21/100      12.2G     0.2095     0.2693     0.7981         12        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.999      0.994      0.995      0.991\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     22/100      12.4G     0.2067     0.2634     0.8149         10        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163          1      0.994      0.995      0.989\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     23/100      12.1G     0.1887     0.2408     0.7887         10        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163      0.999      0.994      0.995      0.988\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     24/100      12.1G     0.2313     0.2458     0.8131          9        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.1s0.7ss\n                   all        163        163          1      0.999      0.995      0.988\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K     25/100      12.3G     0.2545     0.2521     0.8257         14        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.2s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 2.0it/s 3.0s0.7ss\n                   all        163        163          1          1      0.995      0.988\n\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 15, best model saved as best.pt.\nTo update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n\n25 epochs completed in 0.145 hours.\nOptimizer stripped from /kaggle/working/runs/detect/yolo8x_trained/weights/last.pt, 136.7MB\nOptimizer stripped from /kaggle/working/runs/detect/yolo8x_trained/weights/best.pt, 136.7MB\n\nValidating /kaggle/working/runs/detect/yolo8x_trained/weights/best.pt...\nUltralytics 8.3.200 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 112 layers, 68,124,531 parameters, 0 gradients, 257.4 GFLOPs\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.8it/s 3.3s0.8ss\n                   all        163        163          1      0.993      0.995      0.993\nSpeed: 0.1ms preprocess, 15.9ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1m/kaggle/working/runs/detect/yolo8x_trained\u001b[0m\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"Use","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom ultralytics import YOLO\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.ops import box_iou, nms\nfrom tqdm import tqdm\nimport torchvision.transforms.v2 as T\nimport pandas as pd\nimport csv\nfrom ensemble_boxes import weighted_boxes_fusion\nimport os\n\n# Constants\nTRAIN_EPOCHS = 1\nIMG_SIZE = 640\nPATIENCE = 20\nBATCH_SIZE = 8\nCONF_THRESHOLD = 0.25\nIOU_THRESHOLD = 0.5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbase_path = \"/kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2\"\n\n# --- Data Transformations ---\ntrain_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.RandomHorizontalFlip(p=0.5),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\nval_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\n# --- Custom Dataset ---\nclass SoupCanDataset(Dataset):\n    def __init__(self, image_dir, label_dir, transforms=None):\n        self.image_dir = Path(image_dir)\n        self.label_dir = Path(label_dir)\n        self.transforms = transforms\n        self.images = [p for p in self.image_dir.glob(\"*\") if p.suffix.lower() in ['.png', '.jpg', '.jpeg']]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n\n        img = Image.open(img_path).convert(\"RGB\")\n        img_width, img_height = img.size\n\n        boxes = []\n        labels = []\n        if label_path.exists():\n            with open(label_path, 'r') as f:\n                for line in f:\n                    try:\n                        cls_id, x_center, y_center, width, height = map(float, line.strip().split())\n                        x1 = (x_center - width / 2) * img_width\n                        y1 = (y_center - height / 2) * img_height\n                        x2 = (x_center + width / 2) * img_width\n                        y2 = (y_center + height / 2) * img_height\n                        boxes.append([x1, y1, x2, y2])\n                        labels.append(int(cls_id))\n                    except ValueError:\n                        print(f\"[warning] Invalid label in {label_path}: {line.strip()}\")\n\n        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx]),\n            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.empty((0,)),\n            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n        }\n\n        if self.transforms:\n            img = self.transforms(img)\n            if len(boxes) > 0:\n                scale = IMG_SIZE / max(img_width, img_height)\n                boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n                boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n                target[\"boxes\"] = boxes.clamp(min=0, max=IMG_SIZE-1)\n\n        return img, target\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Create datasets and dataloaders\ntrain_dataset = SoupCanDataset(f\"{base_path}/train/images\", f\"{base_path}/train/labels\", transforms=train_transforms)\nval_dataset = SoupCanDataset(f\"{base_path}/val/images\", f\"{base_path}/val/labels\", transforms=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)\n\n# --- Train YOLO Models ---\nprint(\"[notice] Training YOLO8x...\")\ntry:\n    yolo8x_model = YOLO(\"yolov8x.pt\")\n    yolo8x_results = yolo8x_model.train(\n        data=\"/kaggle/working/data.yaml\",\n        epochs=TRAIN_EPOCHS,\n        imgsz=IMG_SIZE,\n        patience=PATIENCE,\n        cos_lr=True,\n        dropout=0.4,\n        mosaic=0.2,\n        lr0=0.0001,\n        optimizer=\"SGD\",\n        momentum=0.975,\n        weight_decay=0.0001,\n        single_cls=True,\n        plots=True,\n        cache=True,\n        flipud=0.25,\n        scale=1.0,\n        name=\"yolo8x_trained\",\n        verbose=True\n    )\nexcept Exception as e:\n    print(f\"[error] YOLO8x training failed: {e}\")\n    raise\n\nprint(\"[notice] Training YOLO8n...\")\ntry:\n    yolo8n_model = YOLO(\"yolov8n.pt\")\n    yolo8n_results = yolo8n_model.train(\n        data=\"/kaggle/working/data.yaml\",\n        epochs=TRAIN_EPOCHS,\n        imgsz=IMG_SIZE,\n        patience=PATIENCE,\n        cos_lr=True,\n        dropout=0.2,\n        mosaic=0.5,\n        lr0=0.001,\n        optimizer=\"Adam\",\n        momentum=0.9,\n        weight_decay=0.0005,\n        single_cls=True,\n        plots=True,\n        cache=True,\n        flipud=0.5,\n        scale=0.8,\n        name=\"yolo8n_trained\",\n        verbose=True\n    )\nexcept Exception as e:\n    print(f\"[error] YOLO8n training failed: {e}\")\n    raise\n\n# --- Train Faster R-CNN with ResNet50-FPN ---\nprint(\"[notice] Training Faster R-CNN with ResNet50-FPN...\")\ndef get_faster_rcnn_model(num_classes):\n    model = fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)  # +1 for background\n    return model\n\nfaster_rcnn_model = get_faster_rcnn_model(num_classes=1)  # 1 class (object) + background\nfaster_rcnn_model.to(device)\n\noptimizer = torch.optim.SGD(faster_rcnn_model.parameters(), lr=0.0001, momentum=0.975, weight_decay=0.0001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCHS)\n\ndef train_faster_rcnn(model, train_loader, val_loader, epochs, patience):\n    best_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            train_loss += losses.item()\n        \n        train_loss /= len(train_loader)\n        \n        # Validation with loss computation in training mode\n        model.train()  # Temporarily set to train mode for loss\n        val_loss = 0\n        with torch.no_grad():\n            for images, targets in val_loader:\n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                val_loss += losses.item()\n        \n        val_loss /= len(val_loader)\n        print(f\"[notice] Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), \"/kaggle/working/faster_rcnn_best.pt\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"[notice] Early stopping triggered\")\n                break\n        \n        scheduler.step()\n\ntry:\n    train_faster_rcnn(faster_rcnn_model, train_loader, val_loader, TRAIN_EPOCHS, PATIENCE)\nexcept Exception as e:\n    print(f\"[error] Faster R-CNN training failed: {e}\")\n    raise\n\n# --- Load and Prepare Models for Ensemble ---\nprint(\"[notice] Loading trained models for ensemble...\")\nyolo8x_model = YOLO(\"/kaggle/working/runs/detect/yolo8x_trained2/weights/best.pt\")  # Fixed path\nyolo8n_model = YOLO(\"/kaggle/working/runs/detect/yolo8n_trained/weights/best.pt\")\nfaster_rcnn_model.load_state_dict(torch.load(\"/kaggle/working/faster_rcnn_best.pt\"))\nfaster_rcnn_model.to(device)\nfaster_rcnn_model.eval()\n\n# --- Ensemble Inference ---\ntest_images_path = f\"{base_path}/testImages/images\"\noutput_dir = \"/kaggle/working/predictions/labels\"\nos.makedirs(output_dir, exist_ok=True)\n\ntest_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\nfor img_path in tqdm(list(Path(test_images_path).glob(\"*\")), desc=\"Predicting\"):\n    if img_path.suffix.lower() not in ['.png', '.jpg', '.jpeg']:\n        continue\n    \n    img_name = img_path.stem\n    img = Image.open(img_path).convert(\"RGB\")\n    img_width, img_height = img.size\n    \n    # Preprocess image\n    img_tensor = test_transforms(img).unsqueeze(0).to(device)  # Add batch dimension\n    \n    # YOLOv8x predictions\n    yolo8x_boxes = []\n    yolo8x_scores = []\n    yolo8x_labels = []\n    try:\n        yolo8x_results = yolo8x_model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)\n        for result in yolo8x_results:\n            boxes = result.boxes.data\n            if boxes is not None:\n                for box in boxes:\n                    x1, y1, x2, y2, conf, cls_id = box.tolist()\n                    if conf >= CONF_THRESHOLD:\n                        # Normalize to [0, 1] relative to original image size\n                        yolo8x_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n                        yolo8x_scores.append(conf)\n                        yolo8x_labels.append(int(cls_id))\n    except Exception as e:\n        print(f\"[warning] YOLOv8x prediction failed for {img_name}: {e}\")\n    \n    # YOLOv8n predictions\n    yolo8n_boxes = []\n    yolo8n_scores = []\n    yolo8n_labels = []\n    try:\n        yolo8n_results = yolo8n_model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)\n        for result in yolo8n_results:\n            boxes = result.boxes.data\n            if boxes is not None:\n                for box in boxes:\n                    x1, y1, x2, y2, conf, cls_id = box.tolist()\n                    if conf >= CONF_THRESHOLD:\n                        yolo8n_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n                        yolo8n_scores.append(conf)\n                        yolo8n_labels.append(int(cls_id))\n    except Exception as e:\n        print(f\"[warning] YOLOv8n prediction failed for {img_name}: {e}\")\n    \n    # Faster R-CNN predictions\n    frcnn_boxes = []\n    frcnn_scores = []\n    frcnn_labels = []\n    try:\n        with torch.no_grad():\n            predictions = faster_rcnn_model([img_tensor[0]])[0]\n            for box, score, label in zip(predictions['boxes'], predictions['scores'], predictions['labels']):\n                if score >= CONF_THRESHOLD and label == 1:  # Class 1 is object\n                    x1, y1, x2, y2 = box.tolist()\n                    # Scale boxes to original image size and normalize\n                    scale_x = img_width / IMG_SIZE\n                    scale_y = img_height / IMG_SIZE\n                    x1, x2 = x1 * scale_x / img_width, x2 * scale_x / img_width\n                    y1, y2 = y1 * scale_y / img_height, y2 * scale_y / img_height\n                    frcnn_boxes.append([x1, y1, x2, y2])\n                    frcnn_scores.append(score.item())\n                    frcnn_labels.append(0)  # Map to class 0 for consistency\n    except Exception as e:\n        print(f\"[warning] Faster R-CNN prediction failed for {img_name}: {e}\")\n    \n    # Ensemble using Weighted Boxes Fusion\n    boxes_list = [yolo8x_boxes, yolo8n_boxes, frcnn_boxes]\n    scores_list = [yolo8x_scores, yolo8n_scores, frcnn_scores]\n    labels_list = [yolo8x_labels, yolo8n_labels, frcnn_labels]\n    weights = [1.0, 0.9, 0.8]  # YOLOv8x > YOLOv8n > Faster R-CNN\n    \n    try:\n        fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n            boxes_list,\n            scores_list,\n            labels_list,\n            weights=weights,\n            iou_thr=IOU_THRESHOLD,\n            skip_box_thr=CONF_THRESHOLD\n        )\n    except Exception as e:\n        print(f\"[warning] WBF failed for {img_name}: {e}\")\n        fused_boxes, fused_scores, fused_labels = [], [], []\n    \n    # Save ensemble predictions in YOLO format\n    output_txt = Path(output_dir) / f\"{img_name}.txt\"\n    with open(output_txt, \"w\") as f:\n        for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n            x1, y1, x2, y2 = box\n            x_center = (x1 + x2) / 2\n            y_center = (y1 + y2) / 2\n            width = x2 - x1\n            height = y2 - y1\n            f.write(f\"{int(label)} {score:.6f} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n\nprint(f\"[notice] All ensemble detections saved to: {output_dir}\")\n\n# --- Convert Predictions to Submission CSV ---\ndef predictions_to_csv(\n    preds_folder: str = \"/kaggle/working/predictions/labels\",\n    output_csv: str = \"/kaggle/working/submission.csv\",\n    test_images_folder: str = \"/kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/testImages/images\",\n    allowed_extensions: tuple = (\".jpg\", \".png\", \".jpeg\")\n):\n    preds_path = Path(preds_folder)\n    test_images_path = Path(test_images_folder)\n    \n    test_images = {p.stem for p in test_images_path.glob(\"*\") if p.suffix.lower() in allowed_extensions}\n    predictions = []\n    predicted_images = set()\n    \n    for txt_file in preds_path.glob(\"*.txt\"):\n        image_id = txt_file.stem\n        predicted_images.add(image_id)\n        with open(txt_file, \"r\") as f:\n            valid_lines = [line.strip() for line in f if len(line.strip().split()) == 6]\n        pred_str = \" \".join(valid_lines) if valid_lines else \"no boxes\"\n        predictions.append({\"image_id\": image_id, \"prediction_string\": pred_str})\n    \n    missing_images = test_images - predicted_images\n    for image_id in missing_images:\n        predictions.append({\"image_id\": image_id, \"prediction_string\": \"no boxes\"})\n    \n    submission_df = pd.DataFrame(predictions)\n    submission_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_MINIMAL)\n    print(f\"[notice] Submission saved to {output_csv}\")\n\npredictions_to_csv()\n\n# --- Updated Ensemble Inference and Validation ---\n@torch.no_grad()\ndef run_ensemble_inference(image_tensor, conf_thres=0.25, iou_thres=0.5):\n    \"\"\"\n    Runs inference with YOLO and Faster R-CNN models and combines their results.\n    Accepts a PyTorch tensor with a batch dimension.\n    \"\"\"\n    # YOLOv8x inference\n    yolo8x_results = yolo8x_model.predict(image_tensor, conf=conf_thres, iou=iou_thres, verbose=False)\n    if len(yolo8x_results[0].boxes.data) == 0:\n        yolo8x_preds = torch.empty((0, 6), device=device)\n    else:\n        yolo8x_preds = yolo8x_results[0].boxes.data.to(device)\n\n    # YOLOv8n inference\n    yolo8n_results = yolo8n_model.predict(image_tensor, conf=conf_thres, iou=iou_thres, verbose=False)\n    if len(yolo8n_results[0].boxes.data) == 0:\n        yolo8n_preds = torch.empty((0, 6), device=device)\n    else:\n        yolo8n_preds = yolo8n_results[0].boxes.data.to(device)\n\n    # Faster R-CNN inference\n    faster_rcnn_image = image_tensor.to(device)  # Move to GPU directly\n    faster_rcnn_results = faster_rcnn_model([faster_rcnn_image[0]])  # Pass as list, single image per call\n    if len(faster_rcnn_results[0]['boxes']) == 0:\n        faster_rcnn_preds = torch.empty((0, 6), device=device)\n    else:\n        boxes = faster_rcnn_results[0]['boxes'].to(device)\n        scores = faster_rcnn_results[0]['scores'].to(device)\n        labels = faster_rcnn_results[0]['labels'].to(device)\n        mask = (labels == 1)  # Filter for class 1 (object)\n        faster_rcnn_preds = torch.cat((boxes[mask], scores[mask].unsqueeze(1), torch.zeros_like(scores[mask].unsqueeze(1))), dim=1)\n\n    # Combine detections\n    combined_detections = torch.cat((yolo8x_preds, yolo8n_preds, faster_rcnn_preds), dim=0)\n\n    if combined_detections.shape[0] == 0:\n        return torch.empty((0, 4), device=device), torch.empty((0,), device=device), torch.empty((0,), device=device)\n\n    # Apply NMS\n    combined_boxes = combined_detections[:, :4]\n    combined_scores = combined_detections[:, 4]\n    combined_classes = combined_detections[:, 5]\n\n    keep_indices = nms(combined_boxes, combined_scores, iou_thres)\n    \n    final_boxes = combined_boxes[keep_indices]\n    final_scores = combined_scores[keep_indices]\n    final_classes = combined_classes[keep_indices]\n\n    # Scale boxes to match IMG_SIZE\n    orig_shape = yolo8x_results[0].orig_shape\n    scale_x = IMG_SIZE / orig_shape[1]\n    scale_y = IMG_SIZE / orig_shape[0]\n    final_boxes[:, [0, 2]] *= scale_x\n    final_boxes[:, [1, 3]] *= scale_y\n    final_boxes = final_boxes.clamp(min=0, max=IMG_SIZE-1)\n\n    return final_boxes, final_scores, final_classes\n\n@torch.no_grad()\ndef validate_ensemble(val_loader):\n    \"\"\"\n    Validates the ensemble model on the validation dataset.\n    \"\"\"\n    print(\"[notice] Validating ensemble model...\")\n    all_metrics = []\n\n    for images, targets in tqdm(val_loader, desc=\"Validating Ensemble\"):\n        images = torch.stack(images).to(device)\n        batch_size = len(images)\n\n        final_boxes_batch = []\n        final_scores_batch = []\n        final_classes_batch = []\n        for i in range(batch_size):\n            final_boxes, final_scores, final_classes = run_ensemble_inference(images[i].unsqueeze(0))\n            final_boxes_batch.append(final_boxes)\n            final_scores_batch.append(final_scores)\n            final_classes_batch.append(final_classes)\n\n        for i in range(batch_size):\n            gt_boxes = targets[i]['boxes'].to(device)\n            if len(gt_boxes) == 0 or len(final_boxes_batch[i]) == 0:\n                all_metrics.append({'precision': 0, 'recall': 0})\n                continue\n\n            iou_matrix = box_iou(gt_boxes, final_boxes_batch[i])\n            detected_count = 0\n\n            for gt_idx in range(len(gt_boxes)):\n                if torch.max(iou_matrix[gt_idx]) >= 0.5:\n                    detected_count += 1\n\n            true_positives = detected_count\n            predicted_positives = len(final_boxes_batch[i])\n            actual_positives = len(gt_boxes)\n\n            precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n            recall = true_positives / actual_positives if actual_positives > 0 else 0\n            all_metrics.append({'precision': precision, 'recall': recall})\n\n    if all_metrics:\n        avg_precision = np.mean([m['precision'] for m in all_metrics])\n        avg_recall = np.mean([m['recall'] for m in all_metrics])\n        print(f\"Ensemble Validation Results:\")\n        print(f\"Average Precision: {avg_precision:.4f}\")\n        print(f\"Average Recall: {avg_recall:.4f}\")\n    else:\n        print(\"No detections found. Validation metrics not calculated.\")\n\n# Run the validation on the ensembled models\nvalidate_ensemble(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:33:34.074939Z","iopub.execute_input":"2025-09-15T13:33:34.075278Z","iopub.status.idle":"2025-09-15T13:38:27.189256Z","shell.execute_reply.started":"2025-09-15T13:33:34.075249Z","shell.execute_reply":"2025-09-15T13:38:27.188210Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"[notice] Training YOLO8x...\nUltralytics 8.3.199 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.4, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.25, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.975, mosaic=0.2, multi_scale=False, name=yolo8x_trained3, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/yolo8x_trained3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=1.0, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0001, workers=8, workspace=None\nOverriding class names with single class.\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n 22        [15, 18, 21]  1   8718931  ultralytics.nn.modules.head.Detect           [1, [320, 640, 640]]          \nModel summary: 209 layers, 68,153,571 parameters, 68,153,555 gradients, 258.1 GFLOPs\n\nTransferred 589/595 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.1±0.3 ms, read: 3191.2±533.3 MB/s, size: 4236.3 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train/labels... 201 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 201/201 342.1it/s 0.6s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 201/201 35.0it/s 5.7s0.1s\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.1±0.2 ms, read: 997.6±647.4 MB/s, size: 3817.3 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val/labels... 163 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 163/163 226.0it/s 0.7s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 163/163 34.5it/s 4.7s0.1s\nPlotting labels to /kaggle/working/runs/detect/yolo8x_trained3/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0001, momentum=0.975) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0001), 103 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/detect/yolo8x_trained3\u001b[0m\nStarting training for 1 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K        1/1      11.3G     0.3587      4.217     0.8467          8        640: 100% ━━━━━━━━━━━━ 13/13 0.8it/s 16.5s1.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.6it/s 3.8s0.9ss\n                   all        163        163      0.592      0.442      0.545       0.53\n\n1 epochs completed in 0.007 hours.\nOptimizer stripped from /kaggle/working/runs/detect/yolo8x_trained3/weights/last.pt, 136.7MB\nOptimizer stripped from /kaggle/working/runs/detect/yolo8x_trained3/weights/best.pt, 136.7MB\n\nValidating /kaggle/working/runs/detect/yolo8x_trained3/weights/best.pt...\nUltralytics 8.3.199 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 112 layers, 68,124,531 parameters, 0 gradients, 257.4 GFLOPs\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 1.7it/s 3.6s0.8ss\n                   all        163        163      0.597      0.446      0.546      0.531\nSpeed: 0.1ms preprocess, 17.4ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1m/kaggle/working/runs/detect/yolo8x_trained3\u001b[0m\n[notice] Training YOLO8n...\nUltralytics 8.3.199 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.2, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.9, mosaic=0.5, multi_scale=False, name=yolo8n_trained2, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/yolo8n_trained2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.8, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding class names with single class.\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1381.3±1333.3 MB/s, size: 4236.3 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train/labels... 201 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 201/201 348.8it/s 0.6s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/train is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 201/201 34.4it/s 5.8s0.1s\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.1±0.3 ms, read: 1071.4±696.7 MB/s, size: 3817.3 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val/labels... 163 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 163/163 233.4it/s 0.7s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/val is not writeable, cache not saved.\nWARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB RAM): 100% ━━━━━━━━━━━━ 163/163 33.5it/s 4.9s0.1s\nPlotting labels to /kaggle/working/runs/detect/yolo8n_trained2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/detect/yolo8n_trained2\u001b[0m\nStarting training for 1 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K        1/1      5.15G      0.516      2.674     0.8603         16        640: 100% ━━━━━━━━━━━━ 13/13 5.5it/s 2.4s0.2s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.3it/s 1.4s0.3s\n                   all        163        163    0.00777          1      0.826       0.63\n\n1 epochs completed in 0.002 hours.\nOptimizer stripped from /kaggle/working/runs/detect/yolo8n_trained2/weights/last.pt, 6.2MB\nOptimizer stripped from /kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt, 6.2MB\n\nValidating /kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt...\nUltralytics 8.3.199 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 6/6 4.6it/s 1.3s0.3s\n                   all        163        163    0.00777          1      0.825      0.631\nSpeed: 0.1ms preprocess, 1.5ms inference, 0.0ms loss, 2.2ms postprocess per image\nResults saved to \u001b[1m/kaggle/working/runs/detect/yolo8n_trained2\u001b[0m\n[notice] Training Faster R-CNN with ResNet50-FPN...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 26/26 [00:47<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 1: Train Loss = 0.3144, Val Loss = 0.1110\n[notice] Loading trained models for ensemble...\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 159/159 [01:48<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"[notice] All ensemble detections saved to: /kaggle/working/predictions/labels\n[notice] Submission saved to /kaggle/working/submission.csv\n[notice] Validating ensemble model...\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:31<00:00,  1.52s/it]","output_type":"stream"},{"name":"stdout","text":"Ensemble Validation Results:\nAverage Precision: 0.0462\nAverage Recall: 0.1043\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom ultralytics import YOLO\nimport torchvision.transforms.v2 as T\nfrom tqdm import tqdm\nimport os\nimport pandas as pd\nimport csv\nfrom ensemble_boxes import weighted_boxes_fusion\n\n# Constants\nIMG_SIZE = 640  # Same as training\nCONF_THRESHOLD = 0.25  # Adjust as needed\nIOU_THRESHOLD = 0.5  # For WBF\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbase_path = \"/kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2\"\n\n# Load trained models\nyolo8x_model = YOLO(\"/kaggle/working/runs/detect/yolo8x_trained3/weights/best.pt\")\nyolo8n_model = YOLO(\"/kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt\")\nfaster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=False)\nin_features = faster_rcnn_model.roi_heads.box_predictor.cls_score.in_features\nfaster_rcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)  # 1 class + background\nfaster_rcnn_model.load_state_dict(torch.load(\"/kaggle/working/faster_rcnn_best.pt\"))\nfaster_rcnn_model.eval()\nfaster_rcnn_model.to(device)\n\n# Test transformations (aligned with training)\ntest_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\n# Prediction and ensemble\ntest_images_path = f\"{base_path}/testImages/images\"\noutput_dir = \"/kaggle/working/predictions/labels\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor img_path in tqdm(list(Path(test_images_path).glob(\"*\")), desc=\"Predicting\"):\n    if img_path.suffix.lower() not in ['.png', '.jpg', '.jpeg']:\n        continue\n    \n    img_name = img_path.stem\n    img = Image.open(img_path).convert(\"RGB\")\n    img_width, img_height = img.size\n    \n    # Preprocess image\n    img_tensor = test_transforms(img).unsqueeze(0).to(device)  # Add batch dimension\n    \n    # YOLOv8x predictions\n    yolo8x_boxes = []\n    yolo8x_scores = []\n    yolo8x_labels = []\n    try:\n        yolo8x_results = yolo8x_model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)\n        for result in yolo8x_results:\n            boxes = result.boxes.data\n            if boxes is not None:\n                for box in boxes:\n                    x1, y1, x2, y2, conf, cls_id = box.tolist()\n                    if conf >= CONF_THRESHOLD:\n                        # Normalize to [0, 1] relative to original image size\n                        yolo8x_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n                        yolo8x_scores.append(conf)\n                        yolo8x_labels.append(int(cls_id))\n    except Exception as e:\n        print(f\"[warning] YOLOv8x prediction failed for {img_name}: {e}\")\n    \n    # YOLOv8n predictions\n    yolo8n_boxes = []\n    yolo8n_scores = []\n    yolo8n_labels = []\n    try:\n        yolo8n_results = yolo8n_model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)\n        for result in yolo8n_results:\n            boxes = result.boxes.data\n            if boxes is not None:\n                for box in boxes:\n                    x1, y1, x2, y2, conf, cls_id = box.tolist()\n                    if conf >= CONF_THRESHOLD:\n                        yolo8n_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n                        yolo8n_scores.append(conf)\n                        yolo8n_labels.append(int(cls_id))\n    except Exception as e:\n        print(f\"[warning] YOLOv8n prediction failed for {img_name}: {e}\")\n    \n    # Faster R-CNN predictions\n    frcnn_boxes = []\n    frcnn_scores = []\n    frcnn_labels = []\n    try:\n        with torch.no_grad():\n            predictions = faster_rcnn_model([img_tensor[0]])[0]\n            for box, score, label in zip(predictions['boxes'], predictions['scores'], predictions['labels']):\n                if score >= CONF_THRESHOLD and label == 1:  # Class 1 is object\n                    x1, y1, x2, y2 = box.tolist()\n                    # Scale boxes to original image size and normalize\n                    scale_x = img_width / IMG_SIZE\n                    scale_y = img_height / IMG_SIZE\n                    x1, x2 = x1 * scale_x / img_width, x2 * scale_x / img_width\n                    y1, y2 = y1 * scale_y / img_height, y2 * scale_y / img_height\n                    frcnn_boxes.append([x1, y1, x2, y2])\n                    frcnn_scores.append(score.item())\n                    frcnn_labels.append(0)  # Map to class 0 for consistency\n    except Exception as e:\n        print(f\"[warning] Faster R-CNN prediction failed for {img_name}: {e}\")\n    \n    # Ensemble using Weighted Boxes Fusion\n    boxes_list = [yolo8x_boxes, yolo8n_boxes, frcnn_boxes]\n    scores_list = [yolo8x_scores, yolo8n_scores, frcnn_scores]\n    labels_list = [yolo8x_labels, yolo8n_labels, frcnn_labels]\n    weights = [1.0, 0.9, 0.8]  # YOLOv8x > YOLOv8n > Faster R-CNN\n    \n    try:\n        fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n            boxes_list,\n            scores_list,\n            labels_list,\n            weights=weights,\n            iou_thr=IOU_THRESHOLD,\n            skip_box_thr=CONF_THRESHOLD\n        )\n    except Exception as e:\n        print(f\"[warning] WBF failed for {img_name}: {e}\")\n        fused_boxes, fused_scores, fused_labels = [], [], []\n    \n    # Save ensemble predictions in YOLO format\n    output_txt = Path(output_dir) / f\"{img_name}.txt\"\n    with open(output_txt, \"w\") as f:\n        for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n            x1, y1, x2, y2 = box\n            x_center = (x1 + x2) / 2\n            y_center = (y1 + y2) / 2\n            width = x2 - x1\n            height = y2 - y1\n            f.write(f\"{int(label)} {score:.6f} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n\nprint(f\"[notice] All ensemble detections saved to: {output_dir}\")\n\n# Convert predictions to submission.csv (unchanged from original)\ndef predictions_to_csv(\n    preds_folder: str = \"/kaggle/working/predictions/labels\",\n    output_csv: str = \"/kaggle/working/submission.csv\",\n    test_images_folder: str = \"/kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/testImages/images\",\n    allowed_extensions: tuple = (\".jpg\", \".png\", \".jpeg\")\n):\n    preds_path = Path(preds_folder)\n    test_images_path = Path(test_images_folder)\n    \n    test_images = {p.stem for p in test_images_path.glob(\"*\") if p.suffix.lower() in allowed_extensions}\n    predictions = []\n    predicted_images = set()\n    \n    for txt_file in preds_path.glob(\"*.txt\"):\n        image_id = txt_file.stem\n        predicted_images.add(image_id)\n        with open(txt_file, \"r\") as f:\n            valid_lines = [line.strip() for line in f if len(line.strip().split()) == 6]\n        pred_str = \" \".join(valid_lines) if valid_lines else \"no boxes\"\n        predictions.append({\"image_id\": image_id, \"prediction_string\": pred_str})\n    \n    missing_images = test_images - predicted_images\n    for image_id in missing_images:\n        predictions.append({\"image_id\": image_id, \"prediction_string\": \"no boxes\"})\n    \n    submission_df = pd.DataFrame(predictions)\n    submission_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_MINIMAL)\n    print(f\"[notice] Submission saved to {output_csv}\")\n\npredictions_to_csv()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T13:42:49.740505Z","iopub.execute_input":"2025-09-15T13:42:49.741117Z","iopub.status.idle":"2025-09-15T13:44:35.354637Z","shell.execute_reply.started":"2025-09-15T13:42:49.741096Z","shell.execute_reply":"2025-09-15T13:44:35.353814Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 205MB/s]\nPredicting: 100%|██████████| 159/159 [01:43<00:00,  1.53it/s]","output_type":"stream"},{"name":"stdout","text":"[notice] All ensemble detections saved to: /kaggle/working/predictions/labels\n[notice] Submission saved to /kaggle/working/submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Fine-Tune ResNet50-FPN","metadata":{}},{"cell_type":"markdown","source":"This process involves fine-tuning a pre-trained Faster R-CNN model on a custom dataset using a manual training loop.\n\n**Model Fine-Tuning Methodology**\n\n- **Initialization and Model Loading:** The code loads a pre-trained Faster R-CNN with a ResNet50-FPN backbone. This is a classic two-stage architecture known for its high accuracy. It also leverages transfer learning by using a model that has already learned general features from a large dataset.\n- **Head Replacement:** The final classification head (model.roi_heads.box_predictor) of the pre-trained model is replaced with a new one specifically configured for the single object class in the dataset, plus a background class. This is a crucial step to adapt the model for the new task.\n- **Custom Training Loop:** Unlike the built-in .train() method used for YOLO, Faster R-CNN is trained using a manual, explicit training loop. This loop provides more granular control over the training process.\n  - **Training Loop:** For each epoch, the model is set to training mode (model.train()). It then iterates through batches of images and their corresponding labels from the train_loader.\n  - **Forward Pass:** model(images, targets) performs a forward pass, calculating the predicted bounding boxes and class probabilities. It also returns a dictionary of loss values.\n  - **Backward Pass:** losses.backward() computes the gradients of the total loss with respect to the model's weights.\n  - **Weight Update:** optimizer.step() uses these gradients to update the model's weights, minimizing the loss.\n  - **Validation Loop:** After each training epoch, a validation step is performed. The model is set to evaluation mode (model.eval()) and processes the validation data without updating weights. This provides an unbiased measure of the model's performance on unseen data. The loss is calculated and used to monitor for overfitting.\n\n- **Early Stopping:** The loop incorporates a crucial early stopping mechanism. It monitors the model's performance on a validation set and saves the best-performing weights. If the model's performance doesn't improve for a certain number of epochs (PATIENCE), training is halted to prevent overfitting.\n\n**Hyperparameters:**\n\nThe training process is controlled by a set of hyperparameters that define how the model learns.\n\n- **lr:** The initial learning rate for the optimizer, set to 0.0001.\n- **momentum:** A parameter for SGD that helps accelerate the descent in the right direction, set to 0.975.\n- **weight_decay:** A regularization term that penalizes large weights, set to 0.0001.\n- **epochs:** The number of times the entire dataset is passed forward and backward through the neural network during training, controlled by TRAIN_EPOCHS.\n- **patience:** The number of epochs with no improvement in the validation loss after which training will be stopped, controlled by PATIENCE.\n- **T_max:** A parameter for the cosine annealing learning rate scheduler, set to TRAIN_EPOCHS.\n\n**Theories and Concepts**\n\n- **Faster R-CNN:** This is a two-stage object detection model. The first stage, a Region Proposal Network (RPN), scans the image and proposes regions that likely contain an object. The second stage then takes these proposals, classifies the objects within them, and refines the bounding boxes.\n- **Transfer Learning:** The code uses a model pre-trained on a large dataset like COCO. By using these pre-trained weights, the model already has a strong foundation for understanding visual features. Fine-tuning these weights on a new, smaller dataset allows it to quickly adapt and perform well without needing to train from scratch.\n- **Loss Functions:** Object detection models use a combination of different losses. Faster R-CNN's total loss is a sum of several components: a classification loss to determine the correct object class, and a localization loss to fine-tune the bounding box coordinates.\n- **SGD with Momentum:** SGD is an optimization algorithm that iteratively updates model weights. Momentum adds a fraction of the previous weight update to the current one, helping to accelerate training and overcome local minima in the loss landscape.\n- **Learning Rate Scheduler:** The learning rate controls how much the model's weights are adjusted in each step. A scheduler like cosine annealing gradually reduces the learning rate over time. This helps the model make large updates at the beginning of training and smaller, more precise adjustments later, which can lead to better convergence.\n- **Early Stopping:** This is a powerful regularization technique used to prevent overfitting. By stopping training when validation performance plateaus, it ensures the model doesn't become too specialized to the training data and maintains its ability to generalize to new, unseen data.","metadata":{}},{"cell_type":"markdown","source":"**Preprocess dataset for RestNet50**","metadata":{}},{"cell_type":"code","source":"# --- Data Transformations ---\ntrain_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.RandomHorizontalFlip(p=0.5),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\nval_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\n# --- Custom Dataset ---\nclass SoupCanDataset(Dataset):\n    def __init__(self, image_dir, label_dir, transforms=None):\n        self.image_dir = Path(image_dir)\n        self.label_dir = Path(label_dir)\n        self.transforms = transforms\n        self.images = [p for p in self.image_dir.glob(\"*\") if p.suffix.lower() in ['.png', '.jpg', '.jpeg']]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n\n        img = Image.open(img_path).convert(\"RGB\")\n        img_width, img_height = img.size\n\n        boxes = []\n        labels = []\n        if label_path.exists():\n            with open(label_path, 'r') as f:\n                for line in f:\n                    try:\n                        cls_id, x_center, y_center, width, height = map(float, line.strip().split())\n                        x1 = (x_center - width / 2) * img_width\n                        y1 = (y_center - height / 2) * img_height\n                        x2 = (x_center + width / 2) * img_width\n                        y2 = (y_center + height / 2) * img_height\n                        boxes.append([x1, y1, x2, y2])\n                        labels.append(int(cls_id))\n                    except ValueError:\n                        print(f\"[warning] Invalid label in {label_path}: {line.strip()}\")\n\n        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx]),\n            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.empty((0,)),\n            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n        }\n\n        if self.transforms:\n            img = self.transforms(img)\n            if len(boxes) > 0:\n                scale = IMG_SIZE / max(img_width, img_height)\n                boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n                boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n                target[\"boxes\"] = boxes.clamp(min=0, max=IMG_SIZE-1)\n\n        return img, target\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Create datasets and dataloaders\ntrain_dataset = SoupCanDataset(f\"{base_path}/train/images\", f\"{base_path}/train/labels\", transforms=train_transforms)\nval_dataset = SoupCanDataset(f\"{base_path}/val/images\", f\"{base_path}/val/labels\", transforms=val_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Fine-Tune RestNet50**","metadata":{}},{"cell_type":"code","source":"# --- Train Faster R-CNN with ResNet50-FPN ---\nprint(\"[notice] Training Faster R-CNN with ResNet50-FPN...\")\ndef get_faster_rcnn_model(num_classes):\n    model = fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)  # +1 for background\n    return model\n\nfaster_rcnn_model = get_faster_rcnn_model(num_classes=1)  # 1 class (object) + background\nfaster_rcnn_model.to(device)\n\noptimizer = torch.optim.SGD(faster_rcnn_model.parameters(), lr=0.0001, momentum=0.975, weight_decay=0.0001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCHS)\n\ndef train_faster_rcnn(model, train_loader, val_loader, epochs, patience):\n    best_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            train_loss += losses.item()\n        \n        train_loss /= len(train_loader)\n        \n        # Validation with loss computation in training mode\n        model.train()  # Temporarily set to train mode for loss\n        val_loss = 0\n        with torch.no_grad():\n            for images, targets in val_loader:\n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                val_loss += losses.item()\n        \n        val_loss /= len(val_loader)\n        print(f\"[notice] Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), \"/kaggle/working/faster_rcnn_best.pt\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"[notice] Early stopping triggered\")\n                break\n        \n        scheduler.step()\n\ntry:\n    train_faster_rcnn(faster_rcnn_model, train_loader, val_loader, TRAIN_EPOCHS, PATIENCE)\nexcept Exception as e:\n    print(f\"[error] Faster R-CNN training failed: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:44:19.552951Z","iopub.execute_input":"2025-09-16T11:44:19.553890Z","iopub.status.idle":"2025-09-16T12:41:46.454576Z","shell.execute_reply.started":"2025-09-16T11:44:19.553853Z","shell.execute_reply":"2025-09-16T12:41:46.453481Z"}},"outputs":[{"name":"stdout","text":"[notice] Training Faster R-CNN with ResNet50-FPN...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100: 100%|██████████| 26/26 [00:38<00:00,  1.47s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 1: Train Loss = 0.2796, Val Loss = 0.1207\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100: 100%|██████████| 26/26 [00:38<00:00,  1.49s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 2: Train Loss = 0.0994, Val Loss = 0.0787\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100: 100%|██████████| 26/26 [00:40<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 3: Train Loss = 0.0645, Val Loss = 0.0497\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100: 100%|██████████| 26/26 [00:41<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 4: Train Loss = 0.0415, Val Loss = 0.0403\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100: 100%|██████████| 26/26 [00:41<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 5: Train Loss = 0.0374, Val Loss = 0.0366\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100: 100%|██████████| 26/26 [00:41<00:00,  1.60s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 6: Train Loss = 0.0311, Val Loss = 0.0332\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100: 100%|██████████| 26/26 [00:41<00:00,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 7: Train Loss = 0.0301, Val Loss = 0.0297\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 8: Train Loss = 0.0288, Val Loss = 0.0280\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100: 100%|██████████| 26/26 [00:42<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 9: Train Loss = 0.0258, Val Loss = 0.0247\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 10: Train Loss = 0.0247, Val Loss = 0.0253\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 11: Train Loss = 0.0238, Val Loss = 0.0236\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 12: Train Loss = 0.0234, Val Loss = 0.0231\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 13: Train Loss = 0.0223, Val Loss = 0.0227\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 14: Train Loss = 0.0228, Val Loss = 0.0218\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 15: Train Loss = 0.0221, Val Loss = 0.0220\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 16: Train Loss = 0.0199, Val Loss = 0.0209\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 17: Train Loss = 0.0188, Val Loss = 0.0208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 18: Train Loss = 0.0213, Val Loss = 0.0210\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 19: Train Loss = 0.0175, Val Loss = 0.0207\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 20: Train Loss = 0.0174, Val Loss = 0.0199\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 21: Train Loss = 0.0162, Val Loss = 0.0201\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 22: Train Loss = 0.0165, Val Loss = 0.0205\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 23: Train Loss = 0.0177, Val Loss = 0.0190\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 24: Train Loss = 0.0166, Val Loss = 0.0191\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 25: Train Loss = 0.0149, Val Loss = 0.0195\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 26: Train Loss = 0.0161, Val Loss = 0.0199\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 27: Train Loss = 0.0151, Val Loss = 0.0187\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 28: Train Loss = 0.0161, Val Loss = 0.0188\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 29: Train Loss = 0.0165, Val Loss = 0.0189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 30: Train Loss = 0.0135, Val Loss = 0.0188\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 31: Train Loss = 0.0132, Val Loss = 0.0186\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 32: Train Loss = 0.0136, Val Loss = 0.0186\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 33: Train Loss = 0.0151, Val Loss = 0.0184\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100: 100%|██████████| 26/26 [00:42<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 34: Train Loss = 0.0136, Val Loss = 0.0183\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 35: Train Loss = 0.0136, Val Loss = 0.0188\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 36: Train Loss = 0.0127, Val Loss = 0.0182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 37: Train Loss = 0.0139, Val Loss = 0.0177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 38: Train Loss = 0.0165, Val Loss = 0.0177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 39: Train Loss = 0.0122, Val Loss = 0.0180\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100: 100%|██████████| 26/26 [00:42<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 40: Train Loss = 0.0117, Val Loss = 0.0179\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 41: Train Loss = 0.0127, Val Loss = 0.0181\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100: 100%|██████████| 26/26 [00:42<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 42: Train Loss = 0.0122, Val Loss = 0.0182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100: 100%|██████████| 26/26 [00:42<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 43: Train Loss = 0.0113, Val Loss = 0.0175\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 44: Train Loss = 0.0120, Val Loss = 0.0174\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 45: Train Loss = 0.0112, Val Loss = 0.0179\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 46: Train Loss = 0.0126, Val Loss = 0.0177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 47: Train Loss = 0.0115, Val Loss = 0.0172\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100: 100%|██████████| 26/26 [00:41<00:00,  1.61s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 48: Train Loss = 0.0121, Val Loss = 0.0173\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 49: Train Loss = 0.0117, Val Loss = 0.0172\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 50: Train Loss = 0.0114, Val Loss = 0.0177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 51: Train Loss = 0.0124, Val Loss = 0.0178\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 52: Train Loss = 0.0106, Val Loss = 0.0178\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 53: Train Loss = 0.0116, Val Loss = 0.0173\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 54: Train Loss = 0.0125, Val Loss = 0.0177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 55: Train Loss = 0.0102, Val Loss = 0.0179\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100: 100%|██████████| 26/26 [00:42<00:00,  1.63s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 56: Train Loss = 0.0117, Val Loss = 0.0182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 57: Train Loss = 0.0114, Val Loss = 0.0178\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 58: Train Loss = 0.0110, Val Loss = 0.0175\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100: 100%|██████████| 26/26 [00:42<00:00,  1.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"[notice] Epoch 59: Train Loss = 0.0115, Val Loss = 0.0174\n[notice] Early stopping triggered\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Ensemble Model Validation","metadata":{}},{"cell_type":"markdown","source":"This phase describes the final steps of model training, where the trained models are combined for robust object detection and their performance is evaluated.\n\n**Ensemble Methodology:**\n\n- **Loading Trained Models:** The pipeline begins by loading the best-performing models saved during the training phase. The yolo8x and yolo8n models are loaded from their respective best.pt files, while the faster_rcnn model's state dictionary is loaded into its architecture. The faster_rcnn model is also set to evaluation mode (.eval()), which is crucial for inference as it disables training-specific layers like dropout.\n- **Ensemble Inference (run_ensemble_inference):** This function takes a single image and runs inference on all three models (YOLOv8x, YOLOv8n, and Faster R-CNN) to get their individual bounding box predictions. It then combines all these predictions into a single list.\n- **Non-Maximum Suppression (NMS):** After combining the predictions, NMS is applied to filter out redundant and overlapping bounding boxes. This process ensures that for each object, only the most confident and representative bounding box remains, preventing the same object from being detected multiple times by different models.\n- **Validation (validate_ensemble):** This function evaluates the combined model's performance on the validation dataset. It compares the final, ensembled predictions with the ground truth targets to calculate key performance metrics.\n\n**Hyperparameters:**\n\nThe inference process is controlled by a few key hyperparameters.\n\n- **conf_thres:** The confidence threshold, which filters out any detections with a confidence score below this value.\n- **iou_thres:** The Intersection over Union (IoU) threshold used by the NMS algorithm to determine which overlapping bounding boxes should be suppressed.\n\n**Theories and Concepts:**\n\n- **Non-Maximum Suppression (NMS):** A fundamental post-processing algorithm in object detection. It is used to select the best bounding box among many overlapping candidates for a single object.\n- **Precision and Recall:** These are standard metrics for evaluating object detection models.\n  - Precision measures the accuracy of the model's positive predictions. It is the ratio of true positives to the total number of predicted positives (TP/(TP+FP)).\n  - Recall measures the model's ability to find all relevant instances. It is the ratio of true positives to the total number of actual positives (TP/(TP+FN)).\n\n- **Torch.no_grad():** This context manager disables gradient calculation, which is essential during inference and validation. It reduces memory usage and speeds up computation since the model's weights will not be updated.","metadata":{}},{"cell_type":"markdown","source":"### Load and Prepared Model for Ensemble Validation","metadata":{}},{"cell_type":"code","source":"# --- Load and Prepare Models for Ensemble ---\nprint(\"[notice] Loading trained models for ensemble...\")\nyolo8x_model = YOLO(\"/kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt\")  # Fixed path\nyolo8n_model = YOLO(\"/kaggle/working/runs/detect/yolo8x_trained/weights/best.pt\")\nfaster_rcnn_model.load_state_dict(torch.load(\"/kaggle/working/faster_rcnn_best.pt\"))\nfaster_rcnn_model.to(device)\nfaster_rcnn_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T12:41:46.456577Z","iopub.execute_input":"2025-09-16T12:41:46.456852Z","iopub.status.idle":"2025-09-16T12:41:47.701435Z","shell.execute_reply.started":"2025-09-16T12:41:46.456828Z","shell.execute_reply":"2025-09-16T12:41:47.700632Z"}},"outputs":[{"name":"stdout","text":"[notice] Loading trained models for ensemble...\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### Ensemble Model Validation","metadata":{}},{"cell_type":"code","source":"# --- Ensemble Validation ---\n@torch.no_grad()\ndef run_ensemble_inference(image_tensor, conf_thres=0.25, iou_thres=0.5):\n    \"\"\"\n    Runs inference with YOLO and Faster R-CNN models and combines their results.\n    Accepts a PyTorch tensor with a batch dimension.\n    \"\"\"\n    # YOLOv8x inference\n    yolo8x_results = yolo8x_model.predict(image_tensor, conf=conf_thres, iou=iou_thres, verbose=False)\n    if len(yolo8x_results[0].boxes.data) == 0:\n        yolo8x_preds = torch.empty((0, 6), device=device)\n    else:\n        yolo8x_preds = yolo8x_results[0].boxes.data.to(device)\n\n    # YOLOv8n inference\n    yolo8n_results = yolo8n_model.predict(image_tensor, conf=conf_thres, iou=iou_thres, verbose=False)\n    if len(yolo8n_results[0].boxes.data) == 0:\n        yolo8n_preds = torch.empty((0, 6), device=device)\n    else:\n        yolo8n_preds = yolo8n_results[0].boxes.data.to(device)\n\n    # Faster R-CNN inference\n    faster_rcnn_image = image_tensor.to(device)  # Move to GPU directly\n    faster_rcnn_results = faster_rcnn_model([faster_rcnn_image[0]])  # Pass as list, single image per call\n    if len(faster_rcnn_results[0]['boxes']) == 0:\n        faster_rcnn_preds = torch.empty((0, 6), device=device)\n    else:\n        boxes = faster_rcnn_results[0]['boxes'].to(device)\n        scores = faster_rcnn_results[0]['scores'].to(device)\n        labels = faster_rcnn_results[0]['labels'].to(device)\n        mask = (labels == 1)  # Filter for class 1 (object)\n        faster_rcnn_preds = torch.cat((boxes[mask], scores[mask].unsqueeze(1), torch.zeros_like(scores[mask].unsqueeze(1))), dim=1)\n\n    # Combine detections\n    combined_detections = torch.cat((yolo8x_preds, yolo8n_preds, faster_rcnn_preds), dim=0)\n\n    if combined_detections.shape[0] == 0:\n        return torch.empty((0, 4), device=device), torch.empty((0,), device=device), torch.empty((0,), device=device)\n\n    # Apply NMS\n    combined_boxes = combined_detections[:, :4]\n    combined_scores = combined_detections[:, 4]\n    combined_classes = combined_detections[:, 5]\n\n    keep_indices = nms(combined_boxes, combined_scores, iou_thres)\n    \n    final_boxes = combined_boxes[keep_indices]\n    final_scores = combined_scores[keep_indices]\n    final_classes = combined_classes[keep_indices]\n\n    # Scale boxes to match IMG_SIZE\n    orig_shape = yolo8x_results[0].orig_shape\n    scale_x = IMG_SIZE / orig_shape[1]\n    scale_y = IMG_SIZE / orig_shape[0]\n    final_boxes[:, [0, 2]] *= scale_x\n    final_boxes[:, [1, 3]] *= scale_y\n    final_boxes = final_boxes.clamp(min=0, max=IMG_SIZE-1)\n\n    return final_boxes, final_scores, final_classes\n\n@torch.no_grad()\ndef validate_ensemble(val_loader):\n    \"\"\"\n    Validates the ensemble model on the validation dataset.\n    \"\"\"\n    print(\"[notice] Validating ensemble model...\")\n    all_metrics = []\n\n    for images, targets in tqdm(val_loader, desc=\"Validating Ensemble\"):\n        images = torch.stack(images).to(device)\n        batch_size = len(images)\n\n        final_boxes_batch = []\n        final_scores_batch = []\n        final_classes_batch = []\n        for i in range(batch_size):\n            final_boxes, final_scores, final_classes = run_ensemble_inference(images[i].unsqueeze(0))\n            final_boxes_batch.append(final_boxes)\n            final_scores_batch.append(final_scores)\n            final_classes_batch.append(final_classes)\n\n        for i in range(batch_size):\n            gt_boxes = targets[i]['boxes'].to(device)\n            if len(gt_boxes) == 0 or len(final_boxes_batch[i]) == 0:\n                all_metrics.append({'precision': 0, 'recall': 0})\n                continue\n\n            iou_matrix = box_iou(gt_boxes, final_boxes_batch[i])\n            detected_count = 0\n\n            for gt_idx in range(len(gt_boxes)):\n                if torch.max(iou_matrix[gt_idx]) >= 0.5:\n                    detected_count += 1\n\n            true_positives = detected_count\n            predicted_positives = len(final_boxes_batch[i])\n            actual_positives = len(gt_boxes)\n\n            precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n            recall = true_positives / actual_positives if actual_positives > 0 else 0\n            all_metrics.append({'precision': precision, 'recall': recall})\n\n    if all_metrics:\n        avg_precision = np.mean([m['precision'] for m in all_metrics])\n        avg_recall = np.mean([m['recall'] for m in all_metrics])\n        print(f\"Ensemble Validation Results:\")\n        print(f\"Average Precision: {avg_precision:.4f}\")\n        print(f\"Average Recall: {avg_recall:.4f}\")\n    else:\n        print(\"No detections found. Validation metrics not calculated.\")\n\n# Run the validation on the ensembled models\nvalidate_ensemble(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T12:41:47.702160Z","iopub.execute_input":"2025-09-16T12:41:47.702390Z","iopub.status.idle":"2025-09-16T12:42:18.390314Z","shell.execute_reply.started":"2025-09-16T12:41:47.702374Z","shell.execute_reply":"2025-09-16T12:42:18.389451Z"}},"outputs":[{"name":"stdout","text":"[notice] Validating ensemble model...\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:30<00:00,  1.46s/it]","output_type":"stream"},{"name":"stdout","text":"Ensemble Validation Results:\nAverage Precision: 0.2331\nAverage Recall: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Ensemble Model Hyperparameters Tuning","metadata":{}},{"cell_type":"markdown","source":"The section consists of three main components: ensemble inference, validation, and automated optimization.\n\n**run_ensemble_inference:** This function is the heart of the system.\n - It takes a single image tensor and a set of hyperparameters (conf_thres, iou_thres) as input.\n - It runs each of the pre-loaded models (yolo8x, yolo8n, and faster_rcnn) to generate object detections. The results from each model are processed to extract the bounding boxes, confidence scores, and class labels.\n - A key part of this function is handling the different output formats of the models. The Faster R-CNN output, which is a dictionary of tensors, is converted into a single tensor with the same structure as the YOLO output ([x1, y1, x2, y2, score, class]).\n - The predictions from all three models are concatenated into a single master tensor.\n - Non-Maximum Suppression (NMS) is then applied to this combined set of predictions. NMS is a post-processing algorithm that eliminates redundant or overlapping bounding boxes that likely belong to the same object.\n - Finally, the function scales the remaining bounding boxes back to their original size and returns the final, de-duplicated set of predictions.\n\n**validate_ensemble:** This function serves as the evaluation loop.\n - It iterates through a validation dataset, fetching images and their corresponding ground truth labels.\n - For each image, it calls run_ensemble_inference to get the ensemble's predictions.\n - It then compares the predicted bounding boxes against the ground truth boxes using the Intersection over Union (IoU) metric. A prediction is considered a \"true positive\" if its IoU with a ground truth box exceeds a threshold (typically 0.5).\n - Based on these comparisons, the function calculates the Precision, Recall, and F1 Score for the batch. These metrics are then averaged across the entire validation set to provide an overall performance score.\n\n**Automated Hyperparameter Tuning with Optuna:** This section automates the optimization process.\n - An **objective** function is defined, which tells Optuna what to optimize. In this case, the objective is to maximize the F1 score.\n - The **trial** object within the function suggests new values for the hyperparameters - **conf_thres (confidence threshold)** and **iou_thres (IoU threshold)** - within specified ranges.\n - The study.optimize call runs the objective function multiple times with different hyperparameter combinations. Optuna uses intelligent search algorithms (like Bayesian optimization) to efficiently explore the parameter space, saving significant time compared to manual grid search.","metadata":{}},{"cell_type":"markdown","source":"**Understanding Automated Hyperparameter Tuning with Optuna**\n\nOptuna is an open-source hyperparameter optimization framework designed to automate the process of finding the best set of hyperparameters for a machine learning model. Unlike traditional methods like grid search or random search, Optuna uses intelligent, state-of-the-art algorithms to efficiently explore the hyperparameter space, often leading to better results in less time.\n\n**The Core Concepts of Optuna**\n\nOptuna operates on a \"define-by-run\" principle, which means you define the hyperparameter search space dynamically within your code. The entire optimization process is structured around a few key components:\n\n- **Study:** A study is the main object in Optuna. It represents a single optimization run. You can think of it as a container that holds all the information about the trials, including the objective function, the search direction (maximize or minimize), and the results.\n- **Trial:** A trial is a single execution of your machine learning training and evaluation process with a specific set of hyperparameters. Optuna runs many trials within a study, each with a different set of hyperparameters proposed by the Sampler.\n- **Objective Function:** This is the Python function that you want to optimize. It takes a trial object as its only argument and must return a single numerical value that represents the performance of the model (e.g., accuracy, F1 score, or loss). Inside this function, you define the hyperparameters to be tuned by calling methods on the trial object, such as trial.suggest_float() or trial.suggest_int().\n\n**How the Optimization Process Works**\n\nThe optimization process in Optuna follows a straightforward loop:\n\n- **Define the Objective:** You write your objective function that takes a trial object. Inside this function, you:\n  - Suggest Hyperparameters: Use trial.suggest_float(\"learning_rate\", 1e-5, 1e-2) to let Optuna know that you want to tune the learning rate within a specific range. Optuna will then dynamically choose a value for this parameter for the current trial.\n  - Train and Evaluate: Your code uses the suggested hyperparameters to train a model and evaluate its performance on a validation set.\n  - Return the Metric: The function returns the performance metric (e.g., accuracy).\n- **Create a Study:** You create a study object and specify the direction of optimization. For example, optuna.create_study(direction=\"maximize\") tells Optuna that a higher value of the objective function is better.\n- **Optimize:** You call study.optimize(objective, n_trials=100). Optuna will then execute the objective function for a specified number of trials.\n\n**Intelligent Search Algorithms (Samplers and Pruners)**\n\nWhat makes Optuna so powerful are the algorithms it uses to select hyperparameters and manage trials.\n\n- **Samplers:** These are the algorithms that propose new sets of hyperparameters for each trial. Optuna's default sampler is the Tree-structured Parzen Estimator (TPE), a Bayesian optimization algorithm.\n  - Unlike random search, which samples hyperparameter values independently, TPE builds a probabilistic model of the objective function's performance based on the results of past trials.\n  - It maintains two groups of hyperparameters: one for trials with good performance and one for trials with bad performance.\n  - It then uses these models to propose new hyperparameters that are more likely to yield a good result, intelligently focusing the search on promising regions of the parameter space.\n- **Pruners:** Pruners are a key feature for early stopping. For long-running training processes, a pruner can look at the intermediate results of a trial (e.g., accuracy after each epoch) and decide to stop it early if it's not performing well. This saves a significant amount of computational time and resources. For example, the MedianPruner stops a trial if its performance falls below the median performance of all previous trials at the same epoch.","metadata":{}},{"cell_type":"code","source":"# --- Ensemble Validation with Hyperparameter Tuning ---\n@torch.no_grad()\ndef run_ensemble_inference(image_tensor, conf_thres, iou_thres):\n    \"\"\"\n    Runs inference with YOLO and Faster R-CNN models and combines their results.\n    Accepts a PyTorch tensor with a batch dimension.\n    \"\"\"\n    # YOLOv8x inference\n    yolo8x_results = yolo8x_model.predict(image_tensor, conf=conf_thres, iou=iou_thres, verbose=False)\n    if len(yolo8x_results[0].boxes.data) == 0:\n        yolo8x_preds = torch.empty((0, 6), device=device)\n    else:\n        yolo8x_preds = yolo8x_results[0].boxes.data.to(device)\n\n    # YOLOv8n inference\n    yolo8n_results = yolo8n_model.predict(image_tensor, conf=conf_thres, iou=iou_thres, verbose=False)\n    if len(yolo8n_results[0].boxes.data) == 0:\n        yolo8n_preds = torch.empty((0, 6), device=device)\n    else:\n        yolo8n_preds = yolo8n_results[0].boxes.data.to(device)\n\n    # Faster R-CNN inference\n    faster_rcnn_image = image_tensor.to(device)\n    faster_rcnn_results = faster_rcnn_model([faster_rcnn_image[0]])\n    if len(faster_rcnn_results[0]['boxes']) == 0:\n        faster_rcnn_preds = torch.empty((0, 6), device=device)\n    else:\n        boxes = faster_rcnn_results[0]['boxes'].to(device)\n        scores = faster_rcnn_results[0]['scores'].to(device)\n        labels = faster_rcnn_results[0]['labels'].to(device)\n        mask = (labels == 1)\n        faster_rcnn_preds = torch.cat((boxes[mask], scores[mask].unsqueeze(1), torch.zeros_like(scores[mask].unsqueeze(1))), dim=1)\n\n    # Combine detections\n    combined_detections = torch.cat((yolo8x_preds, yolo8n_preds, faster_rcnn_preds), dim=0)\n\n    if combined_detections.shape[0] == 0:\n        return torch.empty((0, 4), device=device), torch.empty((0,), device=device), torch.empty((0,), device=device)\n\n    # Apply NMS\n    combined_boxes = combined_detections[:, :4]\n    combined_scores = combined_detections[:, 4]\n    combined_classes = combined_detections[:, 5]\n\n    keep_indices = nms(combined_boxes, combined_scores, iou_thres)\n    \n    final_boxes = combined_boxes[keep_indices]\n    final_scores = combined_scores[keep_indices]\n    final_classes = combined_classes[keep_indices]\n\n    # Scale boxes to match IMG_SIZE\n    orig_shape = yolo8x_results[0].orig_shape\n    scale_x = IMG_SIZE / orig_shape[1]\n    scale_y = IMG_SIZE / orig_shape[0]\n    final_boxes[:, [0, 2]] *= scale_x\n    final_boxes[:, [1, 3]] *= scale_y\n    final_boxes = final_boxes.clamp(min=0, max=IMG_SIZE-1)\n\n    return final_boxes, final_scores, final_classes\n\n@torch.no_grad()\ndef validate_ensemble(val_loader, conf_thres, iou_thres):\n    \"\"\"\n    Validates the ensemble model on the validation dataset.\n    \"\"\"\n    all_metrics = []\n\n    for images, targets in tqdm(val_loader, desc=\"Validating Ensemble\"):\n        images = torch.stack(images).to(device)\n        batch_size = len(images)\n\n        final_boxes_batch = []\n        final_scores_batch = []\n        final_classes_batch = []\n        for i in range(batch_size):\n            final_boxes, final_scores, final_classes = run_ensemble_inference(images[i].unsqueeze(0), conf_thres, iou_thres)\n            final_boxes_batch.append(final_boxes)\n            final_scores_batch.append(final_scores)\n            final_classes_batch.append(final_classes)\n\n        for i in range(batch_size):\n            gt_boxes = targets[i]['boxes'].to(device)\n            if len(gt_boxes) == 0 or len(final_boxes_batch[i]) == 0:\n                all_metrics.append({'precision': 0, 'recall': 0})\n                continue\n\n            iou_matrix = box_iou(gt_boxes, final_boxes_batch[i])\n            detected_count = 0\n\n            for gt_idx in range(len(gt_boxes)):\n                if torch.max(iou_matrix[gt_idx]) >= 0.5:\n                    detected_count += 1\n\n            true_positives = detected_count\n            predicted_positives = len(final_boxes_batch[i])\n            actual_positives = len(gt_boxes)\n\n            precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n            recall = true_positives / actual_positives if actual_positives > 0 else 0\n            all_metrics.append({'precision': precision, 'recall': recall})\n\n    if all_metrics:\n        avg_precision = np.mean([m['precision'] for m in all_metrics])\n        avg_recall = np.mean([m['recall'] for m in all_metrics])\n        f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n        print(f\"Validation Results - conf_thres={conf_thres:.2f}, iou_thres={iou_thres:.2f}:\")\n        print(f\"Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}, F1 Score: {f1_score:.4f}\")\n    else:\n        f1_score = 0\n        print(f\"Validation Results - conf_thres={conf_thres:.2f}, iou_thres={iou_thres:.2f}: No detections found, F1 Score: {f1_score:.4f}\")\n\n    return f1_score\n\ndef objective(trial):\n    \"\"\"\n    Optuna objective function to tune hyperparameters.\n    \"\"\"\n    conf_thres = trial.suggest_float(\"conf_thres\", 0.1, 0.9)\n    iou_thres = trial.suggest_float(\"iou_thres\", 0.3, 0.7)\n    \n    f1_score = validate_ensemble(val_loader, conf_thres, iou_thres)\n    return f1_score\n\n# Create and run Optuna study\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\n# Print best parameters and F1 score\nbest_params = study.best_params\nbest_f1 = study.best_value\nprint(f\"Best hyperparameters: {best_params}\")\nprint(f\"Best F1 Score: {best_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T12:51:27.953836Z","iopub.execute_input":"2025-09-16T12:51:27.954203Z","iopub.status.idle":"2025-09-16T13:15:45.075308Z","shell.execute_reply.started":"2025-09-16T12:51:27.954174Z","shell.execute_reply":"2025-09-16T13:15:45.074260Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[I 2025-09-16 12:51:27,969] A new study created in memory with name: no-name-b4543424-49b8-4d34-a23f-5836bc841818\nValidating Ensemble: 100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n[I 2025-09-16 12:51:55,188] Trial 0 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7755666168334344, 'iou_thres': 0.6295414261459875}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.78, iou_thres=0.63:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:27<00:00,  1.31s/it]\n[I 2025-09-16 12:52:22,609] Trial 1 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.2969795417892887, 'iou_thres': 0.4991268810744195}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.30, iou_thres=0.50:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:27<00:00,  1.32s/it]\n[I 2025-09-16 12:52:50,382] Trial 2 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.3339626316555889, 'iou_thres': 0.6369047024342465}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.33, iou_thres=0.64:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.34s/it]\n[I 2025-09-16 12:53:18,516] Trial 3 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.2668385314395314, 'iou_thres': 0.5010290118856199}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.27, iou_thres=0.50:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.35s/it]\n[I 2025-09-16 12:53:46,972] Trial 4 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7702485898984038, 'iou_thres': 0.3057966139151117}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.77, iou_thres=0.31:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.36s/it]\n[I 2025-09-16 12:54:15,509] Trial 5 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.6063265558233624, 'iou_thres': 0.5425224392241373}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.61, iou_thres=0.54:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it]\n[I 2025-09-16 12:54:44,265] Trial 6 finished with value: 0.23158493479055783 and parameters: {'conf_thres': 0.17918164945406634, 'iou_thres': 0.5535546733160253}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.18, iou_thres=0.55:\nAverage Precision: 0.2301, Average Recall: 0.2331, F1 Score: 0.2316\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it]\n[I 2025-09-16 12:55:13,110] Trial 7 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.48992242897882876, 'iou_thres': 0.3120360903792454}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.49, iou_thres=0.31:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.37s/it]\n[I 2025-09-16 12:55:41,909] Trial 8 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.2511971975770768, 'iou_thres': 0.44675856392378643}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.25, iou_thres=0.45:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.38s/it]\n[I 2025-09-16 12:56:10,904] Trial 9 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.6622694639056872, 'iou_thres': 0.39966391353057484}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.66, iou_thres=0.40:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:28<00:00,  1.38s/it]\n[I 2025-09-16 12:56:39,922] Trial 10 finished with value: 0.22699386503067484 and parameters: {'conf_thres': 0.883403580598634, 'iou_thres': 0.699439295203502}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.88, iou_thres=0.70:\nAverage Precision: 0.2270, Average Recall: 0.2270, F1 Score: 0.2270\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 12:57:09,262] Trial 11 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.42944724715536026, 'iou_thres': 0.619423458262578}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.43, iou_thres=0.62:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 12:57:38,639] Trial 12 finished with value: 0.22085889570552147 and parameters: {'conf_thres': 0.8995531334755222, 'iou_thres': 0.41658509468124155}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.90, iou_thres=0.42:\nAverage Precision: 0.2209, Average Recall: 0.2209, F1 Score: 0.2209\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 12:58:08,118] Trial 13 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.6139655582036667, 'iou_thres': 0.5857209215264755}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.61, iou_thres=0.59:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 12:58:37,528] Trial 14 finished with value: 0.2300204498977505 and parameters: {'conf_thres': 0.13096889978941184, 'iou_thres': 0.4812512757155751}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.13, iou_thres=0.48:\nAverage Precision: 0.2270, Average Recall: 0.2331, F1 Score: 0.2300\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 12:59:06,809] Trial 15 finished with value: 0.23158493479055783 and parameters: {'conf_thres': 0.3975329971889463, 'iou_thres': 0.6979188299490119}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.40, iou_thres=0.70:\nAverage Precision: 0.2301, Average Recall: 0.2331, F1 Score: 0.2316\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 12:59:36,109] Trial 16 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7515758916675244, 'iou_thres': 0.5148175013507363}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.75, iou_thres=0.51:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:00:05,551] Trial 17 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.5068518890803902, 'iou_thres': 0.626327741378838}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.51, iou_thres=0.63:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.41s/it]\n[I 2025-09-16 13:00:35,183] Trial 18 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7728912581470555, 'iou_thres': 0.37907723114253034}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.77, iou_thres=0.38:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:01:04,663] Trial 19 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.31657514316442115, 'iou_thres': 0.46181115487199215}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.32, iou_thres=0.46:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:01:34,092] Trial 20 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.5168145646694251, 'iou_thres': 0.580639355317434}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.52, iou_thres=0.58:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:02:03,551] Trial 21 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.35557440580160304, 'iou_thres': 0.6535949517940864}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.36, iou_thres=0.65:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:02:33,049] Trial 22 finished with value: 0.2300204498977505 and parameters: {'conf_thres': 0.22111503364002832, 'iou_thres': 0.6615607562511341}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.22, iou_thres=0.66:\nAverage Precision: 0.2270, Average Recall: 0.2331, F1 Score: 0.2300\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:03:02,403] Trial 23 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.3168653485744751, 'iou_thres': 0.5977962401996536}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.32, iou_thres=0.60:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:03:31,584] Trial 24 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.42980596772033175, 'iou_thres': 0.5531801786703071}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.43, iou_thres=0.55:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:04:00,736] Trial 25 finished with value: 0.21903267227849907 and parameters: {'conf_thres': 0.10016648034316336, 'iou_thres': 0.6597416828942637}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.10, iou_thres=0.66:\nAverage Precision: 0.2065, Average Recall: 0.2331, F1 Score: 0.2190\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:04:30,005] Trial 26 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.3404378885259428, 'iou_thres': 0.5286456838064951}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.34, iou_thres=0.53:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:04:59,329] Trial 27 finished with value: 0.22843496520772427 and parameters: {'conf_thres': 0.1765484079180215, 'iou_thres': 0.6106260122622325}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.18, iou_thres=0.61:\nAverage Precision: 0.2239, Average Recall: 0.2331, F1 Score: 0.2284\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:05:28,661] Trial 28 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.5627488324898824, 'iou_thres': 0.6451295277059972}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.56, iou_thres=0.65:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:05:58,074] Trial 29 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.2784897954041582, 'iou_thres': 0.49454145883267636}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.28, iou_thres=0.49:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:06:27,346] Trial 30 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7001854882004066, 'iou_thres': 0.5688750533078639}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.70, iou_thres=0.57:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:06:56,588] Trial 31 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.25294230351119557, 'iou_thres': 0.5098970716373962}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.25, iou_thres=0.51:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:07:25,938] Trial 32 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.39130549869294073, 'iou_thres': 0.4683792220171365}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.39, iou_thres=0.47:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:07:55,148] Trial 33 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.8211847726135847, 'iou_thres': 0.3446953602456799}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.82, iou_thres=0.34:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:08:24,371] Trial 34 finished with value: 0.23158493479055783 and parameters: {'conf_thres': 0.20083817184353564, 'iou_thres': 0.42901668972480933}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.20, iou_thres=0.43:\nAverage Precision: 0.2301, Average Recall: 0.2331, F1 Score: 0.2316\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:08:53,629] Trial 35 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.28471997883137923, 'iou_thres': 0.5450981557296883}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.28, iou_thres=0.55:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:09:22,998] Trial 36 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.445097579136029, 'iou_thres': 0.6816222709054575}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.45, iou_thres=0.68:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:09:52,492] Trial 37 finished with value: 0.23158493479055783 and parameters: {'conf_thres': 0.16124202385874709, 'iou_thres': 0.5307068741181957}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.16, iou_thres=0.53:\nAverage Precision: 0.2301, Average Recall: 0.2331, F1 Score: 0.2316\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:10:21,922] Trial 38 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.22938971776994954, 'iou_thres': 0.635673631664164}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.23, iou_thres=0.64:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:10:51,371] Trial 39 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.35860625921993244, 'iou_thres': 0.4492990460999965}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.36, iou_thres=0.45:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:11:20,728] Trial 40 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.6270787821608328, 'iou_thres': 0.3764743250194936}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.63, iou_thres=0.38:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:11:50,068] Trial 41 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.8045341495684214, 'iou_thres': 0.3082433715955938}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.80, iou_thres=0.31:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:12:19,302] Trial 42 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7071452876713807, 'iou_thres': 0.35318523780640965}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.71, iou_thres=0.35:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n[I 2025-09-16 13:12:48,597] Trial 43 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.6730750251164458, 'iou_thres': 0.40637625518583}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.67, iou_thres=0.41:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:13:17,939] Trial 44 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.8573480271869985, 'iou_thres': 0.6087096601791255}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.86, iou_thres=0.61:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:13:47,332] Trial 45 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.5486360021884888, 'iou_thres': 0.3319033846390224}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.55, iou_thres=0.33:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:14:16,737] Trial 46 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.7615684648157964, 'iou_thres': 0.5676014821215317}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.76, iou_thres=0.57:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:14:46,144] Trial 47 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.46892370892737023, 'iou_thres': 0.6762661611771166}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.47, iou_thres=0.68:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:15:15,581] Trial 48 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.8659936605146046, 'iou_thres': 0.427177050942485}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.87, iou_thres=0.43:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\n","output_type":"stream"},{"name":"stderr","text":"Validating Ensemble: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it]\n[I 2025-09-16 13:15:45,069] Trial 49 finished with value: 0.2331288343558282 and parameters: {'conf_thres': 0.386831927849308, 'iou_thres': 0.48909870442432707}. Best is trial 0 with value: 0.2331288343558282.\n","output_type":"stream"},{"name":"stdout","text":"Validation Results - conf_thres=0.39, iou_thres=0.49:\nAverage Precision: 0.2331, Average Recall: 0.2331, F1 Score: 0.2331\nBest hyperparameters: {'conf_thres': 0.7755666168334344, 'iou_thres': 0.6295414261459875}\nBest F1 Score: 0.2331\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Ensemble Inference and Submission Generation","metadata":{}},{"cell_type":"markdown","source":"This final phase of the pipeline takes the trained models and combines their predictions to create a single, robust set of detections. This output is then formatted for submission.\n\n**Ensemble Methodology:**\n\n- **Loading Trained Models:** The pipeline begins by loading the best-performing models saved during the training phase. The yolo8x and yolo8n models are loaded from their respective best.pt files, while the faster_rcnn model's state dictionary is loaded into its architecture. The faster_rcnn model is also set to evaluation mode (.eval()), which is crucial for inference as it disables training-specific layers like dropout.\n- **Batch Prediction Loop:** The code iterates through each image in the test set. For each image, it runs a separate inference pass on all three models (YOLOv8x, YOLOv8n, and Faster R-CNN) to get individual sets of bounding box predictions. The image is preprocessed and scaled to the correct size (IMG_SIZE) before being passed to the models.\n- **Prediction Normalization:** A crucial step is the normalization of the bounding box coordinates. The raw coordinates from the models are converted to a [0, 1] range relative to the original image dimensions. This ensures that the fusion algorithm works with a consistent coordinate system.\n- **Weighted Boxes Fusion (WBF):** This is the core of the ensemble process. The weighted_boxes_fusion function is called with the lists of predictions from all three models. It intelligently merges overlapping boxes, giving a final set of detections with higher confidence and accuracy. A list of weights is provided to give more importance to the models that are expected to perform better (e.g., YOLOv8x with a weight of 1.0).\n- **Saving Predictions:** The fused predictions are then formatted and saved to individual .txt files in a format compatible with the YOLO specification. Each line in the file contains the class ID, confidence score, and normalized bounding box coordinates in the format of x_center, y_center, width, and height.\n- **CSV Submission Generation:** The predictions_to_csv function reads all the generated .txt files and compiles them into a single submission.csv file. This is the required format for competition submissions. The function ensures that every image in the test set is accounted for, adding \"no boxes\" for any images without detections.\n\n**Hyperparameters:**\n\nThe inference process is controlled by a few key hyperparameters.\n\n- **CONF_THRESHOLD:** The confidence threshold, which filters out any detections with a confidence score below this value before they are passed to the WBF algorithm.\n- **IOU_THRESHOLD:** The Intersection over Union (IoU) threshold used by the WBF algorithm. It determines how much overlap is required for boxes to be considered for fusion.\n- **weights:** A list of floats that assigns a weight to each model's predictions. These weights influence how much each model contributes to the final fused bounding box.\n\n**Theories and Concepts:**\n\n- **Weighted Boxes Fusion (WBF):** A fundamental post-processing algorithm for model ensembling. Unlike simpler methods like Non-Maximum Suppression (NMS), which can discard good predictions, WBF combines the coordinates and confidence scores of overlapping boxes from different models, producing a single, more reliable bounding box.\n- **Ensemble Learning:** This code demonstrates the power of ensemble learning, where combining the strengths of different models (e.g., YOLO's speed and Faster R-CNN's accuracy) leads to a more robust and higher-performing final system.\n- **Inference:** The process of using a trained model to make predictions on new, unseen data. In this context, the entire pipeline from loading the models to generating the final CSV is an end-to-end inference workflow.\n- **YOLO Format:** The output format used for the prediction .txt files is a common standard in object detection, making it easy to share and use the results with other tools. The format represents a bounding box by its center coordinates, width, and height, all normalized to the range of 0 to 1.","metadata":{}},{"cell_type":"markdown","source":"## Load and Prepared Models for Ensemble","metadata":{}},{"cell_type":"code","source":"# --- Load and Prepare Models for Ensemble ---\nprint(\"[notice] Loading trained models for ensemble...\")\nyolo8x_model = YOLO(\"/kaggle/working/runs/detect/yolo8n_trained2/weights/best.pt\")  # Fixed path\nyolo8n_model = YOLO(\"/kaggle/working/runs/detect/yolo8x_trained/weights/best.pt\")\nfaster_rcnn_model.load_state_dict(torch.load(\"/kaggle/working/faster_rcnn_best.pt\"))\nfaster_rcnn_model.to(device)\nfaster_rcnn_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T12:47:21.102607Z","iopub.execute_input":"2025-09-16T12:47:21.102929Z","iopub.status.idle":"2025-09-16T12:47:21.494753Z","shell.execute_reply.started":"2025-09-16T12:47:21.102906Z","shell.execute_reply":"2025-09-16T12:47:21.494020Z"}},"outputs":[{"name":"stdout","text":"[notice] Loading trained models for ensemble...\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Ensemble Inference","metadata":{}},{"cell_type":"code","source":"CONF_THRESHOLD = 0.7755666168334344\nIOU_THRESHOLD = 0.6295414261459875\n\n# --- Ensemble Inference ---\ntest_images_path = f\"{base_path}/testImages/images\"\noutput_dir = \"/kaggle/working/predictions/labels\"\nos.makedirs(output_dir, exist_ok=True)\n\ntest_transforms = T.Compose([\n    T.ToImage(),\n    T.ToDtype(torch.float32, scale=True),\n    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n])\n\nfor img_path in tqdm(list(Path(test_images_path).glob(\"*\")), desc=\"Predicting\"):\n    if img_path.suffix.lower() not in ['.png', '.jpg', '.jpeg']:\n        continue\n    \n    img_name = img_path.stem\n    img = Image.open(img_path).convert(\"RGB\")\n    img_width, img_height = img.size\n    \n    # Preprocess image\n    img_tensor = test_transforms(img).unsqueeze(0).to(device)  # Add batch dimension\n    \n    # YOLOv8x predictions\n    yolo8x_boxes = []\n    yolo8x_scores = []\n    yolo8x_labels = []\n    try:\n        yolo8x_results = yolo8x_model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)\n        for result in yolo8x_results:\n            boxes = result.boxes.data\n            if boxes is not None:\n                for box in boxes:\n                    x1, y1, x2, y2, conf, cls_id = box.tolist()\n                    if conf >= CONF_THRESHOLD:\n                        # Normalize to [0, 1] relative to original image size\n                        yolo8x_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n                        yolo8x_scores.append(conf)\n                        yolo8x_labels.append(int(cls_id))\n    except Exception as e:\n        print(f\"[warning] YOLOv8x prediction failed for {img_name}: {e}\")\n    \n    # YOLOv8n predictions\n    yolo8n_boxes = []\n    yolo8n_scores = []\n    yolo8n_labels = []\n    try:\n        yolo8n_results = yolo8n_model.predict(img_path, conf=CONF_THRESHOLD, verbose=False)\n        for result in yolo8n_results:\n            boxes = result.boxes.data\n            if boxes is not None:\n                for box in boxes:\n                    x1, y1, x2, y2, conf, cls_id = box.tolist()\n                    if conf >= CONF_THRESHOLD:\n                        yolo8n_boxes.append([x1/img_width, y1/img_height, x2/img_width, y2/img_height])\n                        yolo8n_scores.append(conf)\n                        yolo8n_labels.append(int(cls_id))\n    except Exception as e:\n        print(f\"[warning] YOLOv8n prediction failed for {img_name}: {e}\")\n    \n    # Faster R-CNN predictions\n    frcnn_boxes = []\n    frcnn_scores = []\n    frcnn_labels = []\n    try:\n        with torch.no_grad():\n            predictions = faster_rcnn_model([img_tensor[0]])[0]\n            for box, score, label in zip(predictions['boxes'], predictions['scores'], predictions['labels']):\n                if score >= CONF_THRESHOLD and label == 1:  # Class 1 is object\n                    x1, y1, x2, y2 = box.tolist()\n                    # Scale boxes to original image size and normalize\n                    scale_x = img_width / IMG_SIZE\n                    scale_y = img_height / IMG_SIZE\n                    x1, x2 = x1 * scale_x / img_width, x2 * scale_x / img_width\n                    y1, y2 = y1 * scale_y / img_height, y2 * scale_y / img_height\n                    frcnn_boxes.append([x1, y1, x2, y2])\n                    frcnn_scores.append(score.item())\n                    frcnn_labels.append(0)  # Map to class 0 for consistency\n    except Exception as e:\n        print(f\"[warning] Faster R-CNN prediction failed for {img_name}: {e}\")\n    \n    # Ensemble using Weighted Boxes Fusion\n    boxes_list = [yolo8x_boxes, yolo8n_boxes, frcnn_boxes]\n    scores_list = [yolo8x_scores, yolo8n_scores, frcnn_scores]\n    labels_list = [yolo8x_labels, yolo8n_labels, frcnn_labels]\n    weights = [1.0, 0.9, 0.8]  # YOLOv8x > YOLOv8n > Faster R-CNN\n    \n    try:\n        fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n            boxes_list,\n            scores_list,\n            labels_list,\n            weights=weights,\n            iou_thr=IOU_THRESHOLD,\n            skip_box_thr=CONF_THRESHOLD\n        )\n    except Exception as e:\n        print(f\"[warning] WBF failed for {img_name}: {e}\")\n        fused_boxes, fused_scores, fused_labels = [], [], []\n    \n    # Save ensemble predictions in YOLO format\n    output_txt = Path(output_dir) / f\"{img_name}.txt\"\n    with open(output_txt, \"w\") as f:\n        for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n            x1, y1, x2, y2 = box\n            x_center = (x1 + x2) / 2\n            y_center = (y1 + y2) / 2\n            width = x2 - x1\n            height = y2 - y1\n            f.write(f\"{int(label)} {score:.6f} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n\nprint(f\"[notice] All ensemble detections saved to: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T13:21:43.131187Z","iopub.execute_input":"2025-09-16T13:21:43.131532Z","iopub.status.idle":"2025-09-16T13:23:31.636511Z","shell.execute_reply.started":"2025-09-16T13:21:43.131509Z","shell.execute_reply":"2025-09-16T13:23:31.635584Z"}},"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 159/159 [01:48<00:00,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"[notice] All ensemble detections saved to: /kaggle/working/predictions/labels\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Submission File Generation","metadata":{}},{"cell_type":"code","source":"# --- Convert Predictions to Submission CSV ---\ndef predictions_to_csv(\n    preds_folder: str = \"/kaggle/working/predictions/labels\",\n    output_csv: str = \"/kaggle/working/submission.csv\",\n    test_images_folder: str = \"/kaggle/input/synthetic-2-real-object-detection-challenge-2/Synthetic to Real Object Detection Challenge 2/testImages/images\",\n    allowed_extensions: tuple = (\".jpg\", \".png\", \".jpeg\")\n):\n    preds_path = Path(preds_folder)\n    test_images_path = Path(test_images_folder)\n    \n    test_images = {p.stem for p in test_images_path.glob(\"*\") if p.suffix.lower() in allowed_extensions}\n    predictions = []\n    predicted_images = set()\n    \n    for txt_file in preds_path.glob(\"*.txt\"):\n        image_id = txt_file.stem\n        predicted_images.add(image_id)\n        with open(txt_file, \"r\") as f:\n            valid_lines = [line.strip() for line in f if len(line.strip().split()) == 6]\n        pred_str = \" \".join(valid_lines) if valid_lines else \"no boxes\"\n        predictions.append({\"image_id\": image_id, \"prediction_string\": pred_str})\n    \n    missing_images = test_images - predicted_images\n    for image_id in missing_images:\n        predictions.append({\"image_id\": image_id, \"prediction_string\": \"no boxes\"})\n    \n    submission_df = pd.DataFrame(predictions)\n    submission_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_MINIMAL)\n    print(f\"[notice] Submission saved to {output_csv}\")\n\npredictions_to_csv()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T13:23:31.637828Z","iopub.execute_input":"2025-09-16T13:23:31.638107Z","iopub.status.idle":"2025-09-16T13:23:31.655937Z","shell.execute_reply.started":"2025-09-16T13:23:31.638086Z","shell.execute_reply":"2025-09-16T13:23:31.655114Z"}},"outputs":[{"name":"stdout","text":"[notice] Submission saved to /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":23}]}