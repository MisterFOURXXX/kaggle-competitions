{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"},{"sourceId":494959,"sourceType":"modelInstanceVersion","modelInstanceId":393014,"modelId":411634},{"sourceId":494960,"sourceType":"modelInstanceVersion","modelInstanceId":393015,"modelId":411635},{"sourceId":494963,"sourceType":"modelInstanceVersion","modelInstanceId":393017,"modelId":411637},{"sourceId":495224,"sourceType":"modelInstanceVersion","modelInstanceId":393175,"modelId":411779}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition overview","metadata":{}},{"cell_type":"markdown","source":"**Competition Summary**\n\n- This is the Rohlik Sales Forecasting Challenge, a time-series forecasting competition hosted on Kaggle. The primary objective is to predict the sales volume for various inventory items across 11 different Rohlik Group warehouses for a period of 14 days.\n\n- Accurate forecasts are vital for the e-grocery company's operations, as they directly impact supply chain efficiency, inventory management, and overall sustainability by minimizing waste.\n\n- The model's performance will be evaluated using the Weighted Mean Absolute Error (WMAE). The specific weights for each inventory item are provided in a separate file. The competition runs from November 15, 2024, to February 15, 2025, and offers cash prizes for the top three competitors.\n\n- The dataset includes historical sales and order data, product metadata, and a calendar with holiday information. Some features available in the training set (e.g., sales and availability) are intentionally removed from the test set, as they would not be known at the time of a real-world prediction.","metadata":{}},{"cell_type":"markdown","source":"### [Link to competition](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)","metadata":{}},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"This data dictionary describes the files and columns provided for the competition.","metadata":{}},{"cell_type":"markdown","source":"**sales_train.csv and sales_test.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a specific inventory item in a specific warehouse.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">date</td>\n    <td class=\"tg-7zrl\">The date of the sales record.</td>\n    <td class=\"tg-7zrl\">Date</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse where the item is stored.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">total_orders</td>\n    <td class=\"tg-7zrl\">The historical number of orders for the selected warehouse.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sales</td>\n    <td class=\"tg-7zrl\">The target variable: sales volume (pcs or kg).</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sell_price_main</td>\n    <td class=\"tg-7zrl\">The selling price of the item.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">availability</td>\n    <td class=\"tg-7zrl\">The proportion of the day the item was available. A value of 1 means it was available all day.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">type_0_discount, type_1_discount, etc.</td>\n    <td class=\"tg-7zrl\">The percentage discount offered for various promotion types. Negative values indicate no discount.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**inventory.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a specific inventory item.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">product_unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a product, shared across all warehouses.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">name</td>\n    <td class=\"tg-7zrl\">The name of the product.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">L1_category_name, L2_category_name, etc.</td>\n    <td class=\"tg-7zrl\">Hierarchical category names for the product. L4 is the most granular.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse where the inventory item is located.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**calendar.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">date</td>\n    <td class=\"tg-7zrl\">The date of the calendar event.</td>\n    <td class=\"tg-7zrl\">Date</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">holiday_name</td>\n    <td class=\"tg-7zrl\">The name of the public holiday, if applicable.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">holiday</td>\n    <td class=\"tg-7zrl\">A binary flag (0 or 1) indicating if the date is a holiday.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">shops_closed</td>\n    <td class=\"tg-7zrl\">A flag indicating a public holiday where most shops are closed.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">winter_school_holidays</td>\n    <td class=\"tg-7zrl\">A flag for winter school holidays.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">school_holidays</td>\n    <td class=\"tg-7zrl\">A flag for general school holidays.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**test_weights.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for the inventory item.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">weight</td>\n    <td class=\"tg-7zrl\">The weight used for calculating the Weighted Mean Absolute Error (WMAE) metric for this item.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"### [Link to dataset](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/data)","metadata":{}},{"cell_type":"markdown","source":"# Submission Pipeline Overview","metadata":{}},{"cell_type":"markdown","source":"**Data Preparation**\n\nThe process begins by loading the test data along with inventory, weights, and calendar information. The calendar data is enriched with custom holidays and derived features like days to next holiday. The datasets are then merged and sorted chronologically to maintain temporal integrity.\n\nThe data is then transformed to prepare it for the model. First, a noise reduction step removes low-relevance columns, followed by a preprocessing function that handles categorical encoding and numerical scaling. This function uses pre-trained `OrdinalEncoder` and `StandardScaler` objects, which ensures the test data is transformed consistently with how the training data was processed. Finally, the preprocessed data is formatted into time-series sequences of length 100, which is the required input format for the model.\n\n**Model Loading**\n\nAfter the data is prepared, the pipeline loads a pre-trained Keras model from a `.keras` file. This model uses a hybrid architecture combining WaveNet and Transformer components.\n\nThe WaveNet model, consisting of multiple Conv1D and BatchNormalization layers, is designed to capture temporal dependencies in the data. The Transformer component, identified by its MultiHeadAttention and LayerNormalization layers, helps the model understand long-range relationships between data points in the time series. The model also includes custom loss and metric functions, `custom_wmae_loss` and `WeightedMAEMetric`, which are essential for its specific forecasting task.\n\n**Prediction Generation**\n\nThe process uses the previously loaded WaveNet and Transformer hybrid model to generate sales predictions for the test data. It iterates through the pre-formatted dataset_submission in batches, calling the model.predict method for each batch. The predictions from all batches are then concatenated into a single NumPy array.\n\n**Post-processing and Submission**\n\nA crucial step is to convert the scaled predictions back into their original format. The pipeline loads the `scaler_y` object, which was used to scale the target sales data during the model's training, and applies its `inverse_transform` method to the predictions. This returns the forecast values to their unscaled, interpretable state.\n\nFinally, the unscaled predictions are prepared for submission. A new `sales_hat` column is added to the original submission dataframe, and a unique id column is created by combining the unique_id and date. The final DataFrame is then saved to a `submission.csv` file, containing only the required id and `sales_hat` columns.","metadata":{}},{"cell_type":"markdown","source":"# Related Notebooks","metadata":{}},{"cell_type":"markdown","source":"### [Model training notebook](https://www.kaggle.com/code/misterfour/rohik-sales-forecasting-challenge)\n### [Reference! (add holidays calendar of each country into dataset)](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:28:52.572793Z","iopub.execute_input":"2025-09-21T09:28:52.573305Z","iopub.status.idle":"2025-09-21T09:28:52.838964Z","shell.execute_reply.started":"2025-09-21T09:28:52.573279Z","shell.execute_reply":"2025-09-21T09:28:52.838375Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rohik_scaler_v2/scikitlearn/default/1/scaler.pkl\n/kaggle/input/rohik_encoder_v2/scikitlearn/default/1/encoder.pkl\n/kaggle/input/rohik_scaler_y_v2/scikitlearn/default/1/scaler_y.pkl\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\n/kaggle/input/wavenet_transformer_model.keras/keras/default/1/wavenet_transformer_model.keras\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install TensorFlow and Keras\n!pip install tensorflow==2.15.0\n!pip install keras==2.15.0\n!pip install scikit-learn==1.2.2\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nimport joblib\nfrom tensorflow.keras.layers import Input, Conv1D, Multiply, Add, Dense, Dropout, LayerNormalization\nfrom tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Activation, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Metric\nfrom sklearn.preprocessing import OrdinalEncoder\nimport joblib\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import Parallel, delayed\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nimport joblib\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, Add, Activation, Dense, Dropout,\n    BatchNormalization, GlobalAveragePooling1D, Multiply, LayerNormalization\n)\nfrom tensorflow.keras.layers import MultiHeadAttention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:28:52.850013Z","iopub.execute_input":"2025-09-21T09:28:52.850363Z","iopub.status.idle":"2025-09-21T09:30:01.535151Z","shell.execute_reply.started":"2025-09-21T09:28:52.850344Z","shell.execute_reply":"2025-09-21T09:30:01.534438Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.15.0\n  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.14.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\nCollecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.0)\nCollecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n  Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.73.1)\nCollecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\nCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2.4.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.40.3)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.4)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.15.0) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.1)\nDownloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, keras, tensorboard, ml-dtypes, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.17.2\n    Uninstalling wrapt-1.17.2:\n      Successfully uninstalled wrapt-1.17.2\n  Attempting uninstall: keras\n    Found existing installation: keras 3.8.0\n    Uninstalling keras-3.8.0:\n      Successfully uninstalled keras-3.8.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.18.0\n    Uninstalling tensorboard-2.18.0:\n      Successfully uninstalled tensorboard-2.18.0\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.4.1\n    Uninstalling ml-dtypes-0.4.1:\n      Successfully uninstalled ml-dtypes-0.4.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.18.0\n    Uninstalling tensorflow-2.18.0:\n      Successfully uninstalled tensorflow-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\ntensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\ntensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.2\nRequirement already satisfied: keras==2.15.0 in /usr/local/lib/python3.11/dist-packages (2.15.0)\nRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"2025-09-21 09:29:55.272770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-09-21 09:29:55.272826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-09-21 09:29:55.274566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load Dataset and Preprocess Dataset","metadata":{}},{"cell_type":"markdown","source":"The core purpose of this script is to prepare the submission data (`sales_test.csv`) to match the exact format of the training data, so it can be fed into the pre-trained WaveNet-Transformer model for prediction.\n\n**Step-by-Step Breakdown of the Code**\n\n**1. Data Merging and Feature Engineering**\n\nThe code starts by loading several `datasets—sales_test.csv, inventory.csv, calendar.csv, and test_weights.csv` and merges them into a single `df_submission` dataframe. It also includes functions like `fill_loss_holidays` and enrich_calendar to add new, potentially predictive features.\n\nFeature engineering is the process of using domain knowledge to create new features that are not explicitly present in the original dataset. The goal is to improve the performance of a machine learning model by giving it more relevant information. In this code, combining calendar data with sales data and creating features like `date_days_to_next_holiday` provides the model with crucial context about temporal patterns that would be difficult to learn from raw data alone.\n\n**2. Missing Value Imputation**\n\nAfter merging the data, the code iterates through all numerical columns and fills any missing values (NaN) with the mean of that column.\n\n**Theory: Data Imputation**\n\nThis step is a practical necessity for data integrity. Machine learning models, particularly deep neural networks, generally require complete data and cannot handle missing values. Imputation is the process of filling these gaps. Using the mean is a simple and common strategy that maintains the central tendency of the data.\n\n**3. Feature Selection (Dimensionality Reduction)**\n\nThe code drops a predefined list of \"noise columns\" (`columns_to_drop`). This is the part of the script that directly relates to the user's question about correlation analysis. The list of columns to drop is likely the result of a previous analysis that identified features as having low predictive power, or being highly correlated with other features.\n\n**Theory: The Curse of Dimensionality**\n\nDropping features is a form of dimensionality reduction. Including too many features can lead to the \"curse of dimensionality,\" making models slower to train and more prone to overfitting. Features are typically dropped if they have:\n\n- **Low Variance:** They don't change much, so they provide little predictive information.\n- **High Correlation:** They are redundant with other features, and a model only needs one of them.\n- **Low Importance:** A feature importance analysis (such as from a random forest or a linear model) shows they have a negligible impact on the target variable.\n\n**4. Data Transformation and Scaling**\n\nThe `preprocess_data` function is used to transform the data to match the format expected by the model. This is a crucial step that loads the same `OrdinalEncoder` and `StandardScaler` objects used during the training phase.\n\n**Theory: Preventing Data Leakage**\n\nThis is one of the most important concepts in the entire pipeline. It is paramount to use the same transformations on the test data that were fitted on the training data.  If a new StandardScaler were fit on the test data, it would learn the mean and standard deviation of the test set, which is a form of data leakage from the future. This would lead to unrealistically optimistic performance metrics.\n\n- **Ordinal Encoding:** Converts categorical features (like warehouse names) into numerical values that the model can process.\n- **Standard Scaling:** Normalizes numerical features by transforming them to have a mean of 0 and a standard deviation of 1. This is essential for neural networks to ensure no single feature's magnitude dominates the learning process.\n\n**5. Sequence Creation and Padding**\n\nThe final steps prepare the data into the sequence format required by the WaveNet-Transformer model. The x_submission data is first padded with zeros to ensure it's long enough to create full sequences. Then, `tf.keras.utils.timeseries_dataset_from_array` is used to convert the flat data into a batched, sequential dataset.\n\n**Theory: Time-Series Sequence Modeling**\n\nUnlike traditional models that look at one row at a time, a time-series model like a recurrent neural network or a Transformer requires sequences of data to make predictions. The `SEQUENCE_LENGTH` hyperparameter (100) dictates how many past time steps the model will use to predict the next value. Padding is necessary for the initial data points in the test set, where there isn't enough historical context to form a full sequence. The `timeseries_dataset_from_array` function efficiently handles this process of creating overlapping sequences and prepares the data for the model's input layer.","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Hyperparameters (must match training pipeline)\nSEQUENCE_LENGTH = 100\nBATCH_SIZE = 256\n\n# Additional holiday days (unchanged)\nczech_holiday = [\n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"),\n]\nbrno_holiday = [\n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"),\n]\nmunich_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),\n]\nfrankfurt_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),\n]\n\n# Functions (unchanged)\ndef fill_loss_holidays(df_fill, warehouses, holidays):\n    df = df_fill.copy()\n    for item in holidays:\n        dates, holiday_name = item\n        generated_dates = [pd.to_datetime(date, format='%m/%d/%Y').strftime('%Y-%m-%d') for date in dates]\n        for generated_date in generated_dates:\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday'] = 1\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday_name'] = holiday_name\n    return df\n\ndef enrich_calendar(df):\n    df = df.sort_values('date').reset_index(drop=True)\n    df['next_holiday_date'] = df.loc[df['holiday'] == 1, 'date'].shift(-1)\n    df['next_holiday_date'] = df['next_holiday_date'].bfill()\n    df['date_days_to_next_holiday'] = (df['next_holiday_date'] - df['date']).dt.days\n    df.drop(columns=['next_holiday_date'], inplace=True)\n    df['next_shops_closed_date'] = df.loc[df['shops_closed'] == 1, 'date'].shift(-1)\n    df['next_shops_closed_date'] = df['next_shops_closed_date'].bfill()\n    df['date_days_to_shops_closed'] = (df['next_shops_closed_date'] - df['date']).dt.days\n    df.drop(columns=['next_shops_closed_date'], inplace=True)\n    df['date_day_after_closed_day'] = ((df['shops_closed'] == 0) & (df['shops_closed'].shift(1) == 1)).astype(int)\n    df['date_second_closed_day'] = ((df['shops_closed'] == 1) & (df['shops_closed'].shift(1) == 1)).astype(int)\n    df['date_day_after_two_closed_days'] = ((df['shops_closed'] == 0) & (df['date_second_closed_day'].shift(1) == 1)).astype(int)\n    return df\n\ndef stack_datasets(df, calendar_extended, inventory, weights):\n    df = df.merge(calendar_extended, on=['date', 'warehouse'], how='left')\n    df = df.merge(inventory, on=['unique_id', 'warehouse'], how='left')\n    df = df.merge(weights, on='unique_id', how='left')\n    df['date'] = pd.to_datetime(df['date'])\n    return df\n\ndef sort_dataframe_by_date(df, date_column):\n    df[date_column] = pd.to_datetime(df[date_column])\n    df = df.sort_values(by=date_column)\n    return df\n\ndef encode_datetime(df, datetime_column):\n    df[datetime_column] = pd.to_datetime(df[datetime_column])\n    df['year'] = df[datetime_column].dt.year\n    df['month'] = df[datetime_column].dt.month\n    df['day'] = df[datetime_column].dt.day\n    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n    df['sin_day'] = np.sin(2 * np.pi * df['day'] / 31)\n    df['cos_day'] = np.cos(2 * np.pi * df['day'] / 31)\n    df.drop(datetime_column, axis=1, inplace=True)\n    return df\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv', parse_dates=['date'])\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\nsubmission = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv', parse_dates=['date'])\nweights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')\n\n# Load and preprocess calendar\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ncalendar = fill_loss_holidays(calendar, ['Prague_1', 'Prague_2', 'Prague_3'], czech_holiday)\ncalendar = fill_loss_holidays(calendar, ['Brno_1'], brno_holiday)\ncalendar = fill_loss_holidays(calendar, ['Munich_1'], munich_holidays)\ncalendar = fill_loss_holidays(calendar, ['Frankfurt_1'], frankfurt_holidays)\n\ncalendar_enriched = pd.DataFrame()\nfor location in ['Frankfurt_1', 'Prague_2', 'Brno_1', 'Munich_1', 'Prague_3', 'Prague_1', 'Budapest_1']:\n    calendar_enriched = pd.concat([calendar_enriched, enrich_calendar(calendar.query('date >= \"2020-08-01 00:00:00\" and warehouse == @location'))])\ncalendar_enriched['year'] = calendar_enriched['date'].dt.year\ncalendar_enriched = calendar_enriched.rename(columns={\n    'holiday_name': 'date_holiday_name',\n    'year': 'date_year',\n    'holiday': 'date_holiday_flag',\n    'shops_closed': 'date_shops_closed_flag',\n    'winter_school_holidays': 'date_winter_school_holidays_flag',\n    'school_holidays': 'date_school_holidays_flag',\n})\n\n# Stack datasets\ndf_submission = stack_datasets(submission, calendar_enriched, inventory, weights)\ndf_submission['date_holiday_name'] = df_submission['date_holiday_name'].fillna('Working Day')\n\n# Fill NaN values\nfor col in df_submission.select_dtypes(include=np.number).columns:\n    if df_submission[col].isnull().any():\n        mean_val = df_submission[col].mean()\n        df_submission[col] = df_submission[col].fillna(mean_val)\n        print(f\"Filled NaN in df_submission['{col}'] with mean: {mean_val:.2f}\")\n\n# Sort by date\ndf_submission = sort_dataframe_by_date(df_submission, 'date')\nsubmission = df_submission.copy()\n# Drop noise columns\ncolumns_to_drop = [\n    'type_2_discount',\n    'date_holiday_flag',\n    'date_school_holidays_flag',\n    'date_shops_closed_flag',\n    'date_second_closed_day',\n    'date_winter_school_holidays_flag',\n    'date_day_after_closed_day',\n    'date_day_after_two_closed_days',\n    'type_5_discount',\n    'type_3_discount',\n    'type_1_discount',\n    'unique_id',\n    \"availability\"\n]\ndf_submission = df_submission.drop(columns=columns_to_drop, axis=1, errors='ignore')\n\n# Preprocess data\ndef preprocess_data(df, encoder_path='/kaggle/input/rohik_encoder_v2/scikitlearn/default/1/encoder.pkl', scaler_path='/kaggle/input/rohik_scaler_v2/scikitlearn/default/1/scaler.pkl'):\n    weight = df['weight'].values.astype(np.float32)\n    x = df.drop(['weight'], axis=1).copy()\n\n    datetime_cols = x.select_dtypes(include=['datetime']).columns\n    if len(datetime_cols) > 0:\n        for col in datetime_cols:\n            x[col + '_month'] = x[col].dt.month\n            x[col + '_day'] = x[col].dt.day\n        x = x.drop(datetime_cols, axis=1)\n\n    categorical_cols = x.select_dtypes(include=['object', 'category']).columns\n    numeric_cols = x.select_dtypes(include=['number']).columns\n\n    encoder = joblib.load(encoder_path)\n    scaler = joblib.load(scaler_path)\n    if len(categorical_cols) > 0:\n        x[categorical_cols] = encoder.transform(x[categorical_cols])\n    if len(numeric_cols) > 0:\n        x[numeric_cols] = scaler.transform(x[numeric_cols])\n\n    x = x.values.astype(np.float32)\n    return x, weight\n\nx_submission, weight_submission = preprocess_data(df_submission)\n\n# Pad x_submission for sequence length\npad_length = SEQUENCE_LENGTH - 1\nx_submission_padded = np.pad(x_submission, ((pad_length, 0), (0, 0)), mode='constant', constant_values=0)\n\n# Function to create test dataset\ndef create_test_dataset(features, weights, sequence_length, batch_size):\n    dataset = tf.keras.utils.timeseries_dataset_from_array(\n        data=features,\n        targets=None,\n        sequence_length=sequence_length,\n        batch_size=batch_size,\n        shuffle=False\n    )\n    aligned_weights = weights[-len(features):]\n    return dataset, aligned_weights\n\n# Create test dataset\ndataset_submission, aligned_weights = create_test_dataset(x_submission_padded, weight_submission, SEQUENCE_LENGTH, BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:30:01.536472Z","iopub.execute_input":"2025-09-21T09:30:01.537018Z","iopub.status.idle":"2025-09-21T09:30:11.230425Z","shell.execute_reply.started":"2025-09-21T09:30:01.536968Z","shell.execute_reply":"2025-09-21T09:30:11.229644Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator OrdinalEncoder from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Model Loading","metadata":{}},{"cell_type":"markdown","source":"This section of the code handles two critical tasks for the deep learning model: d**efining a custom loss function and a custom metric, and then loading the model itself**.\n\n**step-by-step breakdown**\n\n**1. Defining a Custom Weighted Loss Function (`custom_wmae_loss`)**\n\nThis function defines the Weighted Mean Absolute Error (WMAE), which the model uses during training to optimize its performance. In a standard Mean Absolute Error (MAE) loss, every prediction error is treated equally. However, in this problem, some forecasts are more important than others (e.g., predicting sales for high-volume products). WMAE addresses this by multiplying the absolute error of each prediction by a corresponding sample_weight\n\n**Code Breakdown:**\n\n- The function takes `y_true` (actual values), `y_pred` (predicted values), and `sample_weight` as inputs.\n- It calculates `tf.abs(y_true - y_pred)`, which is the absolute error for each prediction.\n- It multiplies this by `sample_weight` to get the `weighted_error`.\n- The loss is then computed as the sum of the weighted errors divided by the sum of the weights, effectively providing a weighted average.\n- The `tf.cond` statement is a safety check to prevent a division-by-zero error if all weights are zero.\n\n**2. Defining a Custom Weighted Metric (WeightedMAEMetric)**\n\nThis is a custom TensorFlow Metric class that provides a human-readable, aggregate view of the WMAE during the training process, distinct from the loss function. A Metric object maintains a running state and updates its value incrementally for each batch. This is different from a loss function, which calculates the loss for a single batch. By defining a custom metric, you can track the WMAE on the training and validation data without it directly influencing the model's gradient descent.\n\n**Code Breakdown:**\n\n- The `__init__` method initializes two state variables: `total_weighted_error` and `total_weights`, which will accumulate values over an epoch.\n- The `update_state` method is called after each batch of data. It calculates the weighted error for that batch and adds it to the running totals.\n- The result method is called at the end of an epoch. It computes the final WMAE for the epoch by dividing the total accumulated error by the total accumulated weights.\n- The `reset_state` method is called at the beginning of each epoch to clear the running totals.\n\n**3. Loading the Pre-trained Model**\n\nThis final step loads the entire deep learning model from a file that was saved after a prior training run. When a model is saved in Keras, custom components like `custom_wmae_loss` and `WeightedMAEMetric` are not automatically recognized. The `custom_objects` argument in `tf.keras.models.load_model` is a dictionary that maps the names of the custom components to their actual Python implementations. This is a crucial step for successfully loading models that use custom loss functions, layers, or metrics.\n\n**Code Breakdown:**\n\n- `tf.keras.models.load_model` is called with the file path to the saved model.\n- The `custom_objects` dictionary is passed, linking `custom_wmae_loss` to the custom_wmae_loss function and `WeightedMAEMetric` to the WeightedMAEMetric class.\n- Finally, `model.summary()` prints a high-level overview of the model's architecture, confirming that it was loaded correctly and is ready for use.","metadata":{}},{"cell_type":"code","source":"# Custom WMAE Loss Function\ndef custom_wmae_loss(y_true, y_pred, sample_weight=None):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    if sample_weight is None:\n        sample_weight = tf.ones_like(y_true, dtype=tf.float32)\n    else:\n        sample_weight = tf.cast(sample_weight, tf.float32)\n    weighted_error = tf.abs(y_true - y_pred) * sample_weight\n    sum_of_weights = tf.reduce_sum(sample_weight)\n    return tf.cond(\n        tf.greater(sum_of_weights, 0),\n        lambda: tf.reduce_sum(weighted_error) / sum_of_weights,\n        lambda: tf.constant(0.0, dtype=tf.float32)\n    )\n\n# Custom WMAE Metric\nclass WeightedMAEMetric(Metric):\n    def __init__(self, name='wmae', **kwargs):\n        super(WeightedMAEMetric, self).__init__(name=name, **kwargs)\n        self.total_weighted_error = self.add_weight(name='total_weighted_error', initializer='zeros')\n        self.total_weights = self.add_weight(name='total_weights', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        if sample_weight is None:\n            sample_weight = tf.ones_like(y_true, dtype=tf.float32)\n        else:\n            sample_weight = tf.cast(sample_weight, tf.float32)\n        weighted_error = tf.abs(y_true - y_pred) * sample_weight\n        self.total_weighted_error.assign_add(tf.reduce_sum(weighted_error))\n        self.total_weights.assign_add(tf.reduce_sum(sample_weight))\n\n    def result(self):\n        return tf.cond(\n            tf.greater(self.total_weights, 0),\n            lambda: self.total_weighted_error / self.total_weights,\n            lambda: tf.constant(0.0, dtype=tf.float32)\n        )\n\n    def reset_state(self):\n        self.total_weighted_error.assign(0.0)\n        self.total_weights.assign(0.0)\n\n# --- Save the model in Keras format (.keras) ---\n# Define the path for the .keras file\nkeras_model_path = \"/kaggle/input/wavenet_transformer_model.keras/keras/default/1/wavenet_transformer_model.keras\"\n\n# --- Load the model from Keras format (.keras) ---\nprint(f\"\\n--- Loading Model from '{keras_model_path}' ---\")\nmodel = tf.keras.models.load_model(\n    keras_model_path,\n    custom_objects={\n        'custom_wmae_loss': custom_wmae_loss,\n        'WeightedMAEMetric': WeightedMAEMetric\n    }\n)\nprint(\"Model loaded successfully!\")\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:30:11.231290Z","iopub.execute_input":"2025-09-21T09:30:11.231637Z","iopub.status.idle":"2025-09-21T09:30:20.360617Z","shell.execute_reply.started":"2025-09-21T09:30:11.231606Z","shell.execute_reply":"2025-09-21T09:30:20.360020Z"}},"outputs":[{"name":"stdout","text":"\n--- Loading Model from '/kaggle/input/wavenet_transformer_model.keras/keras/default/1/wavenet_transformer_model.keras' ---\nModel loaded successfully!\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input (InputLayer)          [(None, 100, 18)]            0         []                            \n                                                                                                  \n conv1d_26 (Conv1D)          (None, 100, 256)             14080     ['input[0][0]']               \n                                                                                                  \n conv1d_27 (Conv1D)          (None, 100, 256)             14080     ['input[0][0]']               \n                                                                                                  \n multiply_6 (Multiply)       (None, 100, 256)             0         ['conv1d_26[0][0]',           \n                                                                     'conv1d_27[0][0]']           \n                                                                                                  \n batch_normalization_7 (Bat  (None, 100, 256)             1024      ['multiply_6[0][0]']          \n chNormalization)                                                                                 \n                                                                                                  \n conv1d_30 (Conv1D)          (None, 100, 256)             4864      ['input[0][0]']               \n                                                                                                  \n conv1d_28 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_11 (Add)                (None, 100, 256)             0         ['conv1d_30[0][0]',           \n                                                                     'conv1d_28[0][0]']           \n                                                                                                  \n conv1d_31 (Conv1D)          (None, 100, 256)             196864    ['add_11[0][0]']              \n                                                                                                  \n conv1d_32 (Conv1D)          (None, 100, 256)             196864    ['add_11[0][0]']              \n                                                                                                  \n multiply_7 (Multiply)       (None, 100, 256)             0         ['conv1d_31[0][0]',           \n                                                                     'conv1d_32[0][0]']           \n                                                                                                  \n batch_normalization_8 (Bat  (None, 100, 256)             1024      ['multiply_7[0][0]']          \n chNormalization)                                                                                 \n                                                                                                  \n conv1d_33 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_8[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_12 (Add)                (None, 100, 256)             0         ['add_11[0][0]',              \n                                                                     'conv1d_33[0][0]']           \n                                                                                                  \n conv1d_35 (Conv1D)          (None, 100, 256)             196864    ['add_12[0][0]']              \n                                                                                                  \n conv1d_36 (Conv1D)          (None, 100, 256)             196864    ['add_12[0][0]']              \n                                                                                                  \n multiply_8 (Multiply)       (None, 100, 256)             0         ['conv1d_35[0][0]',           \n                                                                     'conv1d_36[0][0]']           \n                                                                                                  \n batch_normalization_9 (Bat  (None, 100, 256)             1024      ['multiply_8[0][0]']          \n chNormalization)                                                                                 \n                                                                                                  \n conv1d_37 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_9[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_13 (Add)                (None, 100, 256)             0         ['add_12[0][0]',              \n                                                                     'conv1d_37[0][0]']           \n                                                                                                  \n conv1d_39 (Conv1D)          (None, 100, 256)             196864    ['add_13[0][0]']              \n                                                                                                  \n conv1d_40 (Conv1D)          (None, 100, 256)             196864    ['add_13[0][0]']              \n                                                                                                  \n multiply_9 (Multiply)       (None, 100, 256)             0         ['conv1d_39[0][0]',           \n                                                                     'conv1d_40[0][0]']           \n                                                                                                  \n batch_normalization_10 (Ba  (None, 100, 256)             1024      ['multiply_9[0][0]']          \n tchNormalization)                                                                                \n                                                                                                  \n conv1d_41 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_10[0][0]\n                                                                    ']                            \n                                                                                                  \n add_14 (Add)                (None, 100, 256)             0         ['add_13[0][0]',              \n                                                                     'conv1d_41[0][0]']           \n                                                                                                  \n conv1d_43 (Conv1D)          (None, 100, 256)             196864    ['add_14[0][0]']              \n                                                                                                  \n conv1d_44 (Conv1D)          (None, 100, 256)             196864    ['add_14[0][0]']              \n                                                                                                  \n multiply_10 (Multiply)      (None, 100, 256)             0         ['conv1d_43[0][0]',           \n                                                                     'conv1d_44[0][0]']           \n                                                                                                  \n batch_normalization_11 (Ba  (None, 100, 256)             1024      ['multiply_10[0][0]']         \n tchNormalization)                                                                                \n                                                                                                  \n conv1d_45 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_11[0][0]\n                                                                    ']                            \n                                                                                                  \n add_15 (Add)                (None, 100, 256)             0         ['add_14[0][0]',              \n                                                                     'conv1d_45[0][0]']           \n                                                                                                  \n conv1d_47 (Conv1D)          (None, 100, 256)             196864    ['add_15[0][0]']              \n                                                                                                  \n conv1d_48 (Conv1D)          (None, 100, 256)             196864    ['add_15[0][0]']              \n                                                                                                  \n multiply_11 (Multiply)      (None, 100, 256)             0         ['conv1d_47[0][0]',           \n                                                                     'conv1d_48[0][0]']           \n                                                                                                  \n batch_normalization_12 (Ba  (None, 100, 256)             1024      ['multiply_11[0][0]']         \n tchNormalization)                                                                                \n                                                                                                  \n conv1d_29 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv1d_34 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_8[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv1d_38 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_9[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv1d_42 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_10[0][0]\n                                                                    ']                            \n                                                                                                  \n conv1d_46 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_11[0][0]\n                                                                    ']                            \n                                                                                                  \n conv1d_50 (Conv1D)          (None, 100, 256)             65792     ['batch_normalization_12[0][0]\n                                                                    ']                            \n                                                                                                  \n add_17 (Add)                (None, 100, 256)             0         ['conv1d_29[0][0]',           \n                                                                     'conv1d_34[0][0]',           \n                                                                     'conv1d_38[0][0]',           \n                                                                     'conv1d_42[0][0]',           \n                                                                     'conv1d_46[0][0]',           \n                                                                     'conv1d_50[0][0]']           \n                                                                                                  \n activation_1 (Activation)   (None, 100, 256)             0         ['add_17[0][0]']              \n                                                                                                  \n batch_normalization_13 (Ba  (None, 100, 256)             1024      ['activation_1[0][0]']        \n tchNormalization)                                                                                \n                                                                                                  \n multi_head_attention_2 (Mu  (None, 100, 256)             526080    ['batch_normalization_13[0][0]\n ltiHeadAttention)                                                  ',                            \n                                                                     'batch_normalization_13[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_17 (Dropout)        (None, 100, 256)             0         ['multi_head_attention_2[0][0]\n                                                                    ']                            \n                                                                                                  \n add_18 (Add)                (None, 100, 256)             0         ['batch_normalization_13[0][0]\n                                                                    ',                            \n                                                                     'dropout_17[0][0]']          \n                                                                                                  \n layer_normalization_4 (Lay  (None, 100, 256)             512       ['add_18[0][0]']              \n erNormalization)                                                                                 \n                                                                                                  \n dense_7 (Dense)             (None, 100, 128)             32896     ['layer_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n dropout_18 (Dropout)        (None, 100, 128)             0         ['dense_7[0][0]']             \n                                                                                                  \n dense_8 (Dense)             (None, 100, 256)             33024     ['dropout_18[0][0]']          \n                                                                                                  \n add_19 (Add)                (None, 100, 256)             0         ['layer_normalization_4[0][0]'\n                                                                    , 'dense_8[0][0]']            \n                                                                                                  \n layer_normalization_5 (Lay  (None, 100, 256)             512       ['add_19[0][0]']              \n erNormalization)                                                                                 \n                                                                                                  \n multi_head_attention_3 (Mu  (None, 100, 256)             526080    ['layer_normalization_5[0][0]'\n ltiHeadAttention)                                                  , 'layer_normalization_5[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_19 (Dropout)        (None, 100, 256)             0         ['multi_head_attention_3[0][0]\n                                                                    ']                            \n                                                                                                  \n add_20 (Add)                (None, 100, 256)             0         ['layer_normalization_5[0][0]'\n                                                                    , 'dropout_19[0][0]']         \n                                                                                                  \n layer_normalization_6 (Lay  (None, 100, 256)             512       ['add_20[0][0]']              \n erNormalization)                                                                                 \n                                                                                                  \n dense_9 (Dense)             (None, 100, 128)             32896     ['layer_normalization_6[0][0]'\n                                                                    ]                             \n                                                                                                  \n dropout_20 (Dropout)        (None, 100, 128)             0         ['dense_9[0][0]']             \n                                                                                                  \n dense_10 (Dense)            (None, 100, 256)             33024     ['dropout_20[0][0]']          \n                                                                                                  \n add_21 (Add)                (None, 100, 256)             0         ['layer_normalization_6[0][0]'\n                                                                    , 'dense_10[0][0]']           \n                                                                                                  \n layer_normalization_7 (Lay  (None, 100, 256)             512       ['add_21[0][0]']              \n erNormalization)                                                                                 \n                                                                                                  \n conv1d_51 (Conv1D)          (None, 100, 256)             65792     ['layer_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n global_average_pooling1d_1  (None, 256)                  0         ['conv1d_51[0][0]']           \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n dense_11 (Dense)            (None, 256)                  65792     ['global_average_pooling1d_1[0\n                                                                    ][0]']                        \n                                                                                                  \n dropout_21 (Dropout)        (None, 256)                  0         ['dense_11[0][0]']            \n                                                                                                  \n dense_12 (Dense)            (None, 128)                  32896     ['dropout_21[0][0]']          \n                                                                                                  \n dropout_22 (Dropout)        (None, 128)                  0         ['dense_12[0][0]']            \n                                                                                                  \n dense_13 (Dense)            (None, 64)                   8256      ['dropout_22[0][0]']          \n                                                                                                  \n dropout_23 (Dropout)        (None, 64)                   0         ['dense_13[0][0]']            \n                                                                                                  \n output (Dense)              (None, 1)                    65        ['dropout_23[0][0]']          \n                                                                                                  \n tf.clip_by_value_1 (TFOpLa  (None, 1)                    0         ['output[0][0]']              \n mbda)                                                                                            \n                                                                                                  \n==================================================================================================\nTotal params: 4091393 (15.61 MB)\nTrainable params: 4087809 (15.59 MB)\nNon-trainable params: 3584 (14.00 KB)\n__________________________________________________________________________________________________\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Prediction Generation, Post-processing, and Submission","metadata":{}},{"cell_type":"markdown","source":"This step focuses on the final stages of the machine learning workflow: **generating predictions from the trained model and formatting them into a submission file.**\n\n**Step-by-step breakdown**\n\n**1. Generating Predictions**\n\nThis part of the code uses the pre-trained model to make predictions on the test dataset. After the model is trained, its purpose is to generalize what it has learned to new, unseen data. The `model.predict()` method is the standard way to accomplish this. It takes a preprocessed dataset (in this case, `dataset_submission`) as input and uses the model's learned weights and biases to output a forecast.\n\n**Code Breakdown:**\n\n- `y_pred_list = []`: An empty list is initialized to store the predictions for each batch.\n- `for x_batch in dataset_submission`: The code iterates through the test data one batch at a time. This is a memory-efficient practice, especially for large datasets, as it avoids loading the entire dataset into memory at once.\n- `y_pred_batch = model.predict(x_batch, verbose=0)`: The model generates predictions for the current batch. The verbose=0 argument prevents the printing of progress bars to the console.\n- `y_pred_list.append(y_pred_batch)`: The predictions for the current batch are added to the list.\n- `y_pred_padded = np.concatenate(y_pred_list, axis=0)`: After all batches are processed, the list of prediction arrays is combined into a single, padded NumPy array.\n- `y_pred = y_pred_padded[-len(x_submission):]`: The padding that was added to the input data for sequence alignment is removed from the predictions to ensure the final prediction array has the correct length.\n\n**2. Inverse Transformation of Predictions**\n\nThe predictions are inverse-scaled to be in the same units as the original sales data. During the model's training, the target variable (sales) was likely scaled (e.g., using StandardScaler) to help the optimization process. Models often perform better when input and output data are in a standardized range. However, the raw model output is on this scaled range, which is not directly interpretable. To get the actual sales values, an inverse transformation is required.\n\n**Code Breakdown:**\n\n- `scaler_y = joblib.load(...)`: The pre-trained scaler object that was used to scale the target variable during the training phase is loaded from a file.\n- `y_pred.reshape(-1, 1)`: The 1D prediction array is reshaped into a 2D array, which is the required input format for the inverse_transform method of StandardScaler.\n- `y_pred_unscaled = scaler_y.inverse_transform(...)`: The loaded scaler object applies the inverse transformation to convert the predictions back to the original scale.\n\n**3. Creating and Saving the Submission File**\n\nThis is the final step where the predictions are formatted into the required structure for submission. In a data science competition, there are often strict requirements for the submission file format. The file usually needs to contain a unique identifier for each prediction and the predicted value itself.\n\n**Code Breakdown:**\n\n- `submission['date'] = ...`: The date column is formatted to `YYYY-MM-DD` to ensure consistency.\n- `submission['id'] = ...`: A new id column is created by combining the `unique_id` and the formatted date. This provides a unique identifier for each forecast as required.\n- `submission['sales_hat'] = y_pred_unscaled`: The unscaled predictions are added to the submission DataFrame under the column name `sales_hat`.\n- `submission_final = submission[['id', 'sales_hat']]`: A new DataFrame is created containing only the id and `sales_hat` columns, as these are the only two required for submission.\n- `submission_final.to_csv(\"submission.csv\", index=False)`: The final DataFrame is saved as a CSV file. The `index=False` argument prevents pandas from writing the DataFrame's index to the CSV file.","metadata":{}},{"cell_type":"code","source":"# Generate predictions\ny_pred_list = []\nfor x_batch in dataset_submission:\n    y_pred_batch = model.predict(x_batch, verbose=0)\n    y_pred_list.append(y_pred_batch)\n\ny_pred_padded = np.concatenate(y_pred_list, axis=0)\ny_pred = y_pred_padded[-len(x_submission):]\n\n# Load scaler for inverse transformationa\nscaler_y = joblib.load('/kaggle/input/rohik_scaler_y_v2/scikitlearn/default/1/scaler_y.pkl')\ny_pred_unscaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n\n# Prepare submission\nsubmission['date'] = pd.to_datetime(submission['date']).dt.strftime('%Y-%m-%d')\n# Restore unique_id since it was dropped\nsubmission['id'] = submission['unique_id'].astype(str) + '_' + submission['date']\nsubmission['sales_hat'] = y_pred_unscaled\nsubmission_final = submission[['id', 'sales_hat']]\n\n# Save submission\nsubmission_final.to_csv(\"submission.csv\", index=False)\nprint(\"Submission saved to submission.csv\")\nsubmission_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:30:20.361887Z","iopub.execute_input":"2025-09-21T09:30:20.362127Z","iopub.status.idle":"2025-09-21T09:35:57.074858Z","shell.execute_reply.started":"2025-09-21T09:30:20.362110Z","shell.execute_reply":"2025-09-21T09:35:57.074148Z"}},"outputs":[{"name":"stdout","text":"Submission saved to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                    id   sales_hat\n0      1226_2024-06-03   45.729156\n18110  3510_2024-06-03   46.179909\n18100  3517_2024-06-03   46.386429\n18077  3148_2024-06-03   49.095165\n18069  2385_2024-06-03   48.637413\n...                ...         ...\n44578    96_2024-06-16   20.823042\n23275  5228_2024-06-16   20.617605\n13130  4848_2024-06-16  520.061951\n37054  5270_2024-06-16  191.749313\n26157   516_2024-06-16  176.019180\n\n[47021 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sales_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1226_2024-06-03</td>\n      <td>45.729156</td>\n    </tr>\n    <tr>\n      <th>18110</th>\n      <td>3510_2024-06-03</td>\n      <td>46.179909</td>\n    </tr>\n    <tr>\n      <th>18100</th>\n      <td>3517_2024-06-03</td>\n      <td>46.386429</td>\n    </tr>\n    <tr>\n      <th>18077</th>\n      <td>3148_2024-06-03</td>\n      <td>49.095165</td>\n    </tr>\n    <tr>\n      <th>18069</th>\n      <td>2385_2024-06-03</td>\n      <td>48.637413</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44578</th>\n      <td>96_2024-06-16</td>\n      <td>20.823042</td>\n    </tr>\n    <tr>\n      <th>23275</th>\n      <td>5228_2024-06-16</td>\n      <td>20.617605</td>\n    </tr>\n    <tr>\n      <th>13130</th>\n      <td>4848_2024-06-16</td>\n      <td>520.061951</td>\n    </tr>\n    <tr>\n      <th>37054</th>\n      <td>5270_2024-06-16</td>\n      <td>191.749313</td>\n    </tr>\n    <tr>\n      <th>26157</th>\n      <td>516_2024-06-16</td>\n      <td>176.019180</td>\n    </tr>\n  </tbody>\n</table>\n<p>47021 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":5}]}