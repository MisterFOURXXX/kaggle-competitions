{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition overview","metadata":{}},{"cell_type":"markdown","source":"**Competition Summary**\n\n- This is the Rohlik Sales Forecasting Challenge, a time-series forecasting competition hosted on Kaggle. The primary objective is to predict the sales volume for various inventory items across 11 different Rohlik Group warehouses for a period of 14 days.\n\n- Accurate forecasts are vital for the e-grocery company's operations, as they directly impact supply chain efficiency, inventory management, and overall sustainability by minimizing waste.\n\n- The model's performance will be evaluated using the Weighted Mean Absolute Error (WMAE). The specific weights for each inventory item are provided in a separate file. The competition runs from November 15, 2024, to February 15, 2025, and offers cash prizes for the top three competitors.\n\n- The dataset includes historical sales and order data, product metadata, and a calendar with holiday information. Some features available in the training set (e.g., sales and availability) are intentionally removed from the test set, as they would not be known at the time of a real-world prediction.","metadata":{}},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"This data dictionary describes the files and columns provided for the competition.","metadata":{}},{"cell_type":"markdown","source":"**sales_train.csv and sales_test.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a specific inventory item in a specific warehouse.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">date</td>\n    <td class=\"tg-7zrl\">The date of the sales record.</td>\n    <td class=\"tg-7zrl\">Date</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse where the item is stored.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">total_orders</td>\n    <td class=\"tg-7zrl\">The historical number of orders for the selected warehouse.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sales</td>\n    <td class=\"tg-7zrl\">The target variable: sales volume (pcs or kg).</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sell_price_main</td>\n    <td class=\"tg-7zrl\">The selling price of the item.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">availability</td>\n    <td class=\"tg-7zrl\">The proportion of the day the item was available. A value of 1 means it was available all day.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">type_0_discount, type_1_discount, etc.</td>\n    <td class=\"tg-7zrl\">The percentage discount offered for various promotion types. Negative values indicate no discount.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**inventory.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a specific inventory item.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">product_unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a product, shared across all warehouses.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">name</td>\n    <td class=\"tg-7zrl\">The name of the product.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">L1_category_name, L2_category_name, etc.</td>\n    <td class=\"tg-7zrl\">Hierarchical category names for the product. L4 is the most granular.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse where the inventory item is located.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**calendar.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">date</td>\n    <td class=\"tg-7zrl\">The date of the calendar event.</td>\n    <td class=\"tg-7zrl\">Date</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">holiday_name</td>\n    <td class=\"tg-7zrl\">The name of the public holiday, if applicable.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">holiday</td>\n    <td class=\"tg-7zrl\">A binary flag (0 or 1) indicating if the date is a holiday.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">shops_closed</td>\n    <td class=\"tg-7zrl\">A flag indicating a public holiday where most shops are closed.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">winter_school_holidays</td>\n    <td class=\"tg-7zrl\">A flag for winter school holidays.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">school_holidays</td>\n    <td class=\"tg-7zrl\">A flag for general school holidays.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**test_weights.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for the inventory item.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">weight</td>\n    <td class=\"tg-7zrl\">The weight used for calculating the Weighted Mean Absolute Error (WMAE) metric for this item.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"## [Link to competition](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)","metadata":{}},{"cell_type":"markdown","source":"# Model submission notebook","metadata":{}},{"cell_type":"markdown","source":"## [Model submission notebook](https://www.kaggle.com/code/misterfour/rohlik-sales-forecasting-challenge-submission)","metadata":{}},{"cell_type":"markdown","source":"## [Reference! (add holidays calendar of each country into dataset)](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-01T10:56:23.935569Z","iopub.execute_input":"2025-08-01T10:56:23.936758Z","iopub.status.idle":"2025-08-01T10:56:24.302711Z","shell.execute_reply.started":"2025-08-01T10:56:23.936625Z","shell.execute_reply":"2025-08-01T10:56:24.301727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import ( # Consolidated Keras layers\n    Input, Conv1D, Multiply, Add, Dense, Dropout, LayerNormalization,\n    MultiHeadAttention, GlobalAveragePooling1D, Activation, BatchNormalization\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow.keras import mixed_precision # For mixed precision policy, if used\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error # For evaluation metrics\nfrom datetime import datetime\nimport joblib # For saving/loading models or other objects\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom joblib import Parallel, delayed\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Disable GPU usage and force CPU only (if desired)\n# This block should typically come after TensorFlow import but before model definition/training\ntf.config.set_visible_devices([], 'GPU')  # Prevent TensorFlow from using GPU\nphysical_devices = tf.config.list_physical_devices('CPU')\nassert len(physical_devices) > 0, \"No CPU devices found\"\ntf.config.set_logical_device_configuration(\n    physical_devices[0],\n    [tf.config.LogicalDeviceConfiguration()]\n)\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:56:24.304048Z","iopub.execute_input":"2025-08-01T10:56:24.304500Z","iopub.status.idle":"2025-08-01T10:56:44.852694Z","shell.execute_reply.started":"2025-08-01T10:56:24.304474Z","shell.execute_reply":"2025-08-01T10:56:44.851750Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset for training","metadata":{}},{"cell_type":"code","source":"#train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv', parse_dates=['date'])\n#inventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\n#submission = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv', parse_dates=['date'])\n#weights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:56:44.853637Z","iopub.execute_input":"2025-08-01T10:56:44.854335Z","iopub.status.idle":"2025-08-01T10:56:44.858855Z","shell.execute_reply.started":"2025-08-01T10:56:44.854277Z","shell.execute_reply":"2025-08-01T10:56:44.858048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv', parse_dates=['date'])\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\nsubmission = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv', parse_dates=['date'])\nweights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:56:44.860964Z","iopub.execute_input":"2025-08-01T10:56:44.861295Z","iopub.status.idle":"2025-08-01T10:56:53.814905Z","shell.execute_reply.started":"2025-08-01T10:56:44.861268Z","shell.execute_reply":"2025-08-01T10:56:53.813994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Merge dataset and add holidays calendar of each country into dataset","metadata":{}},{"cell_type":"code","source":"# Additional holiday days\n\nczech_holiday = [ \n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"),\n]\nbrno_holiday = [\n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"),\n]\nmunich_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),\n]\nfrankfurt_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),\n]\n\n# Functions\n\ndef fill_loss_holidays(df_fill, warehouses, holidays):\n    df = df_fill.copy()\n    for item in holidays:\n        dates, holiday_name = item\n        generated_dates = [datetime.strptime(date, '%m/%d/%Y').strftime('%Y-%m-%d') for date in dates]\n        for generated_date in generated_dates:\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday'] = 1\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday_name'] = holiday_name\n    return df\n\ndef enrich_calendar(df):\n    df = df.sort_values('date').reset_index(drop=True)\n\n    # Number of days until next holiday\n    df['next_holiday_date'] = df.loc[df['holiday'] == 1, 'date'].shift(-1)\n    # Fill NaT values by using the next valid observation to fill the gap\n    df['next_holiday_date'] = df['next_holiday_date'].bfill() \n    df['date_days_to_next_holiday'] = (df['next_holiday_date'] - df['date']).dt.days\n    df.drop(columns=['next_holiday_date'], inplace=True)\n\n    # Number of days until shops are closed\n    df['next_shops_closed_date'] = df.loc[df['shops_closed'] == 1, 'date'].shift(-1)\n    df['next_shops_closed_date'] = df['next_shops_closed_date'].bfill()\n    df['date_days_to_shops_closed'] = (df['next_shops_closed_date'] - df['date']).dt.days\n    df.drop(columns=['next_shops_closed_date'], inplace=True)\n\n    # Was the shop closed yesterday?\n    df['date_day_after_closed_day'] = ((df['shops_closed'] == 0) & (df['shops_closed'].shift(1) == 1)).astype(int)\n\n    # Are shops closed today and were they also closed yesterday (e.g., December 26 in Germany)?\n    df['date_second_closed_day'] = ((df['shops_closed'] == 1) & (df['shops_closed'].shift(1) == 1)).astype(int)\n\n    # Was the shop closed the last two days?\n    df['date_day_after_two_closed_days'] = ((df['shops_closed'] == 0) & (df['date_second_closed_day'].shift(1) == 1)).astype(int)\n\n    return df\n\n#calendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Prague_1', 'Prague_2', 'Prague_3'], holidays=czech_holiday)\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Brno_1'], holidays=brno_holiday)\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Munich_1'], holidays=munich_holidays)\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Frankfurt_1'], holidays=frankfurt_holidays)\n\ncalendar_enriched = pd.DataFrame()\n\nfor location in ['Frankfurt_1', 'Prague_2', 'Brno_1', 'Munich_1', 'Prague_3', 'Prague_1', 'Budapest_1']:\n    calendar_enriched = pd.concat([\n        calendar_enriched,enrich_calendar(calendar.query('date >= \"2020-08-01 00:00:00\" and warehouse ==@location'))])\ncalendar_enriched.loc[:,'year'] = calendar_enriched['date'].dt.year\ncalendar_enriched.sort_values('date')[['date','holiday_name','shops_closed','warehouse','date_days_to_next_holiday']].head(5)\n\ncalendar_enriched = calendar_enriched.rename(columns={\n    'holiday_name':'date_holiday_name',\n    'year':'date_year',\n    'holiday':'date_holiday_flag',\n    'holiday':'date_holiday_flag',\n    'shops_closed':'date_shops_closed_flag',\n    'winter_school_holidays':'date_winter_school_holidays_flag',\n    'school_holidays':'date_school_holidays_flag',\n})\n\ndef stack_datasets(df, calendar_extended, inventory, weights):\n    \"\"\"\n    Stacks the given DataFrame with additional data from calendar, inventory, and weights.\n\n    Args:\n        df: The main DataFrame to be stacked.\n        calendar_extended: DataFrame containing calendar-related information.\n        inventory: DataFrame containing inventory information.\n        weights: DataFrame containing weight information for unique IDs.\n\n    Returns:\n        pandas.DataFrame: The stacked DataFrame.\n    \"\"\"\n    # Merge with calendar_extended on date and warehouse\n    df = df.merge(calendar_extended, on=['date', 'warehouse'], how='left')\n    \n    # Merge with inventory on unique_id and warehouse\n    df = df.merge(inventory, on=['unique_id', 'warehouse'], how='left')\n    \n    # Perform a VLOOKUP-style merge with weights on unique_id\n    df = df.merge(weights, on='unique_id', how='left')\n    \n    # Ensure 'date' is in datetime format\n    df['date'] = pd.to_datetime(df['date'])\n    \n    return df\n\ndf_train = stack_datasets(train, calendar_enriched, inventory, weights)\ndf_train\n\n# Fill 'date_holiday_name' with 'Working Day' where it's NaN\ndf_train['date_holiday_name'] = df_train['date_holiday_name'].fillna('Working Day')\ndf_train","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:56:53.815763Z","iopub.execute_input":"2025-08-01T10:56:53.816066Z","iopub.status.idle":"2025-08-01T10:57:02.029994Z","shell.execute_reply.started":"2025-08-01T10:56:53.816039Z","shell.execute_reply":"2025-08-01T10:57:02.028810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split dataset into train and test dataset","metadata":{}},{"cell_type":"code","source":"# Calculate the split index for 80% training data\n# The remaining 20% will be for the test set\ndf_split = df_train.copy()\ntrain_split_index = int(len(df_split) * 0.80)\n# Split the DataFrame chronologically\n# IMPORTANT: Perform slicing on the original df_train for both new dataframes\ndf_train = df_split.iloc[:train_split_index].copy()\ndf_test = df_split.iloc[train_split_index:].copy() \n\nprint(\"New df_train shape (80%):\", df_train.shape)\nprint(\"New df_test shape (20%):\", df_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:02.030994Z","iopub.execute_input":"2025-08-01T10:57:02.031334Z","iopub.status.idle":"2025-08-01T10:57:04.446532Z","shell.execute_reply.started":"2025-08-01T10:57:02.031284Z","shell.execute_reply":"2025-08-01T10:57:04.445103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:04.447757Z","iopub.execute_input":"2025-08-01T10:57:04.448252Z","iopub.status.idle":"2025-08-01T10:57:05.883842Z","shell.execute_reply.started":"2025-08-01T10:57:04.448145Z","shell.execute_reply":"2025-08-01T10:57:05.882889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:05.885157Z","iopub.execute_input":"2025-08-01T10:57:05.885514Z","iopub.status.idle":"2025-08-01T10:57:06.252351Z","shell.execute_reply.started":"2025-08-01T10:57:05.885485Z","shell.execute_reply":"2025-08-01T10:57:06.251245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation analysis","metadata":{}},{"cell_type":"code","source":"# Identify numerical columns (excluding non-numeric types like 'object' or 'datetime')\n# Only numeric columns can have a mean calculated for filling.\nnumeric_cols = df_train.select_dtypes(include=np.number).columns\n\n# Fill NaN values in numeric columns with the mean of their respective columns\nfor col in numeric_cols:\n    if df_train[col].isnull().any(): # Check if there are any NaNs in the column\n        col_mean = df_train[col].mean()\n        df_train[col] = df_train[col].fillna(col_mean)\n        print(f\"\\nFilled NaNs in '{col}' with mean: {col_mean:.2f}\")\n\nprint(\"\\ndf_train shape after filling NaNs:\", df_train.shape)\nprint(\"\\nNaN count per column after filling:\")\nprint(df_train.isnull().sum())","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:06.253253Z","iopub.execute_input":"2025-08-01T10:57:06.253550Z","iopub.status.idle":"2025-08-01T10:57:08.165950Z","shell.execute_reply.started":"2025-08-01T10:57:06.253527Z","shell.execute_reply":"2025-08-01T10:57:08.164956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_tr_corr = df_train.copy()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:08.168659Z","iopub.execute_input":"2025-08-01T10:57:08.168931Z","iopub.status.idle":"2025-08-01T10:57:09.098365Z","shell.execute_reply.started":"2025-08-01T10:57:08.168908Z","shell.execute_reply":"2025-08-01T10:57:09.097385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"featurex = df_tr_corr.drop(['sales'], axis=1)\nfeaturey = df_tr_corr[['sales']]\nprint(\"featurex\", featurex.shape)\nprint(\"featurey\", featurey.shape)\nprint('-------------------------------------------------------------------------')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:09.099500Z","iopub.execute_input":"2025-08-01T10:57:09.099805Z","iopub.status.idle":"2025-08-01T10:57:09.589010Z","shell.execute_reply.started":"2025-08-01T10:57:09.099777Z","shell.execute_reply":"2025-08-01T10:57:09.587551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a list of column names with string data type\nstring_columns = df_tr_corr.select_dtypes(include=['object']).columns.tolist() \n\nprint(string_columns)  # Output: ['name', 'city', 'country']","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:09.590113Z","iopub.execute_input":"2025-08-01T10:57:09.590456Z","iopub.status.idle":"2025-08-01T10:57:09.919823Z","shell.execute_reply.started":"2025-08-01T10:57:09.590428Z","shell.execute_reply":"2025-08-01T10:57:09.918474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = OrdinalEncoder()\nfeaturex[['warehouse', 'date_holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']] = encoder.fit_transform(featurex[['warehouse', 'date_holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']])\nfeaturex","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:09.921095Z","iopub.execute_input":"2025-08-01T10:57:09.921542Z","iopub.status.idle":"2025-08-01T10:57:17.111186Z","shell.execute_reply.started":"2025-08-01T10:57:09.921508Z","shell.execute_reply":"2025-08-01T10:57:17.110143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_datetime(df, datetime_column):\n  \"\"\"\n  Encodes datetime features in a pandas DataFrame.\n\n  Args:\n    df: The pandas DataFrame containing the datetime column.\n    datetime_column: The name of the datetime column in the DataFrame.\n\n  Returns:\n    pandas.DataFrame: The DataFrame with encoded datetime features.  \n  \"\"\"\n\n  df[datetime_column] = pd.to_datetime(df[datetime_column]) \n\n  # Extract features\n  df['year'] = df[datetime_column].dt.year\n  df['month'] = df[datetime_column].dt.month\n  df['day'] = df[datetime_column].dt.day\n\n  # Create cyclical features (optional)\n  df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n  df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n  df['sin_day'] = np.sin(2 * np.pi * df['day'] / 31)\n  df['cos_day'] = np.cos(2 * np.pi * df['day'] / 31)\n\n  # Drop the original datetime column (optional)\n  df.drop(datetime_column, axis=1, inplace=True)\n\n  return df\n\n# Example usage:\n# Assuming 'df' is your DataFrame and 'date_time' is the name of your datetime column\nfeaturex = encode_datetime(featurex, 'date')\nfeaturex","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T10:57:17.112226Z","iopub.execute_input":"2025-08-01T10:57:17.112624Z","iopub.status.idle":"2025-08-01T10:57:20.128865Z","shell.execute_reply.started":"2025-08-01T10:57:17.112594Z","shell.execute_reply":"2025-08-01T10:57:20.127401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flatten featurey to a one-dimensional format\n#y_flattened = featurey.values.ravel()  # Converts to 1D array\n\n# For demonstration, let's print the flattened featurey\n#print(\"Flattened featurey:\", y_flattened)\n\n# Calculate mutual information scores for each feature with respect to each target\nX, y = featurex, featurey\ndef calculate_mi_for_target(target_index):\n    return mutual_info_regression(X, y.iloc[:, target_index], random_state=42)\n\n# Step 3: Run parallelized MI calculations on the original y (multi-dimensional)\nmi_scores = Parallel(n_jobs=-1)(delayed(calculate_mi_for_target)(i) for i in range(y.shape[1]))\n\n# Step 4: Convert the list of scores to a DataFrame\nmi_scores_df = pd.DataFrame(mi_scores, columns=featurex.columns, index=featurey.columns)\nprint(\"\\nMutual Information Scores for each feature with respect to each target:\")\nmi_scores_df\n\n# Step 5: Aggregate scores across all targets (e.g., by averaging)\naggregated_mi_scores = mi_scores_df.mean(axis=0).sort_values(ascending=False)\nprint(\"\\nAggregated Mutual Information Scores:\")\naggregated_mi_scores","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:06:55.310674Z","iopub.execute_input":"2025-08-01T11:06:55.311663Z","iopub.status.idle":"2025-08-01T11:51:16.717694Z","shell.execute_reply.started":"2025-08-01T11:06:55.311626Z","shell.execute_reply":"2025-08-01T11:51:16.715439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mi_scores_df.T.describe()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:16.721722Z","iopub.execute_input":"2025-08-01T11:51:16.722249Z","iopub.status.idle":"2025-08-01T11:51:16.789004Z","shell.execute_reply.started":"2025-08-01T11:51:16.722190Z","shell.execute_reply":"2025-08-01T11:51:16.788055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aggregated_mi_scores.describe()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:16.790220Z","iopub.execute_input":"2025-08-01T11:51:16.790476Z","iopub.status.idle":"2025-08-01T11:51:16.807278Z","shell.execute_reply.started":"2025-08-01T11:51:16.790456Z","shell.execute_reply":"2025-08-01T11:51:16.806269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mi_scores_df","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:16.809813Z","iopub.execute_input":"2025-08-01T11:51:16.810806Z","iopub.status.idle":"2025-08-01T11:51:16.839078Z","shell.execute_reply.started":"2025-08-01T11:51:16.810777Z","shell.execute_reply":"2025-08-01T11:51:16.837799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check missing value in dataset","metadata":{}},{"cell_type":"code","source":"A = df_train.isnull().sum()\nB = df_test.isnull().sum()\nprint(\"NaN value in train dataset\")\nprint(A)\nprint(\"-\"*100)\nprint(\"NaN value in test dataset\")\nprint(B)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:16.840331Z","iopub.execute_input":"2025-08-01T11:51:16.840772Z","iopub.status.idle":"2025-08-01T11:51:18.882593Z","shell.execute_reply.started":"2025-08-01T11:51:16.840724Z","shell.execute_reply":"2025-08-01T11:51:18.881369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"## Fill missing value","metadata":{}},{"cell_type":"code","source":"# Fill NaN values in df_train with the mean of each numerical column in df_train\nfor col in df_train.select_dtypes(include=np.number).columns:\n    if df_train[col].isnull().any(): # Check if there are any NaN values in the column\n        mean_val_train = df_train[col].mean()\n        df_train[col] = df_train[col].fillna(mean_val_train) # Removed inplace=True and assigned back\n        print(f\"Filled NaN in df_train['{col}'] with mean: {mean_val_train:.2f}\")\n\n# Fix: Assign the result of fillna back to the column\n# Fill NaN values in df_test with the mean of each numerical column in df_test\nfor col in df_test.select_dtypes(include=np.number).columns:\n    if df_test[col].isnull().any(): # Check if there are any NaN values in the column\n        mean_val_test = df_test[col].mean()\n        df_test[col] = df_test[col].fillna(mean_val_test) # Removed inplace=True and assigned back\n        print(f\"Filled NaN in df_test['{col}'] with mean: {mean_val_test:.2f}\")\n\nprint(\"\\n--- After NaN Imputation ---\")\nprint(\"df_train info:\")\ndf_train.info()\nprint(\"\\ndf_test info:\")\ndf_test.info()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:18.883778Z","iopub.execute_input":"2025-08-01T11:51:18.884003Z","iopub.status.idle":"2025-08-01T11:51:20.138749Z","shell.execute_reply.started":"2025-08-01T11:51:18.883986Z","shell.execute_reply":"2025-08-01T11:51:20.137859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sort dataset by date","metadata":{}},{"cell_type":"code","source":"def sort_dataframe_by_date(df, date_column):\n  \"\"\"\n  Sorts a pandas DataFrame by a specified date column in ascending order.\n\n  Args:\n    df: The pandas DataFrame to be sorted.\n    date_column: The name of the date column in the DataFrame.\n\n  Returns:\n    pandas.DataFrame: The sorted DataFrame.\n  \"\"\"\n\n  # Ensure the date column is in datetime format\n  df[date_column] = pd.to_datetime(df[date_column])\n\n  # Sort the DataFrame by the date column in ascending order\n  df = df.sort_values(by=date_column) \n\n  return df\n\n# Example usage:\n# Assuming 'df' is your DataFrame and the date column is named 'date'\ndf_train = sort_dataframe_by_date(df_train, 'date')\ndf_test = sort_dataframe_by_date(df_test, 'date')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:20.139875Z","iopub.execute_input":"2025-08-01T11:51:20.140253Z","iopub.status.idle":"2025-08-01T11:51:24.763461Z","shell.execute_reply.started":"2025-08-01T11:51:20.140139Z","shell.execute_reply":"2025-08-01T11:51:24.762407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"df_train shape before dropping columns:\", df_train.shape)\nprint(\"df_test shape before dropping columns:\", df_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:24.764545Z","iopub.execute_input":"2025-08-01T11:51:24.764991Z","iopub.status.idle":"2025-08-01T11:51:24.770453Z","shell.execute_reply.started":"2025-08-01T11:51:24.764964Z","shell.execute_reply":"2025-08-01T11:51:24.769363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Drop noise columns","metadata":{}},{"cell_type":"code","source":"columns_to_drop = [\n    'type_2_discount',\n    'date_holiday_flag',\n    'date_school_holidays_flag',\n    'date_shops_closed_flag',\n    'date_second_closed_day',\n    'date_winter_school_holidays_flag',\n    'date_day_after_closed_day',\n    'date_day_after_two_closed_days',\n    'type_5_discount',\n    'type_3_discount',\n    'type_1_discount',\n    'unique_id',\n    \"availability\" \n]\n\n# Drop columns from df_train\ndf_train = df_train.drop(columns=columns_to_drop, axis=1, errors='ignore')\n\n# Drop columns from df_test\ndf_test = df_test.drop(columns=columns_to_drop, axis=1, errors='ignore')\n\nprint(\"df_train shape after dropping columns:\", df_train.shape)\nprint(\"df_test shape after dropping columns:\", df_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:24.771524Z","iopub.execute_input":"2025-08-01T11:51:24.771803Z","iopub.status.idle":"2025-08-01T11:51:25.247675Z","shell.execute_reply.started":"2025-08-01T11:51:24.771782Z","shell.execute_reply":"2025-08-01T11:51:25.246772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split datase into train and test, encode categorical columns and date time columns, data normalization for nummerical columns, split features and target label","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df_train, df_test, target_col):\n    \"\"\"\n    Preprocesses df_train (fit and transform) and df_test (transform) for training and testing.\n\n    Args:\n        df_train: pandas DataFrame for training.\n        df_test: pandas DataFrame for testing.\n        target_col: Name of the target column (e.g., 'sales').\n\n    Returns:\n        x_train: Training features (float32 NumPy array).\n        y_train: Training targets (float32 NumPy array).\n        weight_train: Training weights (float32 NumPy array).\n        x_test: Test features (float32 NumPy array).\n        y_test: Test targets (float32 NumPy array).\n        weight_test: Test weights (float32 NumPy array).\n    \"\"\"\n    # Extract target and weights\n    y_train = df_train[target_col].values.astype(np.float32)\n    y_test = df_test[target_col].values.astype(np.float32)\n    weight_train = df_train['weight'].values.astype(np.float32)\n    weight_test = df_test['weight'].values.astype(np.float32)\n\n    # Extract features\n    x_train = df_train.drop([target_col, 'weight'], axis=1).copy()\n    x_test = df_test.drop([target_col, 'weight'], axis=1).copy()\n\n    # Handle datetime columns\n    datetime_cols = x_train.select_dtypes(include=['datetime']).columns\n    if len(datetime_cols) > 0:\n        for col in datetime_cols:\n            x_train[col + '_month'] = x_train[col].dt.month\n            x_train[col + '_day'] = x_train[col].dt.day\n            x_test[col + '_month'] = x_test[col].dt.month\n            x_test[col + '_day'] = x_test[col].dt.day\n        x_train = x_train.drop(datetime_cols, axis=1)\n        x_test = x_test.drop(datetime_cols, axis=1)\n\n    # Define categorical and numerical columns\n    categorical_cols = x_train.select_dtypes(include=['object', 'category']).columns\n    numeric_cols = x_train.select_dtypes(include=['number']).columns\n\n    # Encode categorical features\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n    if len(categorical_cols) > 0:\n        x_train[categorical_cols] = encoder.fit_transform(x_train[categorical_cols])\n        x_test[categorical_cols] = encoder.transform(x_test[categorical_cols])\n\n    # Scale numerical features\n    scaler = StandardScaler()\n    if len(numeric_cols) > 0:\n        x_train[numeric_cols] = scaler.fit_transform(x_train[numeric_cols])\n        x_test[numeric_cols] = scaler.transform(x_test[numeric_cols])\n\n    # Save encoder and scaler\n    joblib.dump(encoder, 'encoder.pkl')\n    joblib.dump(scaler, 'scaler.pkl')\n\n    # Convert to NumPy arrays\n    x_train = x_train.values.astype(np.float32)\n    x_test = x_test.values.astype(np.float32)\n\n    return x_train, y_train, weight_train, x_test, y_test, weight_test\n\n# Preprocess data\nx_train, y_train, weight_train, x_test, y_test, weight_test = preprocess_data(df_train, df_test, 'sales')\n\nprint(\"x_train shape:\", x_train.shape)\nprint(\"x_test shape:\", x_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:25.250085Z","iopub.execute_input":"2025-08-01T11:51:25.250413Z","iopub.status.idle":"2025-08-01T11:51:35.426355Z","shell.execute_reply.started":"2025-08-01T11:51:25.250389Z","shell.execute_reply":"2025-08-01T11:51:35.425095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split validation dataset and preprocess data for time series","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nSEQUENCE_LENGTH = 100\nBATCH_SIZE = 256\n\n# Preprocessing: Split data and scale targets\nval_split = 0.2\nval_size = int(len(x_train) * val_split)\nx_val, y_val, weights_val = x_train[-val_size:], y_train[-val_size:], weight_train[-val_size:]\nx_train, y_train, weights_train = x_train[:-val_size], y_train[:-val_size], weight_train[:-val_size]\n\n# Cast to float32\nx_train = x_train.astype(np.float32)\nx_val = x_val.astype(np.float32)\nx_test = x_test.astype(np.float32)\nweights_train = weights_train.astype(np.float32)\nweights_val = weights_val.astype(np.float32)\nweight_test = weight_test.astype(np.float32)\n\n# Scale targets\nscaler_y = StandardScaler()\ny_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten().astype(np.float32)\ny_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten().astype(np.float32)\ny_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten().astype(np.float32)\njoblib.dump(scaler_y, 'scaler_y.pkl')\n\n# Function to create time-series dataset with sample weights\ndef create_dataset(features, targets, weights, sequence_length, batch_size):\n    \"\"\"\n    Creates a time-series dataset with features, targets, and sample weights.\n\n    Args:\n        features (np.ndarray): Input features (float32).\n        targets (np.ndarray): Target values (float32).\n        weights (np.ndarray): Sample weights (float32).\n        sequence_length (int): Length of each sequence.\n        batch_size (int): Batch size.\n\n    Returns:\n        tf.data.Dataset: Dataset yielding (inputs, targets, sample_weights).\n        np.ndarray: Aligned weights.\n    \"\"\"\n    dataset = tf.keras.utils.timeseries_dataset_from_array(\n        data=features,\n        targets=targets,\n        sequence_length=sequence_length,\n        batch_size=batch_size\n    )\n    aligned_weights = weights[sequence_length - 1:] if len(weights) > sequence_length else weights\n    dataset = dataset.map(lambda x, y: (x, y, tf.gather(aligned_weights, tf.range(tf.shape(y)[0]), axis=0)))\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset, aligned_weights\n\n# Create datasets\ndataset_train, train_weights = create_dataset(x_train, y_train, weights_train, SEQUENCE_LENGTH, BATCH_SIZE)\ndataset_val, val_weights = create_dataset(x_val, y_val, weights_val, SEQUENCE_LENGTH, BATCH_SIZE)\ndataset_test, test_weights = create_dataset(x_test, y_test, weight_test, SEQUENCE_LENGTH, BATCH_SIZE)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:35.427461Z","iopub.execute_input":"2025-08-01T11:51:35.427899Z","iopub.status.idle":"2025-08-01T11:51:36.891247Z","shell.execute_reply.started":"2025-08-01T11:51:35.427869Z","shell.execute_reply":"2025-08-01T11:51:36.890116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## Model Architecture, model building, and model training - wavenet with transformer model","metadata":{}},{"cell_type":"markdown","source":"- This is a sophisticated deep learning model designed for a time-series prediction task, likely a regression problem. Its architecture combines the strengths of two distinct network types: WaveNet and Transformer.\n\n- The model's core is a series of WaveNet blocks with dilated causal convolutions. These blocks efficiently capture long-range dependencies in the sequential data without relying on recurrent layers. The use of a \"gating mechanism\" (tanh and sigmoid activation) helps the model decide which information to pass through. Skip connections from each block are aggregated and fed into the next part of the network, preserving information from different scales of the time series.\n\n- Following the WaveNet layers are two Transformer encoder layers. These layers, equipped with multi-head self-attention, further process the features extracted by the WaveNet portion. This allows the model to weigh the importance of different parts of the input sequence, capturing complex relationships and global context that might be missed by purely convolutional models.\n\n- The final part of the model is a multi-layered perceptron (MLP) with dense layers and dropout for regularization, culminating in a single output neuron. A custom ClipLayer is used to constrain the final prediction within a specific range of -10.0 to 10.0, which is useful for tasks with a defined output scale.\n\n- A key feature of this model is its custom loss function and metric: Weighted Mean Absolute Error (WMAE). This indicates that the model is trained to minimize prediction errors where certain data points are considered more important than others, as specified by a sample_weight parameter. The learning rate is dynamically adjusted using a cosine decay schedule with a warmup phase, which is a common and effective technique for training deep neural networks.","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nSEQUENCE_LENGTH = 100\nBATCH_SIZE = 256\nEPOCHS = 1\nBASE_LR = 3e-4\nDROPOUT_RATE = 0.3\nFILTERS = 256\nKERNEL_SIZE = 3\nDILATION_RATES = [1, 2, 4, 8, 16, 32]\n\n# Custom WMAE Loss Function\ndef custom_wmae_loss(y_true, y_pred, sample_weight=None):\n    \"\"\"\n    Weighted Mean Absolute Error loss function following evaluation criteria.\n    This version strictly interprets zero sample weights as zero contribution\n    and handles cases where the sum of weights is zero to prevent NaN.\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    if sample_weight is None:\n        sample_weight = tf.ones_like(y_true, dtype=tf.float32)\n    else:\n        sample_weight = tf.cast(sample_weight, tf.float32)\n    weighted_error = tf.abs(y_true - y_pred) * sample_weight\n    sum_of_weights = tf.reduce_sum(sample_weight)\n    return tf.cond(\n        tf.greater(sum_of_weights, 0),\n        lambda: tf.reduce_sum(weighted_error) / sum_of_weights,\n        lambda: tf.constant(0.0, dtype=tf.float32)\n    )\n\n# Custom WMAE Metric\nclass WeightedMAEMetric(Metric):\n    \"\"\"\n    Custom metric to compute Weighted Mean Absolute Error following evaluation criteria.\n    This version strictly interprets zero sample weights as zero contribution\n    and handles cases where the sum of weights is zero to prevent NaN.\n    \"\"\"\n    def __init__(self, name='wmae', **kwargs):\n        super(WeightedMAEMetric, self).__init__(name=name, **kwargs)\n        self.total_weighted_error = self.add_weight(name='total_weighted_error', initializer='zeros')\n        self.total_weights = self.add_weight(name='total_weights', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        if sample_weight is None:\n            sample_weight = tf.ones_like(y_true, dtype=tf.float32)\n        else:\n            sample_weight = tf.cast(sample_weight, tf.float32)\n        weighted_error = tf.abs(y_true - y_pred) * sample_weight\n        self.total_weighted_error.assign_add(tf.reduce_sum(weighted_error))\n        self.total_weights.assign_add(tf.reduce_sum(sample_weight))\n\n    def result(self):\n        return tf.cond(\n            tf.greater(self.total_weights, 0),\n            lambda: self.total_weighted_error / self.total_weights,\n            lambda: tf.constant(0.0, dtype=tf.float32)\n        )\n\n    def reset_state(self):\n        self.total_weighted_error.assign(0.0)\n        self.total_weights.assign(0.0)\n\n# Learning rate scheduler with warmup\n# Note: This assumes y_train is defined elsewhere; adjust accordingly if needed\nnum_train_sequences = max(0, len(y_train) - SEQUENCE_LENGTH + 1) if 'y_train' in globals() else 10000  # Placeholder value\ntotal_steps = EPOCHS * (num_train_sequences // BATCH_SIZE) if BATCH_SIZE > 0 else 0\nwarmup_steps = min(1000, total_steps // 10)\nlr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=BASE_LR,\n    decay_steps=max(1, total_steps - warmup_steps),\n    alpha=0.1\n)\noptimizer = Adam(learning_rate=lr_schedule)\n\n# WaveNet Block\ndef wavenet_block(x, dilation_rate, filters, kernel_size, dropout_rate):\n    \"\"\"\n    WaveNet block with dilated convolutions, gating, and residual/skip connections.\n    \"\"\"\n    conv_filter = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='tanh')(x)\n    conv_gate = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='sigmoid')(x)\n    gated_output = Multiply()([conv_filter, conv_gate])\n    gated_output = BatchNormalization()(gated_output)\n    residual = Conv1D(filters, 1, padding='same')(gated_output)\n    skip = Conv1D(filters, 1, padding='same')(gated_output)\n    if x.shape[-1] != filters:\n        x = Conv1D(filters, 1, padding='same')(x)\n    return Add()([x, residual]), skip\n\n# Transformer Encoder Layer\ndef transformer_encoder(x, num_heads=4, ff_dim=128, dropout_rate=0.3):\n    \"\"\"\n    Transformer encoder layer with multi-head attention and feed-forward network.\n    \"\"\"\n    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n    attn_output = Dropout(dropout_rate)(attn_output)\n    x = Add()([x, attn_output])\n    x = LayerNormalization(epsilon=1e-6)(x)\n    ff_output = Dense(ff_dim, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\n    ff_output = Dropout(dropout_rate)(ff_output)\n    ff_output = Dense(x.shape[-1])(ff_output)\n    x = Add()([x, ff_output])\n    x = LayerNormalization(epsilon=1e-6)(x)\n    return x\n\n# Custom Layer for Clipping\nclass ClipLayer(tf.keras.layers.Layer):\n    def __init__(self, min_value=-10.0, max_value=10.0, **kwargs):\n        super(ClipLayer, self).__init__(**kwargs)\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def call(self, inputs):\n        return tf.clip_by_value(inputs, self.min_value, self.max_value)\n\n# Build WaveNet + Transformer Model\nnum_features = x_train.shape[1] if 'x_train' in globals() else 19  # Placeholder value\ninputs = Input(shape=(SEQUENCE_LENGTH, num_features), name=\"input\")\nx = inputs\nskip_connections = []\n\n# WaveNet blocks\nfor dilation_rate in DILATION_RATES:\n    x, skip = wavenet_block(x, dilation_rate, FILTERS, KERNEL_SIZE, DROPOUT_RATE)\n    skip_connections.append(skip)\n\nx = Add()(skip_connections)\nx = Activation('relu')(x)\nx = BatchNormalization()(x)\n\n# Transformer encoder layers\nx = transformer_encoder(x, num_heads=4, ff_dim=128, dropout_rate=0.3)\nx = transformer_encoder(x, num_heads=4, ff_dim=128, dropout_rate=0.3)\n\n# Final layers\nx = Conv1D(FILTERS, 1, activation='relu')(x)\nx = GlobalAveragePooling1D()(x)\nx = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\nx = Dropout(DROPOUT_RATE)(x)\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\nx = Dropout(DROPOUT_RATE)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\nx = Dropout(DROPOUT_RATE)(x)\n\n# Apply clipping using the custom layer\noutputs = ClipLayer(min_value=-10.0, max_value=10.0)(Dense(1, activation='linear', name=\"output\", dtype='float32')(x))\n\n# Build and compile model\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.compile(\n    optimizer=optimizer,\n    loss=custom_wmae_loss,\n    metrics=['mae'],\n    weighted_metrics=[WeightedMAEMetric()]\n)\n\nmodel.summary()\n\n# Train the model with sample weights\nprint(\"\\n--- Training Model ---\")\nhistory = model.fit(\n    dataset_train,  # Ensure dataset_train is defined\n    validation_data=dataset_val,  # Ensure dataset_val is defined\n    epochs=EPOCHS,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n    ],\n    shuffle=False\n)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T11:51:36.892361Z","iopub.execute_input":"2025-08-01T11:51:36.892696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Remork that** : eaxmple of the model above is train only one epoch due to limited time of save version on Kaggle, but actually you have to set number of epoch to 200 epochs.","metadata":{}},{"cell_type":"markdown","source":"## Save the model","metadata":{}},{"cell_type":"code","source":"# --- Save the model in Keras format (.keras) ---\n# Define the path for the .keras file\nkeras_model_path = \"wavenet_transformer_model.keras\"\n\n# Save the model\nmodel.save(keras_model_path, overwrite=True)\nprint(f\"\\nModel saved successfully to '{keras_model_path}' in Keras format.\")\n\n# --- Load the model from Keras format (.keras) ---\nprint(f\"\\n--- Loading Model from '{keras_model_path}' ---\")\nloaded_model = tf.keras.models.load_model(\n    keras_model_path,\n    custom_objects={\n        'custom_wmae_loss': custom_wmae_loss,\n        'WeightedMAEMetric': WeightedMAEMetric\n    }\n)\nprint(\"Model loaded successfully!\")\nloaded_model.summary()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Model evaluation on regression metrics","metadata":{}},{"cell_type":"code","source":"# --- Evaluate the model on the test dataset ---\nprint(\"\\n--- Evaluating Model on Test Set (Scaled) ---\")\n# model.evaluate directly uses the loss and metrics defined in model.compile\n# The output will include the custom WMAE metric.\nevaluation_results = model.evaluate(dataset_test)\nprint(f\"Test Set Evaluation Results (Scaled):\")\nfor name, value in zip(model.metrics_names, evaluation_results):\n    print(f\"{name}: {value:.4f}\")\n\n# --- Make predictions and calculate unscaled metrics ---\nprint(\"\\n--- Making Predictions and Calculating Unscaled Metrics ---\")\n\n# Initialize lists for true values and predictions (scaled)\ny_true_scaled_list, y_pred_scaled_list, sample_weights_list = [], [], []\n\n# Iterate through the test dataset to get predictions and corresponding true values and weights\nfor x_batch, y_batch_scaled, weights_batch in dataset_test:\n    # Handle NaNs in y_batch_scaled (true values) before extending\n    y_batch_scaled_processed = y_batch_scaled.numpy()\n    if np.any(np.isnan(y_batch_scaled_processed)):\n        print(f\"Warning: NaN values detected in true scaled targets. Replacing with 0 for metric calculation.\")\n        y_batch_scaled_processed = np.nan_to_num(y_batch_scaled_processed, nan=0.0)\n\n    y_pred_batch_scaled = model.predict(x_batch, verbose=0) # Disable verbose logging\n\n    # --- NaN Handling for predictions: Replace NaNs with 0 before further processing ---\n    if np.any(np.isnan(y_pred_batch_scaled)):\n        print(f\"Warning: NaN values detected in model predictions. Replacing with 0 for metric calculation.\")\n        y_pred_batch_scaled = np.nan_to_num(y_pred_batch_scaled, nan=0.0) # Replace NaNs with 0\n\n    y_true_scaled_list.extend(y_batch_scaled_processed) # Use processed true values\n    y_pred_scaled_list.extend(y_pred_batch_scaled.flatten())\n    sample_weights_list.extend(weights_batch.numpy())\n\n# Convert lists to NumPy arrays\ny_true_scaled = np.array(y_true_scaled_list)\ny_pred_scaled = np.array(y_pred_scaled_list)\nsample_weights_for_metrics = np.array(sample_weights_list)\n\n# Inverse transform predictions and true values to original scale\n# Apply nan_to_num after inverse_transform as well, in case scaling produces NaNs\ny_true_unscaled = np.nan_to_num(scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten(), nan=0.0)\ny_pred_unscaled = np.nan_to_num(scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten(), nan=0.0)\n\n# Ensure sample_weights_for_metrics matches the length of predictions after inverse transform\n# This is crucial because timeseries_dataset_from_array might drop initial samples\n# and the number of predictions will match the number of targets in the dataset.\n# The `create_dataset` function already aligns `aligned_weights` to the number of sequences\n# that will be generated, so `sample_weights_for_metrics` should already be correctly aligned.\n\n# Compute evaluation metrics on unscaled data\nmae_unscaled = mean_absolute_error(y_true_unscaled, y_pred_unscaled)\nmse_unscaled = mean_squared_error(y_true_unscaled, y_pred_unscaled)\nrmse_unscaled = np.sqrt(mse_unscaled)\n\n# Calculate WMAE on unscaled data for verification (should be similar to scaled WMAE if scaler is linear)\n# Only consider samples with non-zero weights for WMAE calculation to avoid division by zero\nnon_zero_weight_indices = sample_weights_for_metrics > 0\nif np.sum(non_zero_weight_indices) > 0:\n    wmae_unscaled = np.sum(np.abs(y_true_unscaled[non_zero_weight_indices] - y_pred_unscaled[non_zero_weight_indices]) * sample_weights_for_metrics[non_zero_weight_indices]) / np.sum(sample_weights_for_metrics[non_zero_weight_indices])\nelse:\n    wmae_unscaled = 0.0 # Handle case where all weights are zero\n\n# Print results\nprint(f\"Evaluation Metrics on Test Set (Unscaled):\")\nprint(f\"MAE  : {mae_unscaled:.4f}\")\nprint(f\"MSE  : {mse_unscaled:.4f}\")\nprint(f\"RMSE : {rmse_unscaled:.4f}\")\nprint(f\"WMAE : {wmae_unscaled:.4f}\")\n","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}