{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88742,"databundleVersionId":10173359,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition overview","metadata":{}},{"cell_type":"markdown","source":"**Competition Summary**\n\n- This is the Rohlik Sales Forecasting Challenge, a time-series forecasting competition hosted on Kaggle. The primary objective is to predict the sales volume for various inventory items across 11 different Rohlik Group warehouses for a period of 14 days.\n\n- Accurate forecasts are vital for the e-grocery company's operations, as they directly impact supply chain efficiency, inventory management, and overall sustainability by minimizing waste.\n\n- The model's performance will be evaluated using the Weighted Mean Absolute Error (WMAE). The specific weights for each inventory item are provided in a separate file. The competition runs from November 15, 2024, to February 15, 2025, and offers cash prizes for the top three competitors.\n\n- The dataset includes historical sales and order data, product metadata, and a calendar with holiday information. Some features available in the training set (e.g., sales and availability) are intentionally removed from the test set, as they would not be known at the time of a real-world prediction.","metadata":{}},{"cell_type":"markdown","source":"### [Link to competition](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)","metadata":{}},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"This data dictionary describes the files and columns provided for the competition.","metadata":{}},{"cell_type":"markdown","source":"**sales_train.csv and sales_test.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a specific inventory item in a specific warehouse.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">date</td>\n    <td class=\"tg-7zrl\">The date of the sales record.</td>\n    <td class=\"tg-7zrl\">Date</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse where the item is stored.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">total_orders</td>\n    <td class=\"tg-7zrl\">The historical number of orders for the selected warehouse.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sales</td>\n    <td class=\"tg-7zrl\">The target variable: sales volume (pcs or kg).</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sell_price_main</td>\n    <td class=\"tg-7zrl\">The selling price of the item.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">availability</td>\n    <td class=\"tg-7zrl\">The proportion of the day the item was available. A value of 1 means it was available all day.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">type_0_discount, type_1_discount, etc.</td>\n    <td class=\"tg-7zrl\">The percentage discount offered for various promotion types. Negative values indicate no discount.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**inventory.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a specific inventory item.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">product_unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for a product, shared across all warehouses.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">name</td>\n    <td class=\"tg-7zrl\">The name of the product.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">L1_category_name, L2_category_name, etc.</td>\n    <td class=\"tg-7zrl\">Hierarchical category names for the product. L4 is the most granular.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse where the inventory item is located.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**calendar.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">warehouse</td>\n    <td class=\"tg-7zrl\">The name of the warehouse.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">date</td>\n    <td class=\"tg-7zrl\">The date of the calendar event.</td>\n    <td class=\"tg-7zrl\">Date</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">holiday_name</td>\n    <td class=\"tg-7zrl\">The name of the public holiday, if applicable.</td>\n    <td class=\"tg-7zrl\">String</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">holiday</td>\n    <td class=\"tg-7zrl\">A binary flag (0 or 1) indicating if the date is a holiday.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">shops_closed</td>\n    <td class=\"tg-7zrl\">A flag indicating a public holiday where most shops are closed.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">winter_school_holidays</td>\n    <td class=\"tg-7zrl\">A flag for winter school holidays.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">school_holidays</td>\n    <td class=\"tg-7zrl\">A flag for general school holidays.</td>\n    <td class=\"tg-7zrl\">Boolean</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**test_weights.csv**","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">unique_id</td>\n    <td class=\"tg-7zrl\">A unique identifier for the inventory item.</td>\n    <td class=\"tg-7zrl\">Integer</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">weight</td>\n    <td class=\"tg-7zrl\">The weight used for calculating the Weighted Mean Absolute Error (WMAE) metric for this item.</td>\n    <td class=\"tg-7zrl\">Float</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"### [Link to dataset](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/data)","metadata":{}},{"cell_type":"markdown","source":"# Pipeline Overview","metadata":{}},{"cell_type":"markdown","source":"The pipeline for the Rohlik Sales Forecasting Model is a comprehensive, end-to-end machine learning workflow that predicts product sales using a hybrid deep learning architecture. It starts with data loading and merging from various sources, including historical sales, inventory, and calendar data, which is then enriched with custom holiday and temporal features. After a chronological train-test split, the data undergoes exploratory data analysis (EDA), where missing values are imputed, and features are prepared and encoded. Feature engineering further refines the dataset by dropping irrelevant columns and applying a preprocessing pipeline for scaling and encoding. The processed data is then prepared into time-series datasets for a WaveNet + Transformer hybrid model, which combines dilated causal convolutions with multi-head self-attention to capture both local and global temporal patterns. The model is trained using a custom weighted loss function to handle imbalanced data, and its performance is evaluated on unscaled metrics like RMSE and WMAE.\n\n**Key Stages and Components**\n\n**1. Data Preparation and Enrichment**\n\nThe pipeline begins by loading and merging several datasets (`sales_train.csv`, `inventory.csv`, `sales_test.csv`, and `test_weights.csv`). The `calendar.csv` is also loaded and enriched with specific custom holidays for different locations (e.g., Prague, Munich). This process creates new features like `days_to_next_holiday` and `post_closure_days`, which are crucial for capturing the impact of events on sales. The data is then filtered to include only relevant time periods and locations before all datasets are joined together into a single master dataframe.\n\n**2. Preprocessing and Feature Engineering**\n\nA chronological train-test split ensures the model is evaluated on future data, mimicking a real-world scenario. Initial EDA checks for missing values, which are imputed with the column mean. Key features, including product IDs, warehouse locations, and categories, are encoded using `OrdinalEncoder`. The `date` column is transformed into cyclical features (e.g., `sin_month`, `cos_month`) to represent seasonality effectively. Based on Mutual Information (MI) analysis and domain knowledge, low-relevance features (like most discount types and unique IDs) are dropped to reduce noise and model complexity. The final preprocessing pipeline scales numerical features with `StandardScaler` and saves these transformers for future use.\n\n**3. Modeling and Training**\n\nThe core of the pipeline is a hybrid deep learning model that combines WaveNet and Transformer architectures.\n\n- **WaveNet blocks** use dilated causal convolutions to capture local, short-term dependencies in the time series without a large number of layers.\n- **Transformer encoders** use **multi-head and self-attention** to model long-range dependencies, allowing the model to weigh the importance of different time steps in the sequence.\n\nThe model is trained using a custom Weighted MAE loss function, which assigns higher penalties to errors on more critical predictions based on the `test_weights.csv`. An Adam optimizer with a cosine decay learning rate schedule is used, and training incorporates Early Stopping to prevent overfitting.\n\n**4. Evaluation**\n\nAfter training, the model's performance is evaluated on the test dataset. The pipeline computes both **scaled metrics** (like WMAE on the model's direct output) and **unscaled metrics** (like MAE and RMSE) for better interpretability. The unscaled metrics are calculated after inverse-scaling the predictions back to their original sales values, providing a clear measure of how well the model predicts actual sales numbers. This comprehensive evaluation ensures the model is not only accurate but also robust for weighted samples, a key requirement of the challenge.","metadata":{}},{"cell_type":"markdown","source":"# Model submission notebook","metadata":{}},{"cell_type":"markdown","source":"### [Model submission notebook](https://www.kaggle.com/code/misterfour/rohlik-sales-forecasting-challenge-submission)\n### [Reference! (add holidays calendar of each country into dataset)](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-24T09:00:33.662677Z","iopub.execute_input":"2025-08-24T09:00:33.662930Z","iopub.status.idle":"2025-08-24T09:00:33.935440Z","shell.execute_reply.started":"2025-08-24T09:00:33.662906Z","shell.execute_reply":"2025-08-24T09:00:33.934578Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv\n/kaggle/input/rohlik-sales-forecasting-challenge-v2/solution.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import ( # Consolidated Keras layers\n    Input, Conv1D, Multiply, Add, Dense, Dropout, LayerNormalization,\n    MultiHeadAttention, GlobalAveragePooling1D, Activation, BatchNormalization\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow.keras import mixed_precision # For mixed precision policy, if used\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error # For evaluation metrics\nfrom datetime import datetime\nimport joblib # For saving/loading models or other objects\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom joblib import Parallel, delayed\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Disable GPU usage and force CPU only (if desired)\n# This block should typically come after TensorFlow import but before model definition/training\ntf.config.set_visible_devices([], 'GPU')  # Prevent TensorFlow from using GPU\nphysical_devices = tf.config.list_physical_devices('CPU')\nassert len(physical_devices) > 0, \"No CPU devices found\"\ntf.config.set_logical_device_configuration(\n    physical_devices[0],\n    [tf.config.LogicalDeviceConfiguration()]\n)\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:00:33.936827Z","iopub.execute_input":"2025-08-24T09:00:33.937201Z","iopub.status.idle":"2025-08-24T09:00:49.567030Z","shell.execute_reply.started":"2025-08-24T09:00:33.937155Z","shell.execute_reply":"2025-08-24T09:00:49.566465Z"}},"outputs":[{"name":"stderr","text":"2025-08-24 09:00:35.657793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756026035.882814      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756026035.952800      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_train.csv', parse_dates=['date'])\ninventory = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/inventory.csv')\nsubmission = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/sales_test.csv', parse_dates=['date'])\nweights = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/test_weights.csv')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:00:49.573096Z","iopub.execute_input":"2025-08-24T09:00:49.573405Z","iopub.status.idle":"2025-08-24T09:00:56.520213Z","shell.execute_reply.started":"2025-08-24T09:00:49.573388Z","shell.execute_reply":"2025-08-24T09:00:56.519422Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Merge Dataset and Calendar Enrichment","metadata":{}},{"cell_type":"markdown","source":"This step handles the crucial data enrichment and merging steps of the sales forecasting pipeline, focusing on creating and integrating new temporal features. It specifically addresses the need for detailed holiday and closure information, which can significantly influence sales patterns.\n\n**Custom Holiday and Calendar Enrichment**\n\nThe initial section of the code defines lists of custom holiday dates, such as Easter Day and Mother Day, for specific warehouses in the Czech Republic, Brno, Munich, and Frankfurt. The `fill_loss_holidays` function then takes these lists and populates the holiday and `holiday_name` columns in a base calendar DataFrame, ensuring these important dates are captured for their respective locations.\n\nThe `enrich_calendar` function is a key part of this process. It generates several new, highly predictive temporal features by analyzing the dates within the calendar. These include:\n\n- `date_days_to_next_holiday`: The number of days remaining until the next observed holiday.\n- `date_days_to_shops_closed`: The number of days until the next shop closure.\n- `date_day_after_closed_day`: A binary flag that is 1 if the current day is the day after a shop was closed.\n- `date_second_closed_day`: A binary flag that indicates if a day is part of a multi-day shop closure (e.g., the second day of a long weekend).\n\nAfter enriching the calendar, the code renames the new columns to a consistent format (e.g., `date_holiday_name, date_days_to_next_holiday`) for better clarity and organization.\n\n**Dataset Stacking and Merging**\n\nThe stack_datasets function is a core utility that combines all the disparate data sources into a single, cohesive DataFrame. It performs a series of merges:\n\n- It first merges the main sales data with the newly enriched calendar data on the date and warehouse columns.\n- Next, it joins the result with the inventory data on unique_id and warehouse to incorporate product-specific details.\n- Finally, it merges the weights data on unique_id, which is critical for the model's weighted loss function.\n\nThis function ensures that all relevant features—from product prices and inventory to temporal holidays and closure flags—are available in a single table for the subsequent modeling steps. ","metadata":{}},{"cell_type":"code","source":"# Additional holiday days\n\nczech_holiday = [ \n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"),\n]\nbrno_holiday = [\n    (['03/31/2024', '04/09/2023', '04/17/2022', '04/04/2021', '04/12/2020'], 'Easter Day'),\n    (['05/12/2024', '05/10/2020', '05/09/2021', '05/08/2022', '05/14/2023'], \"Mother Day\"),\n]\nmunich_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),\n]\nfrankfurt_holidays = [\n    (['03/30/2024', '04/08/2023', '04/16/2022', '04/03/2021'], 'Holy Saturday'),\n    (['05/12/2024', '05/14/2023', '05/08/2022', '05/09/2021'], 'Mother Day'),\n]\n\n# Functions\n\ndef fill_loss_holidays(df_fill, warehouses, holidays):\n    df = df_fill.copy()\n    for item in holidays:\n        dates, holiday_name = item\n        generated_dates = [datetime.strptime(date, '%m/%d/%Y').strftime('%Y-%m-%d') for date in dates]\n        for generated_date in generated_dates:\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday'] = 1\n            df.loc[(df['warehouse'].isin(warehouses)) & (df['date'] == generated_date), 'holiday_name'] = holiday_name\n    return df\n\ndef enrich_calendar(df):\n    df = df.sort_values('date').reset_index(drop=True)\n\n    # Number of days until next holiday\n    df['next_holiday_date'] = df.loc[df['holiday'] == 1, 'date'].shift(-1)\n    # Fill NaT values by using the next valid observation to fill the gap\n    df['next_holiday_date'] = df['next_holiday_date'].bfill() \n    df['date_days_to_next_holiday'] = (df['next_holiday_date'] - df['date']).dt.days\n    df.drop(columns=['next_holiday_date'], inplace=True)\n\n    # Number of days until shops are closed\n    df['next_shops_closed_date'] = df.loc[df['shops_closed'] == 1, 'date'].shift(-1)\n    df['next_shops_closed_date'] = df['next_shops_closed_date'].bfill()\n    df['date_days_to_shops_closed'] = (df['next_shops_closed_date'] - df['date']).dt.days\n    df.drop(columns=['next_shops_closed_date'], inplace=True)\n\n    # Was the shop closed yesterday?\n    df['date_day_after_closed_day'] = ((df['shops_closed'] == 0) & (df['shops_closed'].shift(1) == 1)).astype(int)\n\n    # Are shops closed today and were they also closed yesterday (e.g., December 26 in Germany)?\n    df['date_second_closed_day'] = ((df['shops_closed'] == 1) & (df['shops_closed'].shift(1) == 1)).astype(int)\n\n    # Was the shop closed the last two days?\n    df['date_day_after_two_closed_days'] = ((df['shops_closed'] == 0) & (df['date_second_closed_day'].shift(1) == 1)).astype(int)\n\n    return df\n\n#calendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ncalendar = pd.read_csv('/kaggle/input/rohlik-sales-forecasting-challenge-v2/calendar.csv', parse_dates=['date'])\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Prague_1', 'Prague_2', 'Prague_3'], holidays=czech_holiday)\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Brno_1'], holidays=brno_holiday)\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Munich_1'], holidays=munich_holidays)\ncalendar = fill_loss_holidays(df_fill=calendar, warehouses=['Frankfurt_1'], holidays=frankfurt_holidays)\n\ncalendar_enriched = pd.DataFrame()\n\nfor location in ['Frankfurt_1', 'Prague_2', 'Brno_1', 'Munich_1', 'Prague_3', 'Prague_1', 'Budapest_1']:\n    calendar_enriched = pd.concat([\n        calendar_enriched,enrich_calendar(calendar.query('date >= \"2020-08-01 00:00:00\" and warehouse ==@location'))])\ncalendar_enriched.loc[:,'year'] = calendar_enriched['date'].dt.year\ncalendar_enriched.sort_values('date')[['date','holiday_name','shops_closed','warehouse','date_days_to_next_holiday']].head(5)\n\ncalendar_enriched = calendar_enriched.rename(columns={\n    'holiday_name':'date_holiday_name',\n    'year':'date_year',\n    'holiday':'date_holiday_flag',\n    'holiday':'date_holiday_flag',\n    'shops_closed':'date_shops_closed_flag',\n    'winter_school_holidays':'date_winter_school_holidays_flag',\n    'school_holidays':'date_school_holidays_flag',\n})\n\ndef stack_datasets(df, calendar_extended, inventory, weights):\n    \"\"\"\n    Stacks the given DataFrame with additional data from calendar, inventory, and weights.\n\n    Args:\n        df: The main DataFrame to be stacked.\n        calendar_extended: DataFrame containing calendar-related information.\n        inventory: DataFrame containing inventory information.\n        weights: DataFrame containing weight information for unique IDs.\n\n    Returns:\n        pandas.DataFrame: The stacked DataFrame.\n    \"\"\"\n    # Merge with calendar_extended on date and warehouse\n    df = df.merge(calendar_extended, on=['date', 'warehouse'], how='left')\n    \n    # Merge with inventory on unique_id and warehouse\n    df = df.merge(inventory, on=['unique_id', 'warehouse'], how='left')\n    \n    # Perform a VLOOKUP-style merge with weights on unique_id\n    df = df.merge(weights, on='unique_id', how='left')\n    \n    # Ensure 'date' is in datetime format\n    df['date'] = pd.to_datetime(df['date'])\n    \n    return df\n\ndf_train = stack_datasets(train, calendar_enriched, inventory, weights)\ndf_train\n\n# Fill 'date_holiday_name' with 'Working Day' where it's NaN\ndf_train['date_holiday_name'] = df_train['date_holiday_name'].fillna('Working Day')\ndf_train","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:00:56.521062Z","iopub.execute_input":"2025-08-24T09:00:56.521348Z","iopub.status.idle":"2025-08-24T09:01:03.390386Z","shell.execute_reply.started":"2025-08-24T09:00:56.521321Z","shell.execute_reply":"2025-08-24T09:01:03.389630Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         unique_id       date   warehouse  total_orders  sales  \\\n0             4845 2024-03-10  Budapest_1        6436.0  16.34   \n1             4845 2021-05-25  Budapest_1        4663.0  12.63   \n2             4845 2021-12-20  Budapest_1        6507.0  34.55   \n3             4845 2023-04-29  Budapest_1        5463.0  34.52   \n4             4845 2022-04-01  Budapest_1        5997.0  35.92   \n...            ...        ...         ...           ...    ...   \n4007414       4941 2023-06-21    Prague_1        9988.0  26.56   \n4007415       4941 2023-06-24    Prague_1        8518.0  27.42   \n4007416       4941 2023-06-23    Prague_1       10424.0  33.39   \n4007417       4941 2023-06-22    Prague_1       10342.0  22.88   \n4007418       4941 2023-06-20    Prague_1        9613.0  32.10   \n\n         sell_price_main  availability  type_0_discount  type_1_discount  \\\n0                 646.26          1.00          0.00000              0.0   \n1                 455.96          1.00          0.00000              0.0   \n2                 455.96          1.00          0.00000              0.0   \n3                 646.26          0.96          0.20024              0.0   \n4                 486.41          1.00          0.00000              0.0   \n...                  ...           ...              ...              ...   \n4007414            34.06          1.00          0.00000              0.0   \n4007415            34.06          1.00          0.00000              0.0   \n4007416            34.06          1.00          0.00000              0.0   \n4007417            34.06          1.00          0.00000              0.0   \n4007418            34.06          1.00          0.00000              0.0   \n\n         type_2_discount  ...  date_second_closed_day  \\\n0                    0.0  ...                       0   \n1                    0.0  ...                       0   \n2                    0.0  ...                       0   \n3                    0.0  ...                       0   \n4                    0.0  ...                       0   \n...                  ...  ...                     ...   \n4007414              0.0  ...                       0   \n4007415              0.0  ...                       0   \n4007416              0.0  ...                       0   \n4007417              0.0  ...                       0   \n4007418              0.0  ...                       0   \n\n         date_day_after_two_closed_days  date_year  product_unique_id  \\\n0                                     0       2024               2375   \n1                                     1       2021               2375   \n2                                     0       2021               2375   \n3                                     0       2023               2375   \n4                                     0       2022               2375   \n...                                 ...        ...                ...   \n4007414                               0       2023               2422   \n4007415                               0       2023               2422   \n4007416                               0       2023               2422   \n4007417                               0       2023               2422   \n4007418                               0       2023               2422   \n\n                 name  L1_category_name_en       L2_category_name_en  \\\n0        Croissant_35               Bakery              Bakery_L2_18   \n1        Croissant_35               Bakery              Bakery_L2_18   \n2        Croissant_35               Bakery              Bakery_L2_18   \n3        Croissant_35               Bakery              Bakery_L2_18   \n4        Croissant_35               Bakery              Bakery_L2_18   \n...               ...                  ...                       ...   \n4007414    Kohlrabi_9  Fruit and vegetable  Fruit and vegetable_L2_3   \n4007415    Kohlrabi_9  Fruit and vegetable  Fruit and vegetable_L2_3   \n4007416    Kohlrabi_9  Fruit and vegetable  Fruit and vegetable_L2_3   \n4007417    Kohlrabi_9  Fruit and vegetable  Fruit and vegetable_L2_3   \n4007418    Kohlrabi_9  Fruit and vegetable  Fruit and vegetable_L2_3   \n\n                L3_category_name_en       L4_category_name_en    weight  \n0                      Bakery_L3_83               Bakery_L4_1  1.925596  \n1                      Bakery_L3_83               Bakery_L4_1  1.925596  \n2                      Bakery_L3_83               Bakery_L4_1  1.925596  \n3                      Bakery_L3_83               Bakery_L4_1  1.925596  \n4                      Bakery_L3_83               Bakery_L4_1  1.925596  \n...                             ...                       ...       ...  \n4007414  Fruit and vegetable_L3_114  Fruit and vegetable_L4_1  2.262646  \n4007415  Fruit and vegetable_L3_114  Fruit and vegetable_L4_1  2.262646  \n4007416  Fruit and vegetable_L3_114  Fruit and vegetable_L4_1  2.262646  \n4007417  Fruit and vegetable_L3_114  Fruit and vegetable_L4_1  2.262646  \n4007418  Fruit and vegetable_L3_114  Fruit and vegetable_L4_1  2.262646  \n\n[4007419 rows x 32 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>date</th>\n      <th>warehouse</th>\n      <th>total_orders</th>\n      <th>sales</th>\n      <th>sell_price_main</th>\n      <th>availability</th>\n      <th>type_0_discount</th>\n      <th>type_1_discount</th>\n      <th>type_2_discount</th>\n      <th>...</th>\n      <th>date_second_closed_day</th>\n      <th>date_day_after_two_closed_days</th>\n      <th>date_year</th>\n      <th>product_unique_id</th>\n      <th>name</th>\n      <th>L1_category_name_en</th>\n      <th>L2_category_name_en</th>\n      <th>L3_category_name_en</th>\n      <th>L4_category_name_en</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4845</td>\n      <td>2024-03-10</td>\n      <td>Budapest_1</td>\n      <td>6436.0</td>\n      <td>16.34</td>\n      <td>646.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2024</td>\n      <td>2375</td>\n      <td>Croissant_35</td>\n      <td>Bakery</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4845</td>\n      <td>2021-05-25</td>\n      <td>Budapest_1</td>\n      <td>4663.0</td>\n      <td>12.63</td>\n      <td>455.96</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2021</td>\n      <td>2375</td>\n      <td>Croissant_35</td>\n      <td>Bakery</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4845</td>\n      <td>2021-12-20</td>\n      <td>Budapest_1</td>\n      <td>6507.0</td>\n      <td>34.55</td>\n      <td>455.96</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021</td>\n      <td>2375</td>\n      <td>Croissant_35</td>\n      <td>Bakery</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4845</td>\n      <td>2023-04-29</td>\n      <td>Budapest_1</td>\n      <td>5463.0</td>\n      <td>34.52</td>\n      <td>646.26</td>\n      <td>0.96</td>\n      <td>0.20024</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2375</td>\n      <td>Croissant_35</td>\n      <td>Bakery</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4845</td>\n      <td>2022-04-01</td>\n      <td>Budapest_1</td>\n      <td>5997.0</td>\n      <td>35.92</td>\n      <td>486.41</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2022</td>\n      <td>2375</td>\n      <td>Croissant_35</td>\n      <td>Bakery</td>\n      <td>Bakery_L2_18</td>\n      <td>Bakery_L3_83</td>\n      <td>Bakery_L4_1</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4007414</th>\n      <td>4941</td>\n      <td>2023-06-21</td>\n      <td>Prague_1</td>\n      <td>9988.0</td>\n      <td>26.56</td>\n      <td>34.06</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2422</td>\n      <td>Kohlrabi_9</td>\n      <td>Fruit and vegetable</td>\n      <td>Fruit and vegetable_L2_3</td>\n      <td>Fruit and vegetable_L3_114</td>\n      <td>Fruit and vegetable_L4_1</td>\n      <td>2.262646</td>\n    </tr>\n    <tr>\n      <th>4007415</th>\n      <td>4941</td>\n      <td>2023-06-24</td>\n      <td>Prague_1</td>\n      <td>8518.0</td>\n      <td>27.42</td>\n      <td>34.06</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2422</td>\n      <td>Kohlrabi_9</td>\n      <td>Fruit and vegetable</td>\n      <td>Fruit and vegetable_L2_3</td>\n      <td>Fruit and vegetable_L3_114</td>\n      <td>Fruit and vegetable_L4_1</td>\n      <td>2.262646</td>\n    </tr>\n    <tr>\n      <th>4007416</th>\n      <td>4941</td>\n      <td>2023-06-23</td>\n      <td>Prague_1</td>\n      <td>10424.0</td>\n      <td>33.39</td>\n      <td>34.06</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2422</td>\n      <td>Kohlrabi_9</td>\n      <td>Fruit and vegetable</td>\n      <td>Fruit and vegetable_L2_3</td>\n      <td>Fruit and vegetable_L3_114</td>\n      <td>Fruit and vegetable_L4_1</td>\n      <td>2.262646</td>\n    </tr>\n    <tr>\n      <th>4007417</th>\n      <td>4941</td>\n      <td>2023-06-22</td>\n      <td>Prague_1</td>\n      <td>10342.0</td>\n      <td>22.88</td>\n      <td>34.06</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2422</td>\n      <td>Kohlrabi_9</td>\n      <td>Fruit and vegetable</td>\n      <td>Fruit and vegetable_L2_3</td>\n      <td>Fruit and vegetable_L3_114</td>\n      <td>Fruit and vegetable_L4_1</td>\n      <td>2.262646</td>\n    </tr>\n    <tr>\n      <th>4007418</th>\n      <td>4941</td>\n      <td>2023-06-20</td>\n      <td>Prague_1</td>\n      <td>9613.0</td>\n      <td>32.10</td>\n      <td>34.06</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2422</td>\n      <td>Kohlrabi_9</td>\n      <td>Fruit and vegetable</td>\n      <td>Fruit and vegetable_L2_3</td>\n      <td>Fruit and vegetable_L3_114</td>\n      <td>Fruit and vegetable_L4_1</td>\n      <td>2.262646</td>\n    </tr>\n  </tbody>\n</table>\n<p>4007419 rows × 32 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Split Dataset","metadata":{}},{"cell_type":"markdown","source":"This code performs a chronological train-test split on a time-series dataset, a critical step in time-series forecasting. The core concept is to prevent data leakage by ensuring that the model is trained only on past data and evaluated exclusively on future data. This mimics a real-world scenario where a model trained on historical information is used to make predictions for an unseen future. The code splits a DataFrame (`df_split`) into two parts: a training set and a testing set.\n\nSplit Index Calculation: It first calculates a split index, which is 80% of the total number of rows.\n\n     train_split_index = int(len(df_split) * 0.80)\n\nChronological Slicing: It then uses this index to slice the DataFrame. The training set (`df_train`) contains the first 80% of the data, representing the earliest time periods. The testing set (`df_test`) contains the remaining 20%, representing the most recent time periods. The use of .iloc ensures that the slicing is based on row position, maintaining the chronological order.\n\n     df_train = df_split.iloc[:train_split_index].copy()\n     df_test = df_split.iloc[train_split_index:].copy()","metadata":{}},{"cell_type":"markdown","source":"**Theories and Concepts**\n\n- **Preventing Data Leakage:** A random split could include future data points in the training set. For example, a random split might use sales data from December 2024 to train a model that is then tested on sales from October 2024. This data leakage would give the model an unfair advantage, as it has \"seen the future,\" leading to overly optimistic performance metrics that won't hold up in a real forecasting situation.\n- **Mimicking Real-World Forecasting:** A chronological split ensures the model's performance on the test set is a reliable proxy for its performance on future, unseen data. This method provides an honest evaluation of the model's ability to generalize to new time periods, which is the ultimate goal of any forecasting model.","metadata":{}},{"cell_type":"code","source":"# Calculate the split index for 80% training data\n# The remaining 20% will be for the test set\ndf_split = df_train.copy()\ntrain_split_index = int(len(df_split) * 0.80)\n# Split the DataFrame chronologically\n# IMPORTANT: Perform slicing on the original df_train for both new dataframes\ndf_train = df_split.iloc[:train_split_index].copy()\ndf_test = df_split.iloc[train_split_index:].copy() \n\nprint(\"New df_train shape (80%):\", df_train.shape)\nprint(\"New df_test shape (20%):\", df_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:03.391199Z","iopub.execute_input":"2025-08-24T09:01:03.391784Z","iopub.status.idle":"2025-08-24T09:01:05.530060Z","shell.execute_reply.started":"2025-08-24T09:01:03.391761Z","shell.execute_reply":"2025-08-24T09:01:05.529328Z"}},"outputs":[{"name":"stdout","text":"New df_train shape (80%): (3205935, 32)\nNew df_test shape (20%): (801484, 32)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"This step performs Exploratory Data Analysis (EDA) to prepare the sales forecasting dataset for modeling. The process involves handling missing values, encoding categorical and temporal features, and using a statistical method to assess feature relevance.","metadata":{}},{"cell_type":"markdown","source":"**Missing Value Imputation**\n\nThe first step is a missing value check using `df.isnull().sum()`. The output shows that the `total_orders` and sales columns in both the training and test sets have a small number of NaN (Not a Number) values.\n\n**Theory: Data Integrity and Imputation**\n\nMissing data can cause errors or lead to biased results in machine learning models. The code addresses this using imputation, a technique to fill in missing values. The chosen method is to replace NaNs with the column mean. This is a simple and common strategy, particularly when the number of missing values is small, as it preserves the overall mean of the feature and prevents data loss. The output confirms that all NaNs in the `total_orders` and `sales` columns of the training set are successfully filled.","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:05.531018Z","iopub.execute_input":"2025-08-24T09:01:05.531302Z","iopub.status.idle":"2025-08-24T09:01:06.642509Z","shell.execute_reply.started":"2025-08-24T09:01:05.531274Z","shell.execute_reply":"2025-08-24T09:01:06.641738Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"unique_id                            0\ndate                                 0\nwarehouse                            0\ntotal_orders                        34\nsales                               34\nsell_price_main                      0\navailability                         0\ntype_0_discount                      0\ntype_1_discount                      0\ntype_2_discount                      0\ntype_3_discount                      0\ntype_4_discount                      0\ntype_5_discount                      0\ntype_6_discount                      0\ndate_holiday_name                    0\ndate_holiday_flag                    0\ndate_shops_closed_flag               0\ndate_winter_school_holidays_flag     0\ndate_school_holidays_flag            0\ndate_days_to_next_holiday            0\ndate_days_to_shops_closed            0\ndate_day_after_closed_day            0\ndate_second_closed_day               0\ndate_day_after_two_closed_days       0\ndate_year                            0\nproduct_unique_id                    0\nname                                 0\nL1_category_name_en                  0\nL2_category_name_en                  0\nL3_category_name_en                  0\nL4_category_name_en                  0\nweight                               0\ndtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:06.643612Z","iopub.execute_input":"2025-08-24T09:01:06.643977Z","iopub.status.idle":"2025-08-24T09:01:06.954842Z","shell.execute_reply.started":"2025-08-24T09:01:06.643940Z","shell.execute_reply":"2025-08-24T09:01:06.954221Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"unique_id                            0\ndate                                 0\nwarehouse                            0\ntotal_orders                        18\nsales                               18\nsell_price_main                      0\navailability                         0\ntype_0_discount                      0\ntype_1_discount                      0\ntype_2_discount                      0\ntype_3_discount                      0\ntype_4_discount                      0\ntype_5_discount                      0\ntype_6_discount                      0\ndate_holiday_name                    0\ndate_holiday_flag                    0\ndate_shops_closed_flag               0\ndate_winter_school_holidays_flag     0\ndate_school_holidays_flag            0\ndate_days_to_next_holiday            0\ndate_days_to_shops_closed            0\ndate_day_after_closed_day            0\ndate_second_closed_day               0\ndate_day_after_two_closed_days       0\ndate_year                            0\nproduct_unique_id                    0\nname                                 0\nL1_category_name_en                  0\nL2_category_name_en                  0\nL3_category_name_en                  0\nL4_category_name_en                  0\nweight                               0\ndtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Identify numerical columns (excluding non-numeric types like 'object' or 'datetime')\n# Only numeric columns can have a mean calculated for filling.\nnumeric_cols = df_train.select_dtypes(include=np.number).columns\n\n# Fill NaN values in numeric columns with the mean of their respective columns\nfor col in numeric_cols:\n    if df_train[col].isnull().any(): # Check if there are any NaNs in the column\n        col_mean = df_train[col].mean()\n        df_train[col] = df_train[col].fillna(col_mean)\n        print(f\"\\nFilled NaNs in '{col}' with mean: {col_mean:.2f}\")\n\nprint(\"\\ndf_train shape after filling NaNs:\", df_train.shape)\nprint(\"\\nNaN count per column after filling:\")\nprint(df_train.isnull().sum())","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:06.955878Z","iopub.execute_input":"2025-08-24T09:01:06.956129Z","iopub.status.idle":"2025-08-24T09:01:08.454248Z","shell.execute_reply.started":"2025-08-24T09:01:06.956089Z","shell.execute_reply":"2025-08-24T09:01:08.453628Z"}},"outputs":[{"name":"stdout","text":"\nFilled NaNs in 'total_orders' with mean: 6093.52\n\nFilled NaNs in 'sales' with mean: 112.54\n\ndf_train shape after filling NaNs: (3205935, 32)\n\nNaN count per column after filling:\nunique_id                           0\ndate                                0\nwarehouse                           0\ntotal_orders                        0\nsales                               0\nsell_price_main                     0\navailability                        0\ntype_0_discount                     0\ntype_1_discount                     0\ntype_2_discount                     0\ntype_3_discount                     0\ntype_4_discount                     0\ntype_5_discount                     0\ntype_6_discount                     0\ndate_holiday_name                   0\ndate_holiday_flag                   0\ndate_shops_closed_flag              0\ndate_winter_school_holidays_flag    0\ndate_school_holidays_flag           0\ndate_days_to_next_holiday           0\ndate_days_to_shops_closed           0\ndate_day_after_closed_day           0\ndate_second_closed_day              0\ndate_day_after_two_closed_days      0\ndate_year                           0\nproduct_unique_id                   0\nname                                0\nL1_category_name_en                 0\nL2_category_name_en                 0\nL3_category_name_en                 0\nL4_category_name_en                 0\nweight                              0\ndtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Feature and Target Separation","metadata":{}},{"cell_type":"code","source":"df_tr_corr = df_train.copy()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:08.456353Z","iopub.execute_input":"2025-08-24T09:01:08.456543Z","iopub.status.idle":"2025-08-24T09:01:09.093372Z","shell.execute_reply.started":"2025-08-24T09:01:08.456529Z","shell.execute_reply":"2025-08-24T09:01:09.092616Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"featurex = df_tr_corr.drop(['sales'], axis=1)\nfeaturey = df_tr_corr[['sales']]\nprint(\"featurex\", featurex.shape)\nprint(\"featurey\", featurey.shape)\nprint('-------------------------------------------------------------------------')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:09.094231Z","iopub.execute_input":"2025-08-24T09:01:09.094498Z","iopub.status.idle":"2025-08-24T09:01:09.503473Z","shell.execute_reply.started":"2025-08-24T09:01:09.094476Z","shell.execute_reply":"2025-08-24T09:01:09.502867Z"}},"outputs":[{"name":"stdout","text":"featurex (3205935, 31)\nfeaturey (3205935, 1)\n-------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Feature Encoding for Mutual Information (MI) Analysis","metadata":{}},{"cell_type":"markdown","source":"**Categorical Encoding:** \n\nThe OrdinalEncoder is used to convert categorical string columns (`warehouse, date_holiday_name, and product categories`) into integers. Ordinal encoding assigns a unique integer to each unique category. While this method implies an order, which may not exist, it's a simple way to make the data model-compatible.","metadata":{}},{"cell_type":"code","source":"# Get a list of column names with string data type\nstring_columns = df_tr_corr.select_dtypes(include=['object']).columns.tolist() \n\nprint(string_columns)  # Output: ['name', 'city', 'country']","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:09.504147Z","iopub.execute_input":"2025-08-24T09:01:09.504368Z","iopub.status.idle":"2025-08-24T09:01:09.741942Z","shell.execute_reply.started":"2025-08-24T09:01:09.504351Z","shell.execute_reply":"2025-08-24T09:01:09.741199Z"}},"outputs":[{"name":"stdout","text":"['warehouse', 'date_holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"encoder = OrdinalEncoder()\nfeaturex[['warehouse', 'date_holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']] = encoder.fit_transform(featurex[['warehouse', 'date_holiday_name', 'name', 'L1_category_name_en', 'L2_category_name_en', 'L3_category_name_en', 'L4_category_name_en']])\nfeaturex","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:09.742627Z","iopub.execute_input":"2025-08-24T09:01:09.742900Z","iopub.status.idle":"2025-08-24T09:01:15.290449Z","shell.execute_reply.started":"2025-08-24T09:01:09.742875Z","shell.execute_reply":"2025-08-24T09:01:15.289667Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         unique_id       date  warehouse  total_orders  sell_price_main  \\\n0             4845 2024-03-10        1.0        6436.0           646.26   \n1             4845 2021-05-25        1.0        4663.0           455.96   \n2             4845 2021-12-20        1.0        6507.0           455.96   \n3             4845 2023-04-29        1.0        5463.0           646.26   \n4             4845 2022-04-01        1.0        5997.0           486.41   \n...            ...        ...        ...           ...              ...   \n3205930        833 2020-08-06        4.0        5380.0            23.81   \n3205931        833 2023-03-13        4.0       10032.0            36.10   \n3205932        833 2022-08-26        4.0        8686.0            36.10   \n3205933        833 2022-10-11        4.0        9043.0            25.26   \n3205934        833 2021-08-21        4.0        6181.0            25.26   \n\n         availability  type_0_discount  type_1_discount  type_2_discount  \\\n0                1.00          0.00000              0.0              0.0   \n1                1.00          0.00000              0.0              0.0   \n2                1.00          0.00000              0.0              0.0   \n3                0.96          0.20024              0.0              0.0   \n4                1.00          0.00000              0.0              0.0   \n...               ...              ...              ...              ...   \n3205930          1.00          0.00000              0.0              0.0   \n3205931          0.21          0.00000              0.0              0.0   \n3205932          0.34          0.00000              0.0              0.0   \n3205933          1.00          0.00000              0.0              0.0   \n3205934          1.00          0.00000              0.0              0.0   \n\n         type_3_discount  ...  date_second_closed_day  \\\n0                    0.0  ...                       0   \n1                    0.0  ...                       0   \n2                    0.0  ...                       0   \n3                    0.0  ...                       0   \n4                    0.0  ...                       0   \n...                  ...  ...                     ...   \n3205930              0.0  ...                       0   \n3205931              0.0  ...                       0   \n3205932              0.0  ...                       0   \n3205933              0.0  ...                       0   \n3205934              0.0  ...                       0   \n\n         date_day_after_two_closed_days  date_year  product_unique_id    name  \\\n0                                     0       2024               2375   665.0   \n1                                     1       2021               2375   665.0   \n2                                     0       2021               2375   665.0   \n3                                     0       2023               2375   665.0   \n4                                     0       2022               2375   665.0   \n...                                 ...        ...                ...     ...   \n3205930                               0       2020                435  1005.0   \n3205931                               0       2023                435  1005.0   \n3205932                               0       2022                435  1005.0   \n3205933                               0       2022                435  1005.0   \n3205934                               0       2021                435  1005.0   \n\n         L1_category_name_en  L2_category_name_en  L3_category_name_en  \\\n0                        0.0                  2.0                 52.0   \n1                        0.0                  2.0                 52.0   \n2                        0.0                  2.0                 52.0   \n3                        0.0                  2.0                 52.0   \n4                        0.0                  2.0                 52.0   \n...                      ...                  ...                  ...   \n3205930                  1.0                 25.0                 92.0   \n3205931                  1.0                 25.0                 92.0   \n3205932                  1.0                 25.0                 92.0   \n3205933                  1.0                 25.0                 92.0   \n3205934                  1.0                 25.0                 92.0   \n\n         L4_category_name_en    weight  \n0                        0.0  1.925596  \n1                        0.0  1.925596  \n2                        0.0  1.925596  \n3                        0.0  1.925596  \n4                        0.0  1.925596  \n...                      ...       ...  \n3205930                 10.0  2.438045  \n3205931                 10.0  2.438045  \n3205932                 10.0  2.438045  \n3205933                 10.0  2.438045  \n3205934                 10.0  2.438045  \n\n[3205935 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>date</th>\n      <th>warehouse</th>\n      <th>total_orders</th>\n      <th>sell_price_main</th>\n      <th>availability</th>\n      <th>type_0_discount</th>\n      <th>type_1_discount</th>\n      <th>type_2_discount</th>\n      <th>type_3_discount</th>\n      <th>...</th>\n      <th>date_second_closed_day</th>\n      <th>date_day_after_two_closed_days</th>\n      <th>date_year</th>\n      <th>product_unique_id</th>\n      <th>name</th>\n      <th>L1_category_name_en</th>\n      <th>L2_category_name_en</th>\n      <th>L3_category_name_en</th>\n      <th>L4_category_name_en</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4845</td>\n      <td>2024-03-10</td>\n      <td>1.0</td>\n      <td>6436.0</td>\n      <td>646.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2024</td>\n      <td>2375</td>\n      <td>665.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4845</td>\n      <td>2021-05-25</td>\n      <td>1.0</td>\n      <td>4663.0</td>\n      <td>455.96</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2021</td>\n      <td>2375</td>\n      <td>665.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4845</td>\n      <td>2021-12-20</td>\n      <td>1.0</td>\n      <td>6507.0</td>\n      <td>455.96</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021</td>\n      <td>2375</td>\n      <td>665.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4845</td>\n      <td>2023-04-29</td>\n      <td>1.0</td>\n      <td>5463.0</td>\n      <td>646.26</td>\n      <td>0.96</td>\n      <td>0.20024</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>2375</td>\n      <td>665.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4845</td>\n      <td>2022-04-01</td>\n      <td>1.0</td>\n      <td>5997.0</td>\n      <td>486.41</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2022</td>\n      <td>2375</td>\n      <td>665.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3205930</th>\n      <td>833</td>\n      <td>2020-08-06</td>\n      <td>4.0</td>\n      <td>5380.0</td>\n      <td>23.81</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2020</td>\n      <td>435</td>\n      <td>1005.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n    </tr>\n    <tr>\n      <th>3205931</th>\n      <td>833</td>\n      <td>2023-03-13</td>\n      <td>4.0</td>\n      <td>10032.0</td>\n      <td>36.10</td>\n      <td>0.21</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2023</td>\n      <td>435</td>\n      <td>1005.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n    </tr>\n    <tr>\n      <th>3205932</th>\n      <td>833</td>\n      <td>2022-08-26</td>\n      <td>4.0</td>\n      <td>8686.0</td>\n      <td>36.10</td>\n      <td>0.34</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2022</td>\n      <td>435</td>\n      <td>1005.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n    </tr>\n    <tr>\n      <th>3205933</th>\n      <td>833</td>\n      <td>2022-10-11</td>\n      <td>4.0</td>\n      <td>9043.0</td>\n      <td>25.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2022</td>\n      <td>435</td>\n      <td>1005.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n    </tr>\n    <tr>\n      <th>3205934</th>\n      <td>833</td>\n      <td>2021-08-21</td>\n      <td>4.0</td>\n      <td>6181.0</td>\n      <td>25.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021</td>\n      <td>435</td>\n      <td>1005.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n    </tr>\n  </tbody>\n</table>\n<p>3205935 rows × 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"**Cyclical Feature Encoding:** \n\nThe `encode_datetime` function transforms the date column. It extracts basic features like year, month, and day. More importantly, it uses sine and cosine transformations to create `sin_month, cos_month, sin_day, and cos_day` features. This is based on the theory that time-based data (`months or days of the week`) is cyclical, with no beginning or end. Representing them as a single number (e.g., month 12 is followed by month 1) can mislead a model. Sine and cosine transformations map these values to a continuous circle, correctly capturing the cyclical nature and allowing the model to understand the relationship between months or days at the start and end of a cycle.","metadata":{}},{"cell_type":"code","source":"def encode_datetime(df, datetime_column):\n  \"\"\"\n  Encodes datetime features in a pandas DataFrame.\n\n  Args:\n    df: The pandas DataFrame containing the datetime column.\n    datetime_column: The name of the datetime column in the DataFrame.\n\n  Returns:\n    pandas.DataFrame: The DataFrame with encoded datetime features.  \n  \"\"\"\n\n  df[datetime_column] = pd.to_datetime(df[datetime_column]) \n\n  # Extract features\n  df['year'] = df[datetime_column].dt.year\n  df['month'] = df[datetime_column].dt.month\n  df['day'] = df[datetime_column].dt.day\n\n  # Create cyclical features (optional)\n  df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n  df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n  df['sin_day'] = np.sin(2 * np.pi * df['day'] / 31)\n  df['cos_day'] = np.cos(2 * np.pi * df['day'] / 31)\n\n  # Drop the original datetime column (optional)\n  df.drop(datetime_column, axis=1, inplace=True)\n\n  return df\n\n# Example usage:\n# Assuming 'df' is your DataFrame and 'date_time' is the name of your datetime column\nfeaturex = encode_datetime(featurex, 'date')\nfeaturex","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:15.291470Z","iopub.execute_input":"2025-08-24T09:01:15.291815Z","iopub.status.idle":"2025-08-24T09:01:17.503306Z","shell.execute_reply.started":"2025-08-24T09:01:15.291789Z","shell.execute_reply":"2025-08-24T09:01:17.502630Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"         unique_id  warehouse  total_orders  sell_price_main  availability  \\\n0             4845        1.0        6436.0           646.26          1.00   \n1             4845        1.0        4663.0           455.96          1.00   \n2             4845        1.0        6507.0           455.96          1.00   \n3             4845        1.0        5463.0           646.26          0.96   \n4             4845        1.0        5997.0           486.41          1.00   \n...            ...        ...           ...              ...           ...   \n3205930        833        4.0        5380.0            23.81          1.00   \n3205931        833        4.0       10032.0            36.10          0.21   \n3205932        833        4.0        8686.0            36.10          0.34   \n3205933        833        4.0        9043.0            25.26          1.00   \n3205934        833        4.0        6181.0            25.26          1.00   \n\n         type_0_discount  type_1_discount  type_2_discount  type_3_discount  \\\n0                0.00000              0.0              0.0              0.0   \n1                0.00000              0.0              0.0              0.0   \n2                0.00000              0.0              0.0              0.0   \n3                0.20024              0.0              0.0              0.0   \n4                0.00000              0.0              0.0              0.0   \n...                  ...              ...              ...              ...   \n3205930          0.00000              0.0              0.0              0.0   \n3205931          0.00000              0.0              0.0              0.0   \n3205932          0.00000              0.0              0.0              0.0   \n3205933          0.00000              0.0              0.0              0.0   \n3205934          0.00000              0.0              0.0              0.0   \n\n         type_4_discount  ...  L3_category_name_en  L4_category_name_en  \\\n0                0.15312  ...                 52.0                  0.0   \n1                0.15025  ...                 52.0                  0.0   \n2                0.15025  ...                 52.0                  0.0   \n3                0.15312  ...                 52.0                  0.0   \n4                0.15649  ...                 52.0                  0.0   \n...                  ...  ...                  ...                  ...   \n3205930          0.00000  ...                 92.0                 10.0   \n3205931          0.00000  ...                 92.0                 10.0   \n3205932          0.00000  ...                 92.0                 10.0   \n3205933          0.00000  ...                 92.0                 10.0   \n3205934          0.00000  ...                 92.0                 10.0   \n\n           weight  year  month  day     sin_month     cos_month   sin_day  \\\n0        1.925596  2024      3   10  1.000000e+00  6.123234e-17  0.897805   \n1        1.925596  2021      5   25  5.000000e-01 -8.660254e-01 -0.937752   \n2        1.925596  2021     12   20 -2.449294e-16  1.000000e+00 -0.790776   \n3        1.925596  2023      4   29  8.660254e-01 -5.000000e-01 -0.394356   \n4        1.925596  2022      4    1  8.660254e-01 -5.000000e-01  0.201299   \n...           ...   ...    ...  ...           ...           ...       ...   \n3205930  2.438045  2020      8    6 -8.660254e-01 -5.000000e-01  0.937752   \n3205931  2.438045  2023      3   13  1.000000e+00  6.123234e-17  0.485302   \n3205932  2.438045  2022      8   26 -8.660254e-01 -5.000000e-01 -0.848644   \n3205933  2.438045  2022     10   11 -8.660254e-01  5.000000e-01  0.790776   \n3205934  2.438045  2021      8   21 -8.660254e-01 -5.000000e-01 -0.897805   \n\n          cos_day  \n0       -0.440394  \n1        0.347305  \n2       -0.612106  \n3        0.918958  \n4        0.979530  \n...           ...  \n3205930  0.347305  \n3205931 -0.874347  \n3205932  0.528964  \n3205933 -0.612106  \n3205934 -0.440394  \n\n[3205935 rows x 37 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>warehouse</th>\n      <th>total_orders</th>\n      <th>sell_price_main</th>\n      <th>availability</th>\n      <th>type_0_discount</th>\n      <th>type_1_discount</th>\n      <th>type_2_discount</th>\n      <th>type_3_discount</th>\n      <th>type_4_discount</th>\n      <th>...</th>\n      <th>L3_category_name_en</th>\n      <th>L4_category_name_en</th>\n      <th>weight</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>sin_month</th>\n      <th>cos_month</th>\n      <th>sin_day</th>\n      <th>cos_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4845</td>\n      <td>1.0</td>\n      <td>6436.0</td>\n      <td>646.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15312</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n      <td>2024</td>\n      <td>3</td>\n      <td>10</td>\n      <td>1.000000e+00</td>\n      <td>6.123234e-17</td>\n      <td>0.897805</td>\n      <td>-0.440394</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4845</td>\n      <td>1.0</td>\n      <td>4663.0</td>\n      <td>455.96</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15025</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n      <td>2021</td>\n      <td>5</td>\n      <td>25</td>\n      <td>5.000000e-01</td>\n      <td>-8.660254e-01</td>\n      <td>-0.937752</td>\n      <td>0.347305</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4845</td>\n      <td>1.0</td>\n      <td>6507.0</td>\n      <td>455.96</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15025</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n      <td>2021</td>\n      <td>12</td>\n      <td>20</td>\n      <td>-2.449294e-16</td>\n      <td>1.000000e+00</td>\n      <td>-0.790776</td>\n      <td>-0.612106</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4845</td>\n      <td>1.0</td>\n      <td>5463.0</td>\n      <td>646.26</td>\n      <td>0.96</td>\n      <td>0.20024</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15312</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n      <td>2023</td>\n      <td>4</td>\n      <td>29</td>\n      <td>8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>-0.394356</td>\n      <td>0.918958</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4845</td>\n      <td>1.0</td>\n      <td>5997.0</td>\n      <td>486.41</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.15649</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>1.925596</td>\n      <td>2022</td>\n      <td>4</td>\n      <td>1</td>\n      <td>8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>0.201299</td>\n      <td>0.979530</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3205930</th>\n      <td>833</td>\n      <td>4.0</td>\n      <td>5380.0</td>\n      <td>23.81</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n      <td>2020</td>\n      <td>8</td>\n      <td>6</td>\n      <td>-8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>0.937752</td>\n      <td>0.347305</td>\n    </tr>\n    <tr>\n      <th>3205931</th>\n      <td>833</td>\n      <td>4.0</td>\n      <td>10032.0</td>\n      <td>36.10</td>\n      <td>0.21</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n      <td>2023</td>\n      <td>3</td>\n      <td>13</td>\n      <td>1.000000e+00</td>\n      <td>6.123234e-17</td>\n      <td>0.485302</td>\n      <td>-0.874347</td>\n    </tr>\n    <tr>\n      <th>3205932</th>\n      <td>833</td>\n      <td>4.0</td>\n      <td>8686.0</td>\n      <td>36.10</td>\n      <td>0.34</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n      <td>2022</td>\n      <td>8</td>\n      <td>26</td>\n      <td>-8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>-0.848644</td>\n      <td>0.528964</td>\n    </tr>\n    <tr>\n      <th>3205933</th>\n      <td>833</td>\n      <td>4.0</td>\n      <td>9043.0</td>\n      <td>25.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n      <td>2022</td>\n      <td>10</td>\n      <td>11</td>\n      <td>-8.660254e-01</td>\n      <td>5.000000e-01</td>\n      <td>0.790776</td>\n      <td>-0.612106</td>\n    </tr>\n    <tr>\n      <th>3205934</th>\n      <td>833</td>\n      <td>4.0</td>\n      <td>6181.0</td>\n      <td>25.26</td>\n      <td>1.00</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>92.0</td>\n      <td>10.0</td>\n      <td>2.438045</td>\n      <td>2021</td>\n      <td>8</td>\n      <td>21</td>\n      <td>-8.660254e-01</td>\n      <td>-5.000000e-01</td>\n      <td>-0.897805</td>\n      <td>-0.440394</td>\n    </tr>\n  </tbody>\n</table>\n<p>3205935 rows × 37 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Mutual Information (MI) Analysis","metadata":{}},{"cell_type":"markdown","source":"The step is to perform a correlation analysis to understand the relationship between features and the target variable (`sales`). The code uses `mutual_info_regression`, which calculates Mutual Information (MI) scores.\n\n**Theory: Mutual Information vs. Correlation**\n\nTraditional correlation metrics like Pearson's correlation coefficient measure only linear relationships between two variables. Mutual Information (MI) is a more powerful and general concept from information theory. It measures the reduction in uncertainty about one variable given the value of another. In simpler terms, it quantifies the dependency between two variables, regardless of whether the relationship is linear or non-linear.\n\n- **High MI Score:** A high score indicates that knowing the value of a feature significantly reduces the uncertainty about the value of the target.\n- **Low MI Score:** A low score indicates that the feature has little to no predictive power on its own.\n\nThe output for Aggregated Mutual Information Scores provides a ranked list of features. It confirms `that total_orders, unique_id, and name` are highly relevant, as they have the highest MI scores. Conversely, features like different discount types and certain holiday flags have very low scores, indicating a weak relationship with sales. This analysis is crucial for feature selection, allowing the data scientist to focus on the most informative variables and potentially discard irrelevant ones to improve model performance and reduce training time.","metadata":{}},{"cell_type":"code","source":"# Flatten featurey to a one-dimensional format\n#y_flattened = featurey.values.ravel()  # Converts to 1D array\n\n# For demonstration, let's print the flattened featurey\n#print(\"Flattened featurey:\", y_flattened)\n\n# Calculate mutual information scores for each feature with respect to each target\nX, y = featurex, featurey\ndef calculate_mi_for_target(target_index):\n    return mutual_info_regression(X, y.iloc[:, target_index], random_state=42)\n\n# Step 3: Run parallelized MI calculations on the original y (multi-dimensional)\nmi_scores = Parallel(n_jobs=-1)(delayed(calculate_mi_for_target)(i) for i in range(y.shape[1]))\n\n# Step 4: Convert the list of scores to a DataFrame\nmi_scores_df = pd.DataFrame(mi_scores, columns=featurex.columns, index=featurey.columns)\nprint(\"\\nMutual Information Scores for each feature with respect to each target:\")\nmi_scores_df\n\n# Step 5: Aggregate scores across all targets (e.g., by averaging)\naggregated_mi_scores = mi_scores_df.mean(axis=0).sort_values(ascending=False)\nprint(\"\\nAggregated Mutual Information Scores:\")\naggregated_mi_scores","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:01:17.504014Z","iopub.execute_input":"2025-08-24T09:01:17.504317Z","iopub.status.idle":"2025-08-24T09:30:01.367990Z","shell.execute_reply.started":"2025-08-24T09:01:17.504292Z","shell.execute_reply":"2025-08-24T09:30:01.367273Z"}},"outputs":[{"name":"stdout","text":"\nMutual Information Scores for each feature with respect to each target:\n\nAggregated Mutual Information Scores:\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"total_orders                        1.237632\nunique_id                           0.870261\nname                                0.738922\nproduct_unique_id                   0.721629\nweight                              0.568456\nsell_price_main                     0.300177\ndate_days_to_shops_closed           0.280129\nL3_category_name_en                 0.220470\ndate_days_to_next_holiday           0.212774\nL2_category_name_en                 0.097007\nL4_category_name_en                 0.077303\nday                                 0.069964\nsin_day                             0.069611\nwarehouse                           0.066437\ncos_day                             0.035973\nmonth                               0.033994\ntype_0_discount                     0.030659\ndate_holiday_name                   0.030613\nL1_category_name_en                 0.025312\ntype_6_discount                     0.019307\ntype_4_discount                     0.018949\navailability                        0.018790\nsin_month                           0.018741\ndate_year                           0.017800\nyear                                0.017409\ncos_month                           0.017237\ndate_holiday_flag                   0.005246\ntype_2_discount                     0.005094\ndate_shops_closed_flag              0.004046\ndate_winter_school_holidays_flag    0.003603\ndate_school_holidays_flag           0.002979\ndate_second_closed_day              0.001881\ndate_day_after_closed_day           0.001440\ntype_5_discount                     0.001073\ndate_day_after_two_closed_days      0.001056\ntype_3_discount                     0.000112\ntype_1_discount                     0.000041\ndtype: float64"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"mi_scores_df.T.describe()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:01.368817Z","iopub.execute_input":"2025-08-24T09:30:01.369090Z","iopub.status.idle":"2025-08-24T09:30:01.384851Z","shell.execute_reply.started":"2025-08-24T09:30:01.369064Z","shell.execute_reply":"2025-08-24T09:30:01.384128Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"           sales\ncount  37.000000\nmean    0.157895\nstd     0.291355\nmin     0.000041\n25%     0.005094\n50%     0.025312\n75%     0.097007\nmax     1.237632","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>37.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.157895</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.291355</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000041</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.005094</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.025312</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.097007</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.237632</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"aggregated_mi_scores.describe()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:01.385647Z","iopub.execute_input":"2025-08-24T09:30:01.385898Z","iopub.status.idle":"2025-08-24T09:30:01.402078Z","shell.execute_reply.started":"2025-08-24T09:30:01.385874Z","shell.execute_reply":"2025-08-24T09:30:01.401560Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"count    37.000000\nmean      0.157895\nstd       0.291355\nmin       0.000041\n25%       0.005094\n50%       0.025312\n75%       0.097007\nmax       1.237632\ndtype: float64"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"mi_scores_df","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:01.402716Z","iopub.execute_input":"2025-08-24T09:30:01.402909Z","iopub.status.idle":"2025-08-24T09:30:01.431009Z","shell.execute_reply.started":"2025-08-24T09:30:01.402894Z","shell.execute_reply":"2025-08-24T09:30:01.430484Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"       unique_id  warehouse  total_orders  sell_price_main  availability  \\\nsales   0.870261   0.066437      1.237632         0.300177       0.01879   \n\n       type_0_discount  type_1_discount  type_2_discount  type_3_discount  \\\nsales         0.030659         0.000041         0.005094         0.000112   \n\n       type_4_discount  ...  L3_category_name_en  L4_category_name_en  \\\nsales         0.018949  ...              0.22047             0.077303   \n\n         weight      year     month       day  sin_month  cos_month   sin_day  \\\nsales  0.568456  0.017409  0.033994  0.069964   0.018741   0.017237  0.069611   \n\n        cos_day  \nsales  0.035973  \n\n[1 rows x 37 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>warehouse</th>\n      <th>total_orders</th>\n      <th>sell_price_main</th>\n      <th>availability</th>\n      <th>type_0_discount</th>\n      <th>type_1_discount</th>\n      <th>type_2_discount</th>\n      <th>type_3_discount</th>\n      <th>type_4_discount</th>\n      <th>...</th>\n      <th>L3_category_name_en</th>\n      <th>L4_category_name_en</th>\n      <th>weight</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>sin_month</th>\n      <th>cos_month</th>\n      <th>sin_day</th>\n      <th>cos_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sales</th>\n      <td>0.870261</td>\n      <td>0.066437</td>\n      <td>1.237632</td>\n      <td>0.300177</td>\n      <td>0.01879</td>\n      <td>0.030659</td>\n      <td>0.000041</td>\n      <td>0.005094</td>\n      <td>0.000112</td>\n      <td>0.018949</td>\n      <td>...</td>\n      <td>0.22047</td>\n      <td>0.077303</td>\n      <td>0.568456</td>\n      <td>0.017409</td>\n      <td>0.033994</td>\n      <td>0.069964</td>\n      <td>0.018741</td>\n      <td>0.017237</td>\n      <td>0.069611</td>\n      <td>0.035973</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 37 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"The step focuses on preparing a time-series dataset for a machine learning model, specifically addressing missing values, feature selection, and data scaling.","metadata":{}},{"cell_type":"markdown","source":"## Missing Value Imputation","metadata":{}},{"cell_type":"markdown","source":"**Missing Value Imputation**\n\nThe code first checks for and then handles missing values (NaNs) in the total_orders and sales columns of both the `df_train` and `df_test` dataframes. It iterates through all numerical columns and fills any NaN values with the mean of that column. This is a simple and standard method of imputation.\n\n**Theory: Data Integrity and Imputation**\n\nMachine learning models require complete data. Missing values can cause models to fail or produce biased results. Imputation is the process of filling in these gaps. Using the mean is a common technique that maintains the central tendency of the data. The output confirms that this step successfully fills all missing values, making the data ready for further processing.","metadata":{}},{"cell_type":"markdown","source":"### Check missing value in dataset","metadata":{}},{"cell_type":"code","source":"A = df_train.isnull().sum()\nB = df_test.isnull().sum()\nprint(\"NaN value in train dataset\")\nprint(A)\nprint(\"-\"*100)\nprint(\"NaN value in test dataset\")\nprint(B)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:01.431711Z","iopub.execute_input":"2025-08-24T09:30:01.431928Z","iopub.status.idle":"2025-08-24T09:30:02.786529Z","shell.execute_reply.started":"2025-08-24T09:30:01.431903Z","shell.execute_reply":"2025-08-24T09:30:02.785952Z"}},"outputs":[{"name":"stdout","text":"NaN value in train dataset\nunique_id                           0\ndate                                0\nwarehouse                           0\ntotal_orders                        0\nsales                               0\nsell_price_main                     0\navailability                        0\ntype_0_discount                     0\ntype_1_discount                     0\ntype_2_discount                     0\ntype_3_discount                     0\ntype_4_discount                     0\ntype_5_discount                     0\ntype_6_discount                     0\ndate_holiday_name                   0\ndate_holiday_flag                   0\ndate_shops_closed_flag              0\ndate_winter_school_holidays_flag    0\ndate_school_holidays_flag           0\ndate_days_to_next_holiday           0\ndate_days_to_shops_closed           0\ndate_day_after_closed_day           0\ndate_second_closed_day              0\ndate_day_after_two_closed_days      0\ndate_year                           0\nproduct_unique_id                   0\nname                                0\nL1_category_name_en                 0\nL2_category_name_en                 0\nL3_category_name_en                 0\nL4_category_name_en                 0\nweight                              0\ndtype: int64\n----------------------------------------------------------------------------------------------------\nNaN value in test dataset\nunique_id                            0\ndate                                 0\nwarehouse                            0\ntotal_orders                        18\nsales                               18\nsell_price_main                      0\navailability                         0\ntype_0_discount                      0\ntype_1_discount                      0\ntype_2_discount                      0\ntype_3_discount                      0\ntype_4_discount                      0\ntype_5_discount                      0\ntype_6_discount                      0\ndate_holiday_name                    0\ndate_holiday_flag                    0\ndate_shops_closed_flag               0\ndate_winter_school_holidays_flag     0\ndate_school_holidays_flag            0\ndate_days_to_next_holiday            0\ndate_days_to_shops_closed            0\ndate_day_after_closed_day            0\ndate_second_closed_day               0\ndate_day_after_two_closed_days       0\ndate_year                            0\nproduct_unique_id                    0\nname                                 0\nL1_category_name_en                  0\nL2_category_name_en                  0\nL3_category_name_en                  0\nL4_category_name_en                  0\nweight                               0\ndtype: int64\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Fill missing value","metadata":{}},{"cell_type":"code","source":"# Fill NaN values in df_train with the mean of each numerical column in df_train\nfor col in df_train.select_dtypes(include=np.number).columns:\n    if df_train[col].isnull().any(): # Check if there are any NaN values in the column\n        mean_val_train = df_train[col].mean()\n        df_train[col] = df_train[col].fillna(mean_val_train) # Removed inplace=True and assigned back\n        print(f\"Filled NaN in df_train['{col}'] with mean: {mean_val_train:.2f}\")\n\n# Fix: Assign the result of fillna back to the column\n# Fill NaN values in df_test with the mean of each numerical column in df_test\nfor col in df_test.select_dtypes(include=np.number).columns:\n    if df_test[col].isnull().any(): # Check if there are any NaN values in the column\n        mean_val_test = df_test[col].mean()\n        df_test[col] = df_test[col].fillna(mean_val_test) # Removed inplace=True and assigned back\n        print(f\"Filled NaN in df_test['{col}'] with mean: {mean_val_test:.2f}\")\n\nprint(\"\\n--- After NaN Imputation ---\")\nprint(\"df_train info:\")\ndf_train.info()\nprint(\"\\ndf_test info:\")\ndf_test.info()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:02.787214Z","iopub.execute_input":"2025-08-24T09:30:02.787394Z","iopub.status.idle":"2025-08-24T09:30:03.662313Z","shell.execute_reply.started":"2025-08-24T09:30:02.787379Z","shell.execute_reply":"2025-08-24T09:30:03.661635Z"}},"outputs":[{"name":"stdout","text":"Filled NaN in df_test['total_orders'] with mean: 5604.52\nFilled NaN in df_test['sales'] with mean: 91.75\n\n--- After NaN Imputation ---\ndf_train info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3205935 entries, 0 to 3205934\nData columns (total 32 columns):\n #   Column                            Dtype         \n---  ------                            -----         \n 0   unique_id                         int64         \n 1   date                              datetime64[ns]\n 2   warehouse                         object        \n 3   total_orders                      float64       \n 4   sales                             float64       \n 5   sell_price_main                   float64       \n 6   availability                      float64       \n 7   type_0_discount                   float64       \n 8   type_1_discount                   float64       \n 9   type_2_discount                   float64       \n 10  type_3_discount                   float64       \n 11  type_4_discount                   float64       \n 12  type_5_discount                   float64       \n 13  type_6_discount                   float64       \n 14  date_holiday_name                 object        \n 15  date_holiday_flag                 int64         \n 16  date_shops_closed_flag            int64         \n 17  date_winter_school_holidays_flag  int64         \n 18  date_school_holidays_flag         int64         \n 19  date_days_to_next_holiday         float64       \n 20  date_days_to_shops_closed         float64       \n 21  date_day_after_closed_day         int64         \n 22  date_second_closed_day            int64         \n 23  date_day_after_two_closed_days    int64         \n 24  date_year                         int32         \n 25  product_unique_id                 int64         \n 26  name                              object        \n 27  L1_category_name_en               object        \n 28  L2_category_name_en               object        \n 29  L3_category_name_en               object        \n 30  L4_category_name_en               object        \n 31  weight                            float64       \ndtypes: datetime64[ns](1), float64(14), int32(1), int64(9), object(7)\nmemory usage: 770.5+ MB\n\ndf_test info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 801484 entries, 3205935 to 4007418\nData columns (total 32 columns):\n #   Column                            Non-Null Count   Dtype         \n---  ------                            --------------   -----         \n 0   unique_id                         801484 non-null  int64         \n 1   date                              801484 non-null  datetime64[ns]\n 2   warehouse                         801484 non-null  object        \n 3   total_orders                      801484 non-null  float64       \n 4   sales                             801484 non-null  float64       \n 5   sell_price_main                   801484 non-null  float64       \n 6   availability                      801484 non-null  float64       \n 7   type_0_discount                   801484 non-null  float64       \n 8   type_1_discount                   801484 non-null  float64       \n 9   type_2_discount                   801484 non-null  float64       \n 10  type_3_discount                   801484 non-null  float64       \n 11  type_4_discount                   801484 non-null  float64       \n 12  type_5_discount                   801484 non-null  float64       \n 13  type_6_discount                   801484 non-null  float64       \n 14  date_holiday_name                 801484 non-null  object        \n 15  date_holiday_flag                 801484 non-null  int64         \n 16  date_shops_closed_flag            801484 non-null  int64         \n 17  date_winter_school_holidays_flag  801484 non-null  int64         \n 18  date_school_holidays_flag         801484 non-null  int64         \n 19  date_days_to_next_holiday         801484 non-null  float64       \n 20  date_days_to_shops_closed         801484 non-null  float64       \n 21  date_day_after_closed_day         801484 non-null  int64         \n 22  date_second_closed_day            801484 non-null  int64         \n 23  date_day_after_two_closed_days    801484 non-null  int64         \n 24  date_year                         801484 non-null  int32         \n 25  product_unique_id                 801484 non-null  int64         \n 26  name                              801484 non-null  object        \n 27  L1_category_name_en               801484 non-null  object        \n 28  L2_category_name_en               801484 non-null  object        \n 29  L3_category_name_en               801484 non-null  object        \n 30  L4_category_name_en               801484 non-null  object        \n 31  weight                            801484 non-null  float64       \ndtypes: datetime64[ns](1), float64(14), int32(1), int64(9), object(7)\nmemory usage: 192.6+ MB\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Sorting the Data by Date","metadata":{}},{"cell_type":"markdown","source":"The `sort_dataframe_by_date` function sorts both the training and testing dataframes chronologically based on their date column.\n\n**Theory: Time-Series Data Order**\n\nIn time-series analysis, the order of data is paramount. Sorting ensures that all subsequent operations, such as creating sequences or time-based features, are performed on a correctly ordered dataset. This is a vital step to avoid data leakage and to accurately model temporal dependencies.","metadata":{}},{"cell_type":"code","source":"def sort_dataframe_by_date(df, date_column):\n  \"\"\"\n  Sorts a pandas DataFrame by a specified date column in ascending order.\n\n  Args:\n    df: The pandas DataFrame to be sorted.\n    date_column: The name of the date column in the DataFrame.\n\n  Returns:\n    pandas.DataFrame: The sorted DataFrame.\n  \"\"\"\n\n  # Ensure the date column is in datetime format\n  df[date_column] = pd.to_datetime(df[date_column])\n\n  # Sort the DataFrame by the date column in ascending order\n  df = df.sort_values(by=date_column) \n\n  return df\n\n# Example usage:\n# Assuming 'df' is your DataFrame and the date column is named 'date'\ndf_train = sort_dataframe_by_date(df_train, 'date')\ndf_test = sort_dataframe_by_date(df_test, 'date')","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:03.662980Z","iopub.execute_input":"2025-08-24T09:30:03.663186Z","iopub.status.idle":"2025-08-24T09:30:06.670696Z","shell.execute_reply.started":"2025-08-24T09:30:03.663150Z","shell.execute_reply":"2025-08-24T09:30:06.669684Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(\"df_train shape before dropping columns:\", df_train.shape)\nprint(\"df_test shape before dropping columns:\", df_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:06.671684Z","iopub.execute_input":"2025-08-24T09:30:06.671963Z","iopub.status.idle":"2025-08-24T09:30:06.677820Z","shell.execute_reply.started":"2025-08-24T09:30:06.671941Z","shell.execute_reply":"2025-08-24T09:30:06.676981Z"}},"outputs":[{"name":"stdout","text":"df_train shape before dropping columns: (3205935, 32)\ndf_test shape before dropping columns: (801484, 32)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"markdown","source":"The code explicitly drops a list of \"noise columns\" from both the training and testing dataframes. These columns are likely identified as having low predictive power from a previous correlation analysis (e.g., the mutual information step in the previous code block), or are considered to be redundant.\n\n**Theory: The Curse of Dimensionality**\n\nDropping features is a form of dimensionality reduction. Including too many features, especially those that are irrelevant or redundant, can lead to the \"curse of dimensionality.\" This can make models slower to train, more prone to overfitting, and harder to interpret. By removing low-impact features, the code streamlines the dataset, which can improve model performance and efficiency.","metadata":{}},{"cell_type":"code","source":"columns_to_drop = [\n    'type_2_discount',\n    'date_holiday_flag',\n    'date_school_holidays_flag',\n    'date_shops_closed_flag',\n    'date_second_closed_day',\n    'date_winter_school_holidays_flag',\n    'date_day_after_closed_day',\n    'date_day_after_two_closed_days',\n    'type_5_discount',\n    'type_3_discount',\n    'type_1_discount',\n    'unique_id',\n    \"availability\" \n]\n\n# Drop columns from df_train\ndf_train = df_train.drop(columns=columns_to_drop, axis=1, errors='ignore')\n\n# Drop columns from df_test\ndf_test = df_test.drop(columns=columns_to_drop, axis=1, errors='ignore')\n\nprint(\"df_train shape after dropping columns:\", df_train.shape)\nprint(\"df_test shape after dropping columns:\", df_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:06.678464Z","iopub.execute_input":"2025-08-24T09:30:06.678749Z","iopub.status.idle":"2025-08-24T09:30:07.097780Z","shell.execute_reply.started":"2025-08-24T09:30:06.678723Z","shell.execute_reply":"2025-08-24T09:30:07.097119Z"}},"outputs":[{"name":"stdout","text":"df_train shape after dropping columns: (3205935, 19)\ndf_test shape after dropping columns: (801484, 19)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Data Preprocessing for Modeling","metadata":{}},{"cell_type":"markdown","source":"The `preprocess_data` function consists of several critical steps to transform the data into a format suitable for a machine learning model:\n\n- **Separation of Features, Target, and Weights:** It separates the data into input features (`x_train, x_test`), the target variable (`y_train, y_test`), and a weight column for each set. The weight column is likely used in the model's loss function to give more importance to certain data points.\n- **Datetime Feature Extraction:** It extracts simple numerical features (month and day) from the date column and then drops the original column. This is an alternative to the cyclical encoding from the previous block, but still makes temporal information usable by the model.\n- **Categorical Encoding:** It uses `OrdinalEncoder` to convert categorical text columns (like warehouse or name) into numerical representations. This is necessary because models cannot directly process string data.\n- **Numerical Scaling:** It uses `StandardScaler` to normalize the numerical features. Standardization transforms the data to have a mean of 0 and a standard deviation of 1. This is a crucial step for many algorithms (especially those based on gradient descent, like neural networks) because it ensures that no single feature dominates the learning process due to its magnitude. The scaler fitted on the training data is then applied to the test data to prevent data leakage from the test set.","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df_train, df_test, target_col):\n    \"\"\"\n    Preprocesses df_train (fit and transform) and df_test (transform) for training and testing.\n\n    Args:\n        df_train: pandas DataFrame for training.\n        df_test: pandas DataFrame for testing.\n        target_col: Name of the target column (e.g., 'sales').\n\n    Returns:\n        x_train: Training features (float32 NumPy array).\n        y_train: Training targets (float32 NumPy array).\n        weight_train: Training weights (float32 NumPy array).\n        x_test: Test features (float32 NumPy array).\n        y_test: Test targets (float32 NumPy array).\n        weight_test: Test weights (float32 NumPy array).\n    \"\"\"\n    # Extract target and weights\n    y_train = df_train[target_col].values.astype(np.float32)\n    y_test = df_test[target_col].values.astype(np.float32)\n    weight_train = df_train['weight'].values.astype(np.float32)\n    weight_test = df_test['weight'].values.astype(np.float32)\n\n    # Extract features\n    x_train = df_train.drop([target_col, 'weight'], axis=1).copy()\n    x_test = df_test.drop([target_col, 'weight'], axis=1).copy()\n\n    # Handle datetime columns\n    datetime_cols = x_train.select_dtypes(include=['datetime']).columns\n    if len(datetime_cols) > 0:\n        for col in datetime_cols:\n            x_train[col + '_month'] = x_train[col].dt.month\n            x_train[col + '_day'] = x_train[col].dt.day\n            x_test[col + '_month'] = x_test[col].dt.month\n            x_test[col + '_day'] = x_test[col].dt.day\n        x_train = x_train.drop(datetime_cols, axis=1)\n        x_test = x_test.drop(datetime_cols, axis=1)\n\n    # Define categorical and numerical columns\n    categorical_cols = x_train.select_dtypes(include=['object', 'category']).columns\n    numeric_cols = x_train.select_dtypes(include=['number']).columns\n\n    # Encode categorical features\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n    if len(categorical_cols) > 0:\n        x_train[categorical_cols] = encoder.fit_transform(x_train[categorical_cols])\n        x_test[categorical_cols] = encoder.transform(x_test[categorical_cols])\n\n    # Scale numerical features\n    scaler = StandardScaler()\n    if len(numeric_cols) > 0:\n        x_train[numeric_cols] = scaler.fit_transform(x_train[numeric_cols])\n        x_test[numeric_cols] = scaler.transform(x_test[numeric_cols])\n\n    # Save encoder and scaler\n    joblib.dump(encoder, 'encoder.pkl')\n    joblib.dump(scaler, 'scaler.pkl')\n\n    # Convert to NumPy arrays\n    x_train = x_train.values.astype(np.float32)\n    x_test = x_test.values.astype(np.float32)\n\n    return x_train, y_train, weight_train, x_test, y_test, weight_test\n\n# Preprocess data\nx_train, y_train, weight_train, x_test, y_test, weight_test = preprocess_data(df_train, df_test, 'sales')\n\nprint(\"x_train shape:\", x_train.shape)\nprint(\"x_test shape:\", x_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:07.098519Z","iopub.execute_input":"2025-08-24T09:30:07.098760Z","iopub.status.idle":"2025-08-24T09:30:15.561307Z","shell.execute_reply.started":"2025-08-24T09:30:07.098733Z","shell.execute_reply":"2025-08-24T09:30:15.560480Z"}},"outputs":[{"name":"stdout","text":"x_train shape: (3205935, 18)\nx_test shape: (801484, 18)\ny_train shape: (3205935,)\ny_test shape: (801484,)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Time-Series Dataset Creation","metadata":{}},{"cell_type":"markdown","source":"The final part of the code prepares the data for a time-series model (likely a recurrent neural network, such as an LSTM).\n\n- **Validation Split:** It splits the training data (`x_train, y_train`) further into a training set and a validation set. This is a standard practice for hyperparameter tuning and model evaluation during training. This split is also done chronologically to maintain time-series integrity.\n- **Target Scaling:** The sales target variable is also scaled using StandardScaler. This is common in regression problems to stabilize training and improve convergence.\n- **Sequential Data Generation:** The `create_dataset` function uses `tf.keras.utils.timeseries_dataset_from_array` to convert the flat data arrays into sequences. The `sequence_length` parameter specifies how many consecutive time steps a model will see as input to predict the next time step. This is the core of time-series sequence modeling. The function also aligns the sample weights to the target values within each sequence, ensuring the model's loss is correctly weighted.","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nSEQUENCE_LENGTH = 100\nBATCH_SIZE = 256\n\n# Preprocessing: Split data and scale targets\nval_split = 0.2\nval_size = int(len(x_train) * val_split)\nx_val, y_val, weights_val = x_train[-val_size:], y_train[-val_size:], weight_train[-val_size:]\nx_train, y_train, weights_train = x_train[:-val_size], y_train[:-val_size], weight_train[:-val_size]\n\n# Cast to float32\nx_train = x_train.astype(np.float32)\nx_val = x_val.astype(np.float32)\nx_test = x_test.astype(np.float32)\nweights_train = weights_train.astype(np.float32)\nweights_val = weights_val.astype(np.float32)\nweight_test = weight_test.astype(np.float32)\n\n# Scale targets\nscaler_y = StandardScaler()\ny_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten().astype(np.float32)\ny_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten().astype(np.float32)\ny_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten().astype(np.float32)\njoblib.dump(scaler_y, 'scaler_y.pkl')\n\n# Function to create time-series dataset with sample weights\ndef create_dataset(features, targets, weights, sequence_length, batch_size):\n    \"\"\"\n    Creates a time-series dataset with features, targets, and sample weights.\n\n    Args:\n        features (np.ndarray): Input features (float32).\n        targets (np.ndarray): Target values (float32).\n        weights (np.ndarray): Sample weights (float32).\n        sequence_length (int): Length of each sequence.\n        batch_size (int): Batch size.\n\n    Returns:\n        tf.data.Dataset: Dataset yielding (inputs, targets, sample_weights).\n        np.ndarray: Aligned weights.\n    \"\"\"\n    dataset = tf.keras.utils.timeseries_dataset_from_array(\n        data=features,\n        targets=targets,\n        sequence_length=sequence_length,\n        batch_size=batch_size\n    )\n    aligned_weights = weights[sequence_length - 1:] if len(weights) > sequence_length else weights\n    dataset = dataset.map(lambda x, y: (x, y, tf.gather(aligned_weights, tf.range(tf.shape(y)[0]), axis=0)))\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset, aligned_weights\n\n# Create datasets\ndataset_train, train_weights = create_dataset(x_train, y_train, weights_train, SEQUENCE_LENGTH, BATCH_SIZE)\ndataset_val, val_weights = create_dataset(x_val, y_val, weights_val, SEQUENCE_LENGTH, BATCH_SIZE)\ndataset_test, test_weights = create_dataset(x_test, y_test, weight_test, SEQUENCE_LENGTH, BATCH_SIZE)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:15.562180Z","iopub.execute_input":"2025-08-24T09:30:15.562541Z","iopub.status.idle":"2025-08-24T09:30:16.516057Z","shell.execute_reply.started":"2025-08-24T09:30:15.562514Z","shell.execute_reply":"2025-08-24T09:30:16.515283Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## Model Architecture, model building, and model training - wavenet with transformer model","metadata":{}},{"cell_type":"markdown","source":"The model is a hybrid architecture that combines the strengths of WaveNet and Transformer networks. The code also defines custom loss functions and metrics tailored to the specific problem, and includes advanced training techniques.\n\n**Model Architecture and Theory**\n\nThe model's design is a layered, multi-component structure.\n\n- **WaveNet Blocks:** The initial part of the model uses WaveNet blocks, which are based on dilated causal convolutions.\n  - **Causal Convolutions:** In time-series forecasting, a model should only use past data to predict the future. Causal convolutions ensure this by restricting the receptive field to only look backward in time. The output at a given time step is a function of the input at that time step and all previous time steps, but not future ones.\n  - **Dilated Convolutions:** Dilated convolutions skip input values with a certain step or \"dilation rate.\" This allows the network's receptive field to grow exponentially with each layer, enabling it to efficiently capture long-range dependencies in the sequence without a significant increase in computational cost.\n  - **Gated Activation:** The tanh and sigmoid activations work together as a gating mechanism. The sigmoid acts as a gate, determining which information from the tanh activation should be passed through. This helps the network selectively propagate relevant information through the layers.\n\n- **Transformer Encoder Layers:** Following the WaveNet blocks, the model uses two Transformer encoder layers.\n  - **Multi-Head Self-Attention:** This is the core of the Transformer. It allows the model to weigh the importance of all other parts of the input sequence when processing a specific part. For time-series, this means the model can learn complex, non-linear dependencies across the entire history, not just local patterns. The \"multi-head\" aspect means this is done in parallel, allowing the model to focus on different aspects of the sequence simultaneously.\n\n- **Final Layers:** The output of the hybrid network is passed through a Multi-Layer Perceptron (MLP) with dense layers and dropout for regularization.\n  - **Dropout:** A regularization technique that randomly sets a fraction of neurons to zero during training. This prevents overfitting by forcing the network to learn more robust features that are not dependent on specific neurons.\n  - **Custom ClipLayer:** The final output is passed through a ClipLayer that constrains the prediction to a specific range (-10.0 to 10.0). This is a practical step to ensure the model's predictions remain within a plausible range, which is especially useful for normalized target values.\n\n**Custom Loss and Training**\n\nThe model is compiled with a custom loss function and a dynamic learning rate schedule, highlighting a tailored approach to the problem.\n\n**Weighted Mean Absolute Error (WMAE) Loss:** The `custom_wmae_loss` function computes the mean absolute error, but with an added `sample_weight` parameter.\n  - **Weighted Loss:** A weighted loss function is used when some data points are considered more important than others. By assigning higher weights to certain samples, the model is penalized more heavily for errors on those samples, forcing it to prioritize them during training. This is a crucial technique for imbalanced datasets or when certain predictions have a higher business impact. The custom `WeightedMAEMetric` tracks this same weighted error during evaluation.\n\n**Learning Rate Schedule:** The Adam optimizer is used with a CosineDecay learning rate schedule and a warmup phase.\n  - **Dynamic Learning Rate:** Instead of using a fixed learning rate, this schedule starts with a low learning rate (warmup), gradually increases it, and then slowly decreases it following a cosine function. This helps the model converge more efficiently and find better optima, as a high learning rate early in training can cause instability, while a lower rate at the end allows for fine-tuning.\n\n**Training Process**\n\nThe model is trained using the `.fit()` method.\n\n- `model.fit()`: This function takes the training and validation datasets as input.\n- `callbacks:` An EarlyStopping callback is used. This is a form of regularization that monitors the validation loss and stops training if it stops improving for a specified number of epochs (patience). It also restores the best model weights, ensuring the final model is the one with the best performance on the validation set, preventing further overfitting.","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nSEQUENCE_LENGTH = 100\nBATCH_SIZE = 256\nEPOCHS = 1\nBASE_LR = 3e-4\nDROPOUT_RATE = 0.3\nFILTERS = 256\nKERNEL_SIZE = 3\nDILATION_RATES = [1, 2, 4, 8, 16, 32]\n\n# Custom WMAE Loss Function\ndef custom_wmae_loss(y_true, y_pred, sample_weight=None):\n    \"\"\"\n    Weighted Mean Absolute Error loss function following evaluation criteria.\n    This version strictly interprets zero sample weights as zero contribution\n    and handles cases where the sum of weights is zero to prevent NaN.\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    if sample_weight is None:\n        sample_weight = tf.ones_like(y_true, dtype=tf.float32)\n    else:\n        sample_weight = tf.cast(sample_weight, tf.float32)\n    weighted_error = tf.abs(y_true - y_pred) * sample_weight\n    sum_of_weights = tf.reduce_sum(sample_weight)\n    return tf.cond(\n        tf.greater(sum_of_weights, 0),\n        lambda: tf.reduce_sum(weighted_error) / sum_of_weights,\n        lambda: tf.constant(0.0, dtype=tf.float32)\n    )\n\n# Custom WMAE Metric\nclass WeightedMAEMetric(Metric):\n    \"\"\"\n    Custom metric to compute Weighted Mean Absolute Error following evaluation criteria.\n    This version strictly interprets zero sample weights as zero contribution\n    and handles cases where the sum of weights is zero to prevent NaN.\n    \"\"\"\n    def __init__(self, name='wmae', **kwargs):\n        super(WeightedMAEMetric, self).__init__(name=name, **kwargs)\n        self.total_weighted_error = self.add_weight(name='total_weighted_error', initializer='zeros')\n        self.total_weights = self.add_weight(name='total_weights', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        if sample_weight is None:\n            sample_weight = tf.ones_like(y_true, dtype=tf.float32)\n        else:\n            sample_weight = tf.cast(sample_weight, tf.float32)\n        weighted_error = tf.abs(y_true - y_pred) * sample_weight\n        self.total_weighted_error.assign_add(tf.reduce_sum(weighted_error))\n        self.total_weights.assign_add(tf.reduce_sum(sample_weight))\n\n    def result(self):\n        return tf.cond(\n            tf.greater(self.total_weights, 0),\n            lambda: self.total_weighted_error / self.total_weights,\n            lambda: tf.constant(0.0, dtype=tf.float32)\n        )\n\n    def reset_state(self):\n        self.total_weighted_error.assign(0.0)\n        self.total_weights.assign(0.0)\n\n# Learning rate scheduler with warmup\n# Note: This assumes y_train is defined elsewhere; adjust accordingly if needed\nnum_train_sequences = max(0, len(y_train) - SEQUENCE_LENGTH + 1) if 'y_train' in globals() else 10000  # Placeholder value\ntotal_steps = EPOCHS * (num_train_sequences // BATCH_SIZE) if BATCH_SIZE > 0 else 0\nwarmup_steps = min(1000, total_steps // 10)\nlr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=BASE_LR,\n    decay_steps=max(1, total_steps - warmup_steps),\n    alpha=0.1\n)\noptimizer = Adam(learning_rate=lr_schedule)\n\n# WaveNet Block\ndef wavenet_block(x, dilation_rate, filters, kernel_size, dropout_rate):\n    \"\"\"\n    WaveNet block with dilated convolutions, gating, and residual/skip connections.\n    \"\"\"\n    conv_filter = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='tanh')(x)\n    conv_gate = Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='sigmoid')(x)\n    gated_output = Multiply()([conv_filter, conv_gate])\n    gated_output = BatchNormalization()(gated_output)\n    residual = Conv1D(filters, 1, padding='same')(gated_output)\n    skip = Conv1D(filters, 1, padding='same')(gated_output)\n    if x.shape[-1] != filters:\n        x = Conv1D(filters, 1, padding='same')(x)\n    return Add()([x, residual]), skip\n\n# Transformer Encoder Layer\ndef transformer_encoder(x, num_heads=4, ff_dim=128, dropout_rate=0.3):\n    \"\"\"\n    Transformer encoder layer with multi-head attention and feed-forward network.\n    \"\"\"\n    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n    attn_output = Dropout(dropout_rate)(attn_output)\n    x = Add()([x, attn_output])\n    x = LayerNormalization(epsilon=1e-6)(x)\n    ff_output = Dense(ff_dim, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\n    ff_output = Dropout(dropout_rate)(ff_output)\n    ff_output = Dense(x.shape[-1])(ff_output)\n    x = Add()([x, ff_output])\n    x = LayerNormalization(epsilon=1e-6)(x)\n    return x\n\n# Custom Layer for Clipping\nclass ClipLayer(tf.keras.layers.Layer):\n    def __init__(self, min_value=-10.0, max_value=10.0, **kwargs):\n        super(ClipLayer, self).__init__(**kwargs)\n        self.min_value = min_value\n        self.max_value = max_value\n\n    def call(self, inputs):\n        return tf.clip_by_value(inputs, self.min_value, self.max_value)\n\n# Build WaveNet + Transformer Model\nnum_features = x_train.shape[1] if 'x_train' in globals() else 19  # Placeholder value\ninputs = Input(shape=(SEQUENCE_LENGTH, num_features), name=\"input\")\nx = inputs\nskip_connections = []\n\n# WaveNet blocks\nfor dilation_rate in DILATION_RATES:\n    x, skip = wavenet_block(x, dilation_rate, FILTERS, KERNEL_SIZE, DROPOUT_RATE)\n    skip_connections.append(skip)\n\nx = Add()(skip_connections)\nx = Activation('relu')(x)\nx = BatchNormalization()(x)\n\n# Transformer encoder layers\nx = transformer_encoder(x, num_heads=4, ff_dim=128, dropout_rate=0.3)\nx = transformer_encoder(x, num_heads=4, ff_dim=128, dropout_rate=0.3)\n\n# Final layers\nx = Conv1D(FILTERS, 1, activation='relu')(x)\nx = GlobalAveragePooling1D()(x)\nx = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\nx = Dropout(DROPOUT_RATE)(x)\nx = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\nx = Dropout(DROPOUT_RATE)(x)\nx = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005))(x)\nx = Dropout(DROPOUT_RATE)(x)\n\n# Apply clipping using the custom layer\noutputs = ClipLayer(min_value=-10.0, max_value=10.0)(Dense(1, activation='linear', name=\"output\", dtype='float32')(x))\n\n# Build and compile model\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.compile(\n    optimizer=optimizer,\n    loss=custom_wmae_loss,\n    metrics=['mae'],\n    weighted_metrics=[WeightedMAEMetric()]\n)\n\nmodel.summary()\n\n# Train the model with sample weights\nprint(\"\\n--- Training Model ---\")\nhistory = model.fit(\n    dataset_train,  # Ensure dataset_train is defined\n    validation_data=dataset_val,  # Ensure dataset_val is defined\n    epochs=EPOCHS,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n    ],\n    shuffle=False\n)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T09:30:16.516920Z","iopub.execute_input":"2025-08-24T09:30:16.517139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m18\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m14,080\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m14,080\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m4,864\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│                     │                   │            │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                     │                   │            │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ multiply_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ conv1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_17 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_18 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_4          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ conv1d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ multiply_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_19 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n│                     │                   │            │ conv1d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_21 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_22 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_5          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ conv1d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ multiply_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_16 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_20 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_24 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│                     │                   │            │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│                     │                   │            │ conv1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ conv1d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ conv1d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ conv1d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m526,080\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,896\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m526,080\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m32,896\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_10 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_25 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m65,792\u001b[0m │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ clip_layer          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ output[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mClipLayer\u001b[0m)         │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,080</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,080</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│                     │                   │            │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                     │                   │            │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ multiply_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ conv1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_4          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ conv1d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ multiply_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n│                     │                   │            │ conv1d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multiply_5          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ conv1d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ multiply_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│                     │                   │            │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│                     │                   │            │ conv1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ conv1d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ conv1d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ conv1d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">526,080</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">526,080</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ clip_layer          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ClipLayer</span>)         │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,091,393\u001b[0m (15.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,091,393</span> (15.61 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,087,809\u001b[0m (15.59 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,087,809</span> (15.59 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,584\u001b[0m (14.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,584</span> (14.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n--- Training Model ---\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1756027840.897335     111 service.cc:148] XLA service 0x7aa6080123b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1756027840.899807     111 service.cc:156]   StreamExecutor device (0): Host, Default Version\nI0000 00:00:1756027847.975538     111 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  609/10019\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:08:57\u001b[0m 7s/step - loss: 3.3711 - mae: 0.2335 - wmae: 60.5383","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"**Remark that** : eaxmple of the model above is train only one epoch due to limited time of save version on Kaggle, but actually you have to set number of epoch to 200 epochs.","metadata":{}},{"cell_type":"markdown","source":"## Save the model","metadata":{}},{"cell_type":"markdown","source":"This focuses on the crucial step of **model persistence**: saving and loading a trained deep learning model. This is a fundamental practice in machine learning for deployment, version control, and reproducibility.\n\n**Saving the Model:** The code first defines a path (`keras_model_path`) and then uses the `model.save()` method to save the trained model to this path.\n\n**Theory : Model Persistence**\n\nSaving a model is a critical step after training is complete. It serializes the entire model architecture, including its layers, weights, training configuration (optimizer, loss function), and the state of the optimizer. This allows you to save the result of potentially long and computationally expensive training processes. The .keras format is a modern, single-file container that bundles all these components, making it easy to share and load.\n\n**Loading the Model:** The code then demonstrates how to load the saved model using `tf.keras.models.load_model()`.\n\n**Theory : Custom Objects**\n\nWhen loading a model that was compiled with custom components (like the `custom_wmae_loss` and `WeightedMAEMetric` defined in the previous steps), you must explicitly provide these custom objects to the `load_model` function. Keras needs to know how to rebuild these non-standard parts of the model. Without this step, the loading process would fail with an error because it wouldn't recognize the custom functions and classes.\n\n**Verification:** The code prints a success message and then displays a summary of the loaded model using `loaded_model.summary()`. This step is a quick and effective way to verify that the model was loaded correctly, and its architecture, layer names, and output shapes match the original.","metadata":{}},{"cell_type":"code","source":"# --- Save the model in Keras format (.keras) ---\n# Define the path for the .keras file\nkeras_model_path = \"wavenet_transformer_model.keras\"\n\n# Save the model\nmodel.save(keras_model_path, overwrite=True)\nprint(f\"\\nModel saved successfully to '{keras_model_path}' in Keras format.\")\n\n# --- Load the model from Keras format (.keras) ---\nprint(f\"\\n--- Loading Model from '{keras_model_path}' ---\")\nloaded_model = tf.keras.models.load_model(\n    keras_model_path,\n    custom_objects={\n        'custom_wmae_loss': custom_wmae_loss,\n        'WeightedMAEMetric': WeightedMAEMetric\n    }\n)\nprint(\"Model loaded successfully!\")\nloaded_model.summary()","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Model evaluation on regression metrics","metadata":{}},{"cell_type":"markdown","source":"This final step performs the essential task of **evaluating a trained deep learning model** on an unseen test dataset. The evaluation is done in two stages: first on the scaled data and then, more importantly, on the unscaled (original) data to provide interpretable metrics.\n\n**1. Evaluation on Scaled Data**\n\nThe code first evaluates the model using the `model.evaluate()` function. This leverages the loss and metrics that were defined when the model was compiled (`custom_wmae_loss` and `WeightedMAEMetric`). The evaluation is performed on the `dataset_test`, which contains sequences of preprocessed and scaled data.\n\n**Theory: Model Metrics** \n\nDuring training, a model optimizes a loss function, but performance is typically measured using a separate set of metrics that are more interpretable. For this regression task, the key metric is **Weighted Mean Absolute Error (WMAE)**, which indicates the average magnitude of prediction errors, giving more importance to specific data points. Evaluating on scaled data provides a quick sanity check to ensure the model's performance on the test set is consistent with its validation performance during training.\n\n**2. Unscaled Prediction and Metric Calculation**\n\nThis is the most critical part of the evaluation. While models are trained on scaled data for stability, the final performance must be measured on the original scale to be meaningful and comparable to real-world values.\n\n**Step-by-Step Process:**\n\n- **Iterate and Predict:** The code loops through the `dataset_test` to get batches of features, true values, and sample weights. For each batch, it uses `model.predict()` to generate predictions.\n- **Handling NaNs:** It includes robust checks to handle potential NaN values in both the true values and the predictions. This is a practical step to prevent errors and ensure that the metric calculations are not corrupted.\n- **Inverse Transformation:** The core concept here is to unscale the data. The `scaler_y.inverse_transform()` method is used to convert the scaled predictions and true values back to their original numerical range (e.g., from a value like 0.5 to a value like 500). This step is essential because a scaled error (e.g., 0.1) has no real-world meaning until it is converted back to the original unit (e.g., 100 sales units).\n- **Metric Calculation:** Finally, the code calculates standard regression metrics—Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)—on the unscaled data using scikit-learn functions. It also recalculates the WMAE on the unscaled data.\n\n**Theory: The Importance of Unscaled Metrics**\n\n- **Interpretability:** Metrics on unscaled data provide a direct, tangible understanding of the model's performance. For instance, an MAE of 10 means the model's predictions are, on average, off by 10 units of sales, which is a much more useful piece of information than a scaled MAE of 0.05.\n- **Comparison:** Unscaled metrics allow for direct comparison between different models or with a simple baseline model, regardless of how each model's internal data was scaled.\n- **Regression Metrics:**\n  - **MAE:** The average absolute difference between predicted and actual values. It is easy to interpret and not sensitive to outliers.\n  - **MSE:** The average of the squared differences. It penalizes large errors more heavily than MAE.\n  - **RMSE:** The square root of MSE, which puts the metric back in the same units as the target variable, making it more interpretable than MSE.","metadata":{}},{"cell_type":"code","source":"# --- Evaluate the model on the test dataset ---\nprint(\"\\n--- Evaluating Model on Test Set (Scaled) ---\")\n# model.evaluate directly uses the loss and metrics defined in model.compile\n# The output will include the custom WMAE metric.\nevaluation_results = model.evaluate(dataset_test)\nprint(f\"Test Set Evaluation Results (Scaled):\")\nfor name, value in zip(model.metrics_names, evaluation_results):\n    print(f\"{name}: {value:.4f}\")\n\n# --- Make predictions and calculate unscaled metrics ---\nprint(\"\\n--- Making Predictions and Calculating Unscaled Metrics ---\")\n\n# Initialize lists for true values and predictions (scaled)\ny_true_scaled_list, y_pred_scaled_list, sample_weights_list = [], [], []\n\n# Iterate through the test dataset to get predictions and corresponding true values and weights\nfor x_batch, y_batch_scaled, weights_batch in dataset_test:\n    # Handle NaNs in y_batch_scaled (true values) before extending\n    y_batch_scaled_processed = y_batch_scaled.numpy()\n    if np.any(np.isnan(y_batch_scaled_processed)):\n        print(f\"Warning: NaN values detected in true scaled targets. Replacing with 0 for metric calculation.\")\n        y_batch_scaled_processed = np.nan_to_num(y_batch_scaled_processed, nan=0.0)\n\n    y_pred_batch_scaled = model.predict(x_batch, verbose=0) # Disable verbose logging\n\n    # --- NaN Handling for predictions: Replace NaNs with 0 before further processing ---\n    if np.any(np.isnan(y_pred_batch_scaled)):\n        print(f\"Warning: NaN values detected in model predictions. Replacing with 0 for metric calculation.\")\n        y_pred_batch_scaled = np.nan_to_num(y_pred_batch_scaled, nan=0.0) # Replace NaNs with 0\n\n    y_true_scaled_list.extend(y_batch_scaled_processed) # Use processed true values\n    y_pred_scaled_list.extend(y_pred_batch_scaled.flatten())\n    sample_weights_list.extend(weights_batch.numpy())\n\n# Convert lists to NumPy arrays\ny_true_scaled = np.array(y_true_scaled_list)\ny_pred_scaled = np.array(y_pred_scaled_list)\nsample_weights_for_metrics = np.array(sample_weights_list)\n\n# Inverse transform predictions and true values to original scale\n# Apply nan_to_num after inverse_transform as well, in case scaling produces NaNs\ny_true_unscaled = np.nan_to_num(scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten(), nan=0.0)\ny_pred_unscaled = np.nan_to_num(scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten(), nan=0.0)\n\n# Ensure sample_weights_for_metrics matches the length of predictions after inverse transform\n# This is crucial because timeseries_dataset_from_array might drop initial samples\n# and the number of predictions will match the number of targets in the dataset.\n# The `create_dataset` function already aligns `aligned_weights` to the number of sequences\n# that will be generated, so `sample_weights_for_metrics` should already be correctly aligned.\n\n# Compute evaluation metrics on unscaled data\nmae_unscaled = mean_absolute_error(y_true_unscaled, y_pred_unscaled)\nmse_unscaled = mean_squared_error(y_true_unscaled, y_pred_unscaled)\nrmse_unscaled = np.sqrt(mse_unscaled)\n\n# Calculate WMAE on unscaled data for verification (should be similar to scaled WMAE if scaler is linear)\n# Only consider samples with non-zero weights for WMAE calculation to avoid division by zero\nnon_zero_weight_indices = sample_weights_for_metrics > 0\nif np.sum(non_zero_weight_indices) > 0:\n    wmae_unscaled = np.sum(np.abs(y_true_unscaled[non_zero_weight_indices] - y_pred_unscaled[non_zero_weight_indices]) * sample_weights_for_metrics[non_zero_weight_indices]) / np.sum(sample_weights_for_metrics[non_zero_weight_indices])\nelse:\n    wmae_unscaled = 0.0 # Handle case where all weights are zero\n\n# Print results\nprint(f\"Evaluation Metrics on Test Set (Unscaled):\")\nprint(f\"MAE  : {mae_unscaled:.4f}\")\nprint(f\"MSE  : {mse_unscaled:.4f}\")\nprint(f\"RMSE : {rmse_unscaled:.4f}\")\nprint(f\"WMAE : {wmae_unscaled:.4f}\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}