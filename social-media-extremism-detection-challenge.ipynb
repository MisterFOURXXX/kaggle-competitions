{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":124554,"databundleVersionId":14793388,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition Overview","metadata":{}},{"cell_type":"markdown","source":"The Social Media Extremism Detection Challenge is a Kaggle binary text classification competition focused on distinguishing potentially extremist social media posts from regular content. Participants receive an anonymized dataset of short English-language messages labeled as EXTREMIST (promotes/supports extremist ideology, organizations, or violence) or NON_EXTREMIST. The challenge is part of a Community Impact Initiative emphasizing AI for social good, online safety, and responsible content moderation research.\n\nThe task is educational and research-oriented—exploring fairness, robustness, and interpretability in detecting harmful content. Submissions are evaluated using classification accuracy on a hidden test set.\n\n- Timeline: Started ~November 2025, closed January 7, 2026.\n- Prize Pool: 200 dollar total (100 dollar – 1st, 60 dollar – 2nd, 40 dollar – 3rd) and digital certificates.\n- Content Warning: Dataset contains disturbing or hateful references provided solely for research purposes.","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Online platforms face increasing challenges in moderating extremist content while preserving free expression. Extremist posts are often subtle, context-dependent, and rapidly evolving, making automated detection difficult yet critical for reducing online harm.\n\nThis competition provides a curated, anonymized dataset of 2,776 hand-labeled messages. The data distribution is approximately 48% EXTREMIST and 52% NON_EXTREMIST. While relatively balanced, the nuance of the language requires models that can capture deep semantic context.\n\nThe provided notebook implements a state-of-the-art pipeline: aggressive data augmentation (synonym replacement and word swapping), Optuna hyperparameter tuning across 5 strong transformer models (RoBERTa, DeBERTa-v3, ELECTRA, DistilBERT, and a specialized Toxic-Comment model), and a final soft-voting ensemble. This approach achieves a high held-out accuracy of ~81.8%, ensuring a robust and generalized submission.","metadata":{}},{"cell_type":"markdown","source":"# Objective","metadata":{}},{"cell_type":"markdown","source":"The main goal is to develop a high-accuracy binary classifier that distinguishes extremist from non-extremist content. Specifically, the notebook aims to:\n\n- Enhance Data Quality: Create a perfectly balanced training set through strong augmentation to ensure the model doesn't overfit to majority-class linguistic patterns.\n- Optimize Architecture: Fine-tune 5 diverse transformer models, leveraging Optuna to find the ideal learning rates, dropout, and batch sizes.\n- Implement Advanced Training: Apply Mixed Precision (AMP), Gradient Accumulation, and Cosine LR scheduling to maximize hardware efficiency.\n- Maximize Generalization: Build a 5-model soft-voting ensemble to reduce variance and improve the final Private Leaderboard score.","metadata":{}},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"**Data Characteristics & Constraints**\n\n- Language: English.\n- Format: Short-form social media text (resembles microblogging style).\n- Privacy: Data is anonymized; however, it retains references to public institutions (e.g., NHS) and general geographic/social groups.\n- Balanced Distribution: The training set is relatively balanced (~48% Extremist vs. ~52% Non-Extremist), reducing the need for extreme class-weighting but benefiting from the augmentation techniques described in the pipeline.\n- Note on Content: As this dataset deals with extremism detection, it contains sensitive and potentially disturbing language. It is intended strictly for research and the development of safety-focused AI.\n\n**File Descriptions**\n\nThe dataset is split into three primary files, providing a total of 2,776 labeled examples in the training set and a corresponding unlabeled test set.\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">File Name</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">train.csv</td>\n    <td class=\"tg-0lax\">The training set containing labeled messages used to train and validate machine learning models.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">test.csv</td>\n    <td class=\"tg-0lax\">The test set for which participants must predict the extremism labels.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">sample_submission.csv</td>\n    <td class=\"tg-0lax\">A template demonstrating the required format for submission (ID and Prediction).</td>\n  </tr>\n</tbody>\n</table>\n\n**Field Definitions**\n\nThe following table outlines the schema for both the training and testing data.\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Column Name</th>\n    <th class=\"tg-7zrl\">Data Type</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">ID</td>\n    <td class=\"tg-7zrl\">Integer</td>\n    <td class=\"tg-0lax\">A unique identifier for each social media post (Range: 1–2800).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Original_Message</td>\n    <td class=\"tg-7zrl\">String (Text)</td>\n    <td class=\"tg-0lax\">The raw content of the social media post. Includes slang, punctuation, and casing (UTF-8 encoded).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Extremism_Label</td>\n    <td class=\"tg-7zrl\">Categorical</td>\n    <td class=\"tg-0lax\">The target variable indicating the nature of the content (Present in train.csv only).</td>\n  </tr>\n</tbody>\n</table>\n\n**Label Schema**\n\nFor the purpose of binary classification, the Extremism_Label is defined by the following criteria:\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-2b7s{text-align:right;vertical-align:bottom}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Label Value</th>\n    <th class=\"tg-7zrl\">Numeric Map (Suggested)</th>\n    <th class=\"tg-7zrl\">Definition</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">EXTREMIST</td>\n    <td class=\"tg-2b7s\">1</td>\n    <td class=\"tg-0lax\">Posts that clearly promote, endorse, or advocate for extremist ideologies, specific organizations, or violence.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">NON_EXTREMIST</td>\n    <td class=\"tg-2b7s\">0</td>\n    <td class=\"tg-0lax\">Posts that are neutral, critical without extremism, or unrelated to extremist rhetoric.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"# Pipeline Overview","metadata":{}},{"cell_type":"markdown","source":"The pipeline follows a systematic path from raw text to refined prediction:\n\n- Environment Setup: Installs nlpaug for augmentation, transformers, and Optuna.\n- Data Loading & Cleaning: Loads `train.csv` and `test.csv`, applying light cleaning (removing excessive whitespace and handling null posts).\n- Data Balancing via Augmentation: Uses synonym replacement and random swapping to double the minority class size, resulting in a perfectly balanced training set.\n- Model Selection & Hyperparameter Tuning: Runs Optuna trials on 5 models using fold cross-validation to optimizes parameters like `weight_decay, label_smoothing, and warmup_ratio`.\n- Final Ensemble Training: Re-trains the optimized models on the full augmented dataset.\n- Prediction & Submission: Generates probabilities, applies a 0.5 threshold, and outputs the final `submission.csv`.","metadata":{}},{"cell_type":"markdown","source":"# Approach","metadata":{}},{"cell_type":"markdown","source":"This solution employs a modern, high-performance NLP pipeline tailored for social media text:\n\n**Data Preparation & Augmentation**\n\nInstead of simple oversampling, we use nlpaug to generate synthetic samples that maintain the original intent but vary the vocabulary. This \"jittering\" of the data helps transformers generalize better to slang and misspellings common in extremist rhetoric.\n\n**Model Selection**\n\nThe approach utilizes a diverse ensemble to capture different aspects of text:\n\n- RoBERTa-base & DeBERTa-v3: Excellent at understanding complex contextual relationships and long-range dependencies.\n- ELECTRA-base: Highly efficient at discriminative tasks.\n- DistilBERT: Provides a lightweight baseline to ensure the ensemble doesn't become overly \"top-heavy.\"\n- Toxic-Comment Model: A pre-specialized transformer that brings domain knowledge of online hate speech into the ensemble.\n\n**Training Strategy**\n\n- Mixed Precision (AMP): Allows training larger models on limited GPU memory by using 16-bit floats where possible.\n- Cosine Learning Rate Decay: Smoothly reduces the learning rate, helping the model settle into a more stable local minimum.\n- Soft-Voting Ensemble: By averaging the probabilities (rather than just the hard labels), the ensemble accounts for the confidence levels of each model, leading to more nuanced predictions.","metadata":{}},{"cell_type":"markdown","source":"# Environment and Configuration","metadata":{}},{"cell_type":"markdown","source":"## Environment and Package Installation","metadata":{}},{"cell_type":"markdown","source":"The first step prepares a specialized environment. It installs nlpaug for data creation and optuna for automated tuning.\n\n- Why this matters: Large language models (`transformers`) require specific versions of torch and transformers libraries to ensure that the pre-trained weights load correctly.","metadata":{}},{"cell_type":"code","source":"%%capture\n! pip install nlpaug\n! pip install torch==2.6.0\n! pip install optuna\n! pip install wordcloud\n! pip install protobuf==3.20.0\n! pip install kaggle\n! kaggle competitions download -c social-media-extremism-detection-challenge","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T16:31:06.970670Z","iopub.execute_input":"2026-01-11T16:31:06.970964Z","iopub.status.idle":"2026-01-11T16:35:01.595445Z","shell.execute_reply.started":"2026-01-11T16:31:06.970925Z","shell.execute_reply":"2026-01-11T16:35:01.593402Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T16:35:01.598072Z","iopub.execute_input":"2026-01-11T16:35:01.598356Z","iopub.status.idle":"2026-01-11T16:35:03.451737Z","shell.execute_reply.started":"2026-01-11T16:35:01.598327Z","shell.execute_reply":"2026-01-11T16:35:03.450852Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Download and Unzip Dataset","metadata":{}},{"cell_type":"markdown","source":"Due to this pipeline will be develop on cloud, so it has to download competition dataset via Kaggle API.","metadata":{}},{"cell_type":"code","source":"# configuring the path of Kaggle.json file\n!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json","metadata":{"tags":[]},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# extracting the compessed Dataset\nfrom zipfile import ZipFile\ndataset = 'social-media-extremism-detection-challenge.zip'\n\nwith ZipFile(dataset,'r') as zip:\n  zip.extractall()\n  print('The dataset is extracted')","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset is extracted\n"]}],"execution_count":8},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"markdown","source":"**Library Imports & Configuration**\n\nThis block loads the tools needed for data science (Pandas, NumPy) and deep learning (PyTorch).\n\n- Key Detail: A global random seed is set. In machine learning, \"randomness\" affects weight initialization and data splitting; setting a seed ensures that if you run the code twice, you get the exact same results.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim import AdamW\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    get_cosine_schedule_with_warmup\n)\n\nimport optuna\nimport joblib\n\nimport nlpaug.augmenter.word as naw\n\n# CONFIGURATION & DEVICE SETUP\n%matplotlib inline\n\nSEED = 42\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwarnings.simplefilter('ignore')\n\nSAVE_DIR = \"optuna_tuning\"\nos.makedirs(SAVE_DIR, exist_ok=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T16:35:03.452900Z","iopub.execute_input":"2026-01-11T16:35:03.453257Z","iopub.status.idle":"2026-01-11T16:36:27.912317Z","shell.execute_reply.started":"2026-01-11T16:35:03.453232Z","shell.execute_reply":"2026-01-11T16:36:27.910903Z"}},"outputs":[{"name":"stderr","text":"2026-01-11 16:35:15.501220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768149315.748293      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768149315.818759      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768149316.426986      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768149316.427039      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768149316.427041      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768149316.427044      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Load Dataset and Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"The raw social media messages are loaded and \"standardized.\"\n\n- Cleaning Process: It removes excessive repeated characters (e.g., `\"!!!!!\"`) which can confuse a model's tokenizer. It also handles empty posts to prevent the model from crashing during training.","metadata":{}},{"cell_type":"code","source":"# Load & Clean\ntrain_df = pd.read_csv('train.csv')\ntest_df  = pd.read_csv('test.csv')\n\ntrain_df.rename(columns={'Original_Message':'message', 'Extremism_Label':'label'}, inplace=True)\ntest_df.rename(columns={'Original_Message':'message'}, inplace=True)\n\ntest_ids = test_df['ID'].copy()\ntrain_df.drop(['ID'], axis=1, inplace=True)\ntest_df.drop(['ID'], axis=1, inplace=True)\ntrain_df.dropna(inplace=True)\n\ndef clean_text(text):\n    if not isinstance(text, str) or len(text.strip()) == 0:\n        return \"empty post\"\n    text = text.strip()\n    if len(text) < 5:\n        return \"empty post\"\n    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n\ntrain_df['message'] = train_df['message'].apply(clean_text)\ntest_df['message']  = test_df['message'].apply(clean_text)\n\ntrain_df['label'] = train_df['label'].map({'EXTREMIST': 1, 'NON_EXTREMIST': 0}).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train and Validation Split","metadata":{}},{"cell_type":"markdown","source":"The 2,776 training samples are split: 90 percent for training and 10 percent for validation.\n\n- Stratified Sampling: This ensures the 10 percent \"exam\" (validation set) has the same ratio of `EXTREMIST` to `NON_EXTREMIST` posts as the original data, preventing biased evaluation.","metadata":{}},{"cell_type":"code","source":"train_data, val_data = train_test_split(\n    train_df, test_size=0.1, random_state=42, stratify=train_df['label']\n)\n\nprint(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_df)}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 2024 | Val: 225 | Test: 750\n"]}],"execution_count":2},{"cell_type":"markdown","source":"## Oversampling (Augmentation)","metadata":{}},{"cell_type":"markdown","source":"nlpaug is a comprehensive Python library designed to automate data augmentation for Natural Language Processing (NLP) and audio tasks. In the same way you might flip or rotate an image to give a computer vision model more examples, nlpaug creates synthetic variations of your text or speech to improve model robustness, prevent overfitting, and balance small datasets.\n\n**Core Theories & Concepts**\n\nThe library is built around three fundamental architectural concepts:\n\n**1. The Augmenter (The \"How\")**\n\nAn Augmenter is the basic building block. It defines a specific strategy to change your data. Every augmenter typically follows a hierarchy: Level (Character, Word, or Sentence) to Action (Insert, Substitute, Swap, or Delete).\n\n- Insert: Adding new elements (chars/words) into the sequence.\n- Substitute: Replacing existing elements with something similar.\n- Swap: Changing the order of elements.\n- Delete: Removing elements to simulate missing data.\n\n**2. Flow (The \"Pipeline\")**\n\nReal-world data often contains multiple types of \"noise.\" A Flow allows you to arrange multiple augmenters into a pipeline.\n\n- Sequential: Applies a list of augmentations in a specific order.\n- Sometimes: Applies an augmentation only with a certain probability (e.g., 50 percent of the time).\n\n**3. Safety vs. Diversity**\n\nA key theoretical trade-off in NLP augmentation is Safety (preserving the original label/meaning) versus Diversity (how much the text changes). `nlpaug` offers tools across this spectrum, from \"Safe\" synonym replacement to \"Diverse\" contextual generation.\n\n**Levels of Augmentation**\n\nCharacter LevelFocuses on simulating \"noise\" common in human input or digital processing.\n- Keyboard Augmenter: Simulates typos by replacing characters with nearby keys on a QWERTY keyboard (e.g., \"Google\" to \"Goofle\").\n- OCR Augmenter: Simulates Optical Character Recognition errors where characters look similar (e.g., \"0\" vs \"O\", \"I\" vs \"1\").\n- Random Augmenter: Randomly inserts, swaps, or deletes characters.\n\n**Word Level**\n\nThese strategies change the vocabulary while attempting to keep the meaning intact.\n- Synonym Augmenter: Uses WordNet or PPDB to find and swap words with synonyms.\n- Word Embeddings Augmenter: Uses pre-trained vectors (Word2Vec, GloVe, FastText) to find words that are semantically \"close\" in vector space.\n- Contextual Word Embeddings: Uses Transformer models (BERT, RoBERTa) to predict the most likely word to fill a \"masked\" spot, ensuring the change fits the sentence's grammar.\n- Back Translation: Translates text to another language (e.g., German) and back to English to generate a paraphrase.\n\n**Sentence Level**\n\nFocuses on generating entirely new segments of text.\n- Abstractive Summarization: Uses models like BART or T5 to summarize a long text into a shorter, varied version.\n- Contextual Sentence Augmenter: Uses GPT-2 or XLNet to generate a continuation or a new sentence based on existing context.\n- LAMBADA: Specifically designed for few-shot learning by generating samples that follow a specific class distribution.\n\n**Why use it?**\n\n- Class Imbalance: If you have 1,000 \"Positive\" reviews but only 100 \"Negative\" ones, you can use nlpaug to synthesize 900 new negative reviews.\n- Robustness: By training on \"Keyboard\" noise, your model becomes better at understanding users who make typos.\n- Low Resource: It helps train deep learning models (which are data-hungry) when you have very limited labeled data.\n\n**Popular Augmenters in nlpaug**\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-za14\">Augmenter</th>\n    <th class=\"tg-7zrl\">Level</th>\n    <th class=\"tg-7zrl\">Theory/Concept</th>\n    <th class=\"tg-7zrl\">Best Use Case</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">KeyboardAug</td>\n    <td class=\"tg-7zrl\">Character</td>\n    <td class=\"tg-7zrl\">Typo Simulation</td>\n    <td class=\"tg-0lax\">Social media text / Chatbots</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">SynonymAug</td>\n    <td class=\"tg-7zrl\">Word</td>\n    <td class=\"tg-7zrl\">Lexical Semantics</td>\n    <td class=\"tg-0lax\">General text variety</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ContextualWordEmbs</td>\n    <td class=\"tg-7zrl\">Word</td>\n    <td class=\"tg-7zrl\">Transformers / Masked LM</td>\n    <td class=\"tg-0lax\">High-quality, fluent text</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">BackTranslation</td>\n    <td class=\"tg-7zrl\">Sentence</td>\n    <td class=\"tg-7zrl\">Round-trip Paraphrasing</td>\n    <td class=\"tg-0lax\">Diversifying sentence structure</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">RandomWordAug</td>\n    <td class=\"tg-7zrl\">Word</td>\n    <td class=\"tg-7zrl\">EDA (Easy Data Aug)</td>\n    <td class=\"tg-0lax\">Preventing overfitting</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**Data Balancing via Augmentation**\n\nBecause the dataset may have slight imbalances, nlpaug is used to create synthetic data.\n\n- Synonym Replacement: A word is swapped for its synonym (e.g., \"attack\" to \"assault\").\n- Random Swap: The order of two words is changed.\n- Result: This grows the training set to approx. 4,136 samples, creating a perfectly 50/50 balance that helps the model learn both classes equally well.","metadata":{}},{"cell_type":"code","source":"# Perfectly Balanced Augmentation\nprint(\"\\nGenerating perfectly balanced training data...\")\n\nsynonym_aug = naw.SynonymAug(aug_p=0.3)\nswap_aug    = naw.RandomWordAug(action=\"swap\", aug_p=0.25)\n\ndef strong_augment(text):\n    t = synonym_aug.augment(text)\n    t = t[0] if isinstance(t, list) else t\n    t = swap_aug.augment(t)\n    t = t[0] if isinstance(t, list) else t\n    return t\n\nn_ext = (train_data['label'] == 1).sum()\nn_non = (train_data['label'] == 0).sum()\ntarget_per_class = max(n_ext, n_non) * 2\n\nprint(f\"Original : NON_EXTREMIST: {n_non}, EXTREMIST: {n_ext}\")\nprint(f\"Target per class : {target_per_class}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Generating perfectly balanced training data...\n","Original : NON_EXTREMIST: 990, EXTREMIST: 1034\n","Target per class : 2068\n"]}],"execution_count":3},{"cell_type":"code","source":"aug_samples = []\n\n# Augment NON_EXTREMIST\nnon_df = train_data[train_data['label'] == 0]\nneeded_non = target_per_class - n_non\nprint(f\"Generating {needed_non} NON_EXTREMIST samples...\")\nfor _ in range(needed_non):\n    sample = non_df.sample(1).iloc[0]\n    aug_samples.append({'message': strong_augment(sample['message']), 'label': 0})\n\n# Augment EXTREMIST (light)\next_df = train_data[train_data['label'] == 1]\nneeded_ext = target_per_class - n_ext\nprint(f\"Generating {needed_ext} EXTREMIST samples...\")\nfor _ in range(needed_ext):\n    sample = ext_df.sample(1).iloc[0]\n    new_text = synonym_aug.augment(sample['message'])\n    new_text = new_text[0] if isinstance(new_text, list) else new_text\n    aug_samples.append({'message': new_text, 'label': 1})\n\ntrain_balanced = pd.concat([train_data, pd.DataFrame(aug_samples)], ignore_index=True)\ntrain_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(f\"Final balanced training set: {len(train_balanced)} samples\")\nprint(train_balanced['label'].value_counts().sort_index().to_string())","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating 1078 NON_EXTREMIST samples...\n","Generating 1034 EXTREMIST samples...\n","Final balanced training set: 4136 samples\n","label\n","0    2068\n","1    2068\n"]}],"execution_count":4},{"cell_type":"markdown","source":"## Dataset Tokenization","metadata":{}},{"cell_type":"markdown","source":"In the world of Large Language Models (LLMs), a tokenizer is the essential translation layer that converts raw human text into a format that a neural network can process. While humans read words, LLMs read numbers.\n\n**Why Not Just Use Words?**\n\nYou might wonder why we don't just give every word in the dictionary its own number. Modern LLMs use Subword Tokenization (like BPE or WordPiece) for several critical reasons:\n- Handling New Words: If a model only knows \"happy,\" it would be confused by \"unhappiness.\" Subword tokenizers break it into un + happi + ness. This allows the model to understand words it has never seen before by looking at their components.\n- Efficiency: Character-level tokenization (A, B, C...) makes sequences too long and hard to process. Word-level tokenization makes the \"vocabulary\" too large (millions of words). Subword tokenization hits the \"sweet spot.\"\n- Root Recognition: It helps the model see that \"running,\" \"runner,\" and \"runs\" all share the root \"run,\" rather than treating them as three entirely unrelated concepts.\n\n**1. The Core Concept:From Text to Tensors**\n\nA tokenizer follows a three-step theoretical process to prepare data for the model:\n\n**Step A: Normalization**\n\nThe raw text is cleaned. This includes removing extra whitespaces, converting to lowercase (if the model is \"uncased\"), and sometimes handling unicode normalization (ensuring \"é\" is represented consistently).\n\n**Step B: Pre-tokenization & Splitting**\n\nThe text is split into smaller units. Historically, there were three ways to do this:\n- Word-level: Every word is a token.\n- Problem: Massive vocabulary size and inability to handle \"Out-of-Vocabulary\" (OOV) words (e.g., if the model knows \"run\" but not \"running\").\n- Character-level: Every letter is a token.\n- Problem: Sequences become extremely long, and characters alone carry very little meaning.\n- Subword-level (The Standard): This is what modern LLMs use. It breaks frequent words into single units and rare words into multiple chunks (e.g., \"smartwatch\" becomes [\"smart\", \"##watch\"]).\n\n**Step C: Numerical Mapping**\n\nEvery unique subword in the tokenizer’s vocabulary is assigned a unique Input ID (an integer). The model uses these IDs to look up the corresponding Embedding Vector (the mathematical representation of that word's meaning).\n\n**2. Theoretical Algorithms**\n\nThere are three main algorithms used by popular LLMs:\n- Byte Pair Encoding (BPE): Used by GPT-3, GPT-4, and Llama.10 It starts with individual characters and iteratively merges the most frequently occurring adjacent pairs into a single new token.\n- WordPiece: Used by BERT. It is similar to BPE but uses a likelihood-based approach rather than just raw frequency to decide which characters to merge.\n- Unigram: Used by T5 and ALBERT. Instead of building up from characters, it starts with a massive vocabulary and trims away the least useful tokens until it reaches the desired size.\n\n**3. How the Tokenization Process Works**\n\nThe transition from text to machine-readable data happens in three distinct stages:\n\n- Normalization: The tokenizer cleans the text by removing extra spaces, standardizing casing, or handling special characters (like converting \"Résumé\" to \"resume\").\n- Splitting (Segmentation): The text is broken into chunks. Depending on the model, a token can be a whole word, a part of a word (subword), or even a single character.\n- Mapping to IDs: Each unique chunk is looked up in a \"vocabulary\" (a massive dictionary) and replaced with its corresponding integer ID.","metadata":{}},{"cell_type":"markdown","source":"This creates a \"data pipeline\" for PyTorch. It takes raw text and uses a Tokenizer to turn words into numbers (`input_ids`) and creates an `attention_mask` so the model knows which parts of the text to focus on and which are just padding.","metadata":{}},{"cell_type":"code","source":"# Dataset Class \nclass ExtremismDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=256):\n        self.texts   = df['message'].values\n        self.labels  = df['label'].values if 'label' in df.columns else None\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self): return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx] if self.labels is not None else -1\n\n        enc = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_len,\n            padding=False,\n            add_special_tokens=True,\n            return_attention_mask=True\n        )\n\n        item = {\n            'input_ids': torch.tensor(enc['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(enc['attention_mask'], dtype=torch.long),\n        }\n        if label != -1:\n            item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"The pipeline uses five distinct \"expert\" models. By combining different architectures, the final prediction becomes more robust.","metadata":{}},{"cell_type":"markdown","source":"## Initial Model Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"**StratifiedKFold**\n\n`StratifiedKFold` is a specialized version of the standard K-Fold cross-validation technique. While regular K-Fold randomly splits data into 2$k$ groups (folds), `StratifiedKFold` ensures that each fold maintains the same proportion of classes as the original dataset.\n\nThis is critical when you have imbalanced data—for example, if you are detecting a rare disease where only 1% of patients are positive.4 A random split might accidentally create a \"fold\" with 0% of those positive cases, making it impossible for the model to learn or be tested fairly.\n\n**How it Works (Step-by-Step)**\n\nImagine you have a dataset with 100 samples: 80 are \"Neutral\" and 20 are \"Extremist\" ($80:20$ ratio). If you use a 5-fold StratifiedKFold:\n- Calculate the Ratio: It calculates that the ratio is 4:1.\n- Divide the Classes: It separates the 80 Neutrals and 20 Extremists.\n- Distribute Evenly: It places exactly 16 Neutrals and 4 Extremists into each of the 5 folds.\n- Rotate: It trains on 4 folds and validates on 1, repeating this 5 times until every sample has been part of a validation set once.","metadata":{}},{"cell_type":"markdown","source":"**1. RoBERTa-base (`FacebookAI/roberta-base`)**\n\n- What it is: A \"Robustly Optimized\" version of the original BERT model.\n- The Theory: Facebook researchers found that the original BERT was significantly under-trained. RoBERTa uses the same architecture but changes the training recipe.\n- Key Innovations: \n    - Dynamic Masking: Unlike BERT, which masks the same words in every epoch, RoBERTa changes the mask every time it sees a sequence, forcing it to learn more general patterns.\n    - More Data: Trained on 160GB of text (10x more than BERT).\n    - No NSP: It removed the \"Next Sentence Prediction\" task, which was found to be unnecessary.\n- Role in Ensemble: It serves as a highly stable \"all-rounder\" with a deep understanding of English linguistics.\n\n**2. DeBERTa-v3-base (`microsoft/deberta-v3-base`)**\n\n- What it is: Currently one of the strongest \"base-sized\" models in existence.\n- The Theory: It uses \"Disentangled Attention,\" which treats the content of a word and its relative position as two separate vectors.\n- Key Innovations:\n    - V3 Upgrade: The \"v3\" version specifically uses ELECTRA-style pre-training (explained below), making it much more sample-efficient.\n    - Gradient-Disentangled Embedding: Improves how the model shares information between the layers.\n- Role in Ensemble: This is likely your \"heavy lifter.\" It is exceptionally good at understanding the context and intent behind social media posts.\n\n**3. Toxic-Comment-Model (`martin-ha/toxic-comment-model`)**\n\n- What it is: A specialized model already \"tuned\" for detecting harmful language.\n- The Theory: It is based on DistilBERT but has been fine-tuned on the Jigsaw Toxic Comment Classification dataset.\n- Key Feature: While the other models are general-purpose, this one was born to detect toxicity, insults, and threats.\n- Role in Ensemble: It provides domain expertise. Since extremist content often overlaps with toxicity, this model brings \"pre-existing knowledge\" of what hate speech looks like.\n\n**4. ELECTRA-base (`google/electra-base-discriminator`)**\n\n- What it is: A \"Detective\" model that learns by spotting fakes.\n- The Theory: Instead of guessing \"Masked\" words, ELECTRA uses a Generator-Discriminator setup (similar to a GAN).\n- Key Innovation:\n    - Replaced Token Detection: A small model (the Generator) replaces some words with plausible fakes. The main model (the Discriminator) must decide for every word: \"Is this original or a fake?\"\n- Role in Ensemble: Because it checks every single token for \"suspicion,\" it is very good at spotting \"dog whistles\" or coded language often used in extremist circles.\n\n**5. DistilBERT-base (`distilbert/distilbert-base-uncased`)**\n\n- What it is: A smaller, faster, \"distilled\" version of BERT.\n- The Theory: It uses Knowledge Distillation, where a large \"Teacher\" model (BERT) teaches a smaller \"Student\" model (DistilBERT) how to behave.\n- Key Features:\n    - Efficiency: 40% smaller and 60% faster than BERT.\n    - Uncased: It treats \"Extremist\" and \"extremist\" exactly the same, reducing the complexity of the vocabulary.\n- Role in Ensemble: It acts as a regularizer. Because it is simpler, it is less likely to overfit on tiny noise in the training data, helping the ensemble stay generalized.","metadata":{}},{"cell_type":"markdown","source":"**Ensemble Model : Soft Voting**\n\nThe final model uses Soft Voting (also known as Weighted Averaging) to determine if a post is extremist.\n\n- How it works: Each of the 5 models outputs a probability score (e.g., 0.85 chance of being extremist).\n- The Math: The code takes the average of these probabilities:$$\\text{Final Probability} = \\frac{\\text{RoBERTa} + \\text{DeBERTa} + \\text{ToxicModel} + \\text{ELECTRA} + \\text{DistilBERT}}{5}$$\n- Why it's better: If RoBERTa is 51% sure it's extremist, but DeBERTa is 99% sure, soft voting gives more \"weight\" to DeBERTa's high confidence. A simple \"Hard Vote\" (majority rule) would treat them both as just \"1 vote.\"","metadata":{}},{"cell_type":"code","source":"# 5-MODEL ENSEMBLE \nMODELS = [\n    \"FacebookAI/roberta-base\",\n    \"microsoft/deberta-v3-base\",\n    \"martin-ha/toxic-comment-model\",\n    \"google/electra-base-discriminator\",     # Added: Strong \"weak\" learner\n    \"distilbert/distilbert-base-uncased\"      # Added: Fast & effective\n]\n\ntest_predictions = {}\nval_predictions = {}\n\nskf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)   # Increase number of validation split for more reliable result\n\nfor model_name in MODELS:\n    print(f\"\\nInintial Fined-Tuning {model_name} with 2-fold CV...\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    fold_test_probs = []\n\n    for fold, (tr_idx, val_idx) in enumerate(skf.split(train_balanced, train_balanced['label'])):\n        print(f\"  Fold {fold+1}/2\", end=\"\\r\")             ##############\n\n        tr_fold = train_balanced.iloc[tr_idx].reset_index(drop=True)\n        va_fold = train_balanced.iloc[val_idx].reset_index(drop=True)\n\n        train_ds = ExtremismDataset(tr_fold, tokenizer)\n        val_ds   = ExtremismDataset(va_fold, tokenizer)\n        \n        train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,  collate_fn=data_collator)\n        val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, collate_fn=data_collator)\n\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n        model.to(DEVICE)\n\n        optimizer = AdamW([\n            {'params': model.base_model.parameters() if hasattr(model, 'base_model') else model.parameters(), 'lr': 1e-5},\n            {'params': model.classifier.parameters(), 'lr': 1e-4}\n        ], weight_decay=0.01)\n\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_loader)*10)\n        scaler = GradScaler('cuda')\n\n        best_acc = 0\n        patience = 2\n        wait = 0\n\n        for epoch in range(10):\n            model.train()\n            for batch in train_loader:\n                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                with autocast('cuda'):\n                    outputs = model(**batch)\n                    loss = outputs.loss\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad()\n\n            # Validation\n            model.eval()\n            preds = []\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                    with autocast('cuda'):\n                        logits = model(**batch).logits\n                    preds.extend(torch.softmax(logits, dim=1)[:,1].cpu().numpy())\n            acc = accuracy_score(va_fold['label'], (np.array(preds)>0.5).astype(int))\n            print(f\"\\nValidation Accuracy: {acc:.4f}\")\n            \n            if acc > best_acc:\n                best_acc = acc\n                wait = 0\n                torch.save(model.state_dict(), f\"/best_{model_name.split('/')[-1]}_f{fold}.pt\")\n            else:\n                wait += 1\n                if wait >= patience:\n                    break\n\n    # Held-out validation (on original val_data)\n    val_ds_final = ExtremismDataset(val_data, tokenizer)\n    val_loader_final = DataLoader(val_ds_final, batch_size=2, collate_fn=data_collator)\n    all_val_probs = []\n    for f in range(5):\n        try:\n            model.load_state_dict(torch.load(f\"/best_{model_name.split('/')[-1]}_f{f}.pt\"))\n            model.eval()\n            probs = []\n            with torch.no_grad():\n                for batch in val_loader_final:\n                    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                    with autocast('cuda'):\n                        logits = model(**batch).logits\n                    probs.extend(torch.softmax(logits, dim=1)[:,1].cpu().numpy())\n            all_val_probs.append(probs)\n        except:\n            pass  # Skip missing folds\n    val_predictions[model_name] = np.mean(all_val_probs, axis=0) if all_val_probs else np.zeros(len(val_data))\n\n# Final 5-Model Ensemble + Held-out Accuracy\nfinal_val_prob = np.mean(list(val_predictions.values()), axis=0)\nval_acc = accuracy_score(val_data['label'], (final_val_prob > 0.5).astype(int))\nprint(f\"\\nFINAL HELD-OUT VALIDATION ACCURACY (5-Model Ensemble): {val_acc:.4f}\")","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Inintial Fined-Tuning FacebookAI/roberta-base with 2-fold CV...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ed78bec38974857b7f29c0995edbf09","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3b579808ad6464dbbde77f102e9d0c4","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"681388418b9349499bae90102ef2c4e5","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e083264578c40e79e87797a77f2ca68","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a820f433f3e246d9aab9187f9056c9b3","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Fold 1/2\r"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20864432df954e02b1dc4c251e3eb8e2","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.7340\n","\n","Validation Accuracy: 0.8714\n","\n","Validation Accuracy: 0.8893\n","\n","Validation Accuracy: 0.8946\n","\n","Validation Accuracy: 0.9115\n","\n","Validation Accuracy: 0.9163\n","\n","Validation Accuracy: 0.9033\n","\n","Validation Accuracy: 0.9149\n","  Fold 2/2\r"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8225\n","\n","Validation Accuracy: 0.8085\n","\n","Validation Accuracy: 0.8946\n","\n","Validation Accuracy: 0.9009\n","\n","Validation Accuracy: 0.8985\n","\n","Validation Accuracy: 0.8825\n","\n","Inintial Fined-Tuning microsoft/deberta-v3-base with 2-fold CV...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd16cb731dcb44f6bad39754c38040c9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f2b322c33ae4efbbb3b58b5f165c540","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ea7fbb88cba4059ad70fefd508845fd","version_major":2,"version_minor":0},"text/plain":["spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Fold 1/2\r"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6ada941fbd940178c605517219817d8","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8656\n","\n","Validation Accuracy: 0.8936\n","\n","Validation Accuracy: 0.8907\n","\n","Validation Accuracy: 0.9110\n","\n","Validation Accuracy: 0.9120\n","\n","Validation Accuracy: 0.9120\n","\n","Validation Accuracy: 0.8999\n","  Fold 2/2\r"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8583\n","\n","Validation Accuracy: 0.8878\n","\n","Validation Accuracy: 0.8951\n","\n","Validation Accuracy: 0.8980\n","\n","Validation Accuracy: 0.8960\n","\n","Validation Accuracy: 0.8931\n","\n","Inintial Fined-Tuning martin-ha/toxic-comment-model with 2-fold CV...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7ab3596307242a6913f07cc1d9aedba","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/403 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50cbcdf229e147408e49a0db925ea146","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"978c145dae9349d69245c08f79a19c9e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3c6cc1af156440abe7865db5a8c4d5a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Fold 1/2\r"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58917ea77b7244b4b3ab63eb9c787fb7","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff658625441f44c9970aabcd763327fd","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.6431\n","\n","Validation Accuracy: 0.6712\n","\n","Validation Accuracy: 0.6668\n","\n","Validation Accuracy: 0.7065\n","\n","Validation Accuracy: 0.7495\n","\n","Validation Accuracy: 0.7655\n","\n","Validation Accuracy: 0.7684\n","\n","Validation Accuracy: 0.7631\n","\n","Validation Accuracy: 0.7640\n","  Fold 2/2\n","Validation Accuracy: 0.6228\n","\n","Validation Accuracy: 0.7234\n","\n","Validation Accuracy: 0.7427\n","\n","Validation Accuracy: 0.7602\n","\n","Validation Accuracy: 0.7582\n","\n","Validation Accuracy: 0.7650\n","\n","Validation Accuracy: 0.7737\n","\n","Validation Accuracy: 0.7674\n","\n","Validation Accuracy: 0.7684\n","\n","Inintial Fined-Tuning google/electra-base-discriminator with 2-fold CV...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67f9f9496cc640aaa99d313af6c753a6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d69791a4c614ec8aeaa382dccf65594","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13c58e0713a549bf87d0f7f390f4d064","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4c472f2f17d4d8da4b3490453e87faa","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Fold 1/2\r"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n","  _torch_pytree._register_pytree_node(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b3e229573cd46ac978c85af191c1f1b","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8694\n","\n","Validation Accuracy: 0.8612\n","\n","Validation Accuracy: 0.8965\n","\n","Validation Accuracy: 0.8752\n","\n","Validation Accuracy: 0.9057\n","\n","Validation Accuracy: 0.9096\n","\n","Validation Accuracy: 0.9018\n","\n","Validation Accuracy: 0.9086\n","  Fold 2/2\r"]},{"name":"stderr","output_type":"stream","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8540\n","\n","Validation Accuracy: 0.8486\n","\n","Validation Accuracy: 0.8844\n","\n","Validation Accuracy: 0.8994\n","\n","Validation Accuracy: 0.8994\n","\n","Validation Accuracy: 0.9038\n","\n","Validation Accuracy: 0.9038\n","\n","Validation Accuracy: 0.9052\n","\n","Validation Accuracy: 0.9033\n","\n","Validation Accuracy: 0.9023\n","\n","Inintial Fined-Tuning distilbert/distilbert-base-uncased with 2-fold CV...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a94c8c178d447baa4473664bd29811b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24602d7590f64426bf58f1537ab68360","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db8504042fc142faba5b7cacca4e655d","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cddda1d996f49cea674e09471f018ba","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["  Fold 1/2\r"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd41959ff90c4d17abfcfbbaf491773d","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8607\n","\n","Validation Accuracy: 0.8690\n","\n","Validation Accuracy: 0.8839\n","\n","Validation Accuracy: 0.8965\n","\n","Validation Accuracy: 0.8864\n","\n","Validation Accuracy: 0.8994\n","\n","Validation Accuracy: 0.8999\n","\n","Validation Accuracy: 0.8956\n","\n","Validation Accuracy: 0.8956\n","  Fold 2/2\r"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Validation Accuracy: 0.8607\n","\n","Validation Accuracy: 0.8709\n","\n","Validation Accuracy: 0.8975\n","\n","Validation Accuracy: 0.9018\n","\n","Validation Accuracy: 0.8946\n","\n","Validation Accuracy: 0.9009\n","\n","FINAL HELD-OUT VALIDATION ACCURACY (5-Model Ensemble): 0.8267\n"]}],"execution_count":2},{"cell_type":"markdown","source":"## Hyperparameters Tuning","metadata":{}},{"cell_type":"markdown","source":"Optuna is an open-source framework designed to automate Hyperparameter Optimization (HPO). While traditional methods like Grid Search or Random Search blindly try combinations of settings, Optuna uses a \"smart\" approach called Bayesian Optimization to learn from past experiments and find the best settings faster.\n\n**Key Concepts: Study vs. Trial**\n\nOptuna organizes its work into two main categories:\n- Study: The overall optimization project (e.g., \"Find the best settings for RoBERTa\").\n- Trial: A single \"experiment\" where Optuna tries one specific set of hyperparameters, trains the model, and checks the score.\n\n**How Optuna Works: The \"Trial & Study\" Loop**\n\nOptuna treats hyperparameter tuning as an iterative process involving three main components:\n- Objective Function: This is the core logic you write. It takes a \"Trial\" object, trains your model with suggested parameters, and returns a score (like Accuracy or Loss).\n- Study: This is the \"manager\" that coordinates all experiments. It tracks the history of results and decides which hyperparameters to try next.\n- Trial: A single execution of your objective function. During a trial, Optuna \"suggests\" specific values for your hyperparameters.\n\n**The Internal Mechanics**\n\nOptuna is more efficient than other tools because it uses two specialized engines:\n\n**A. The Sampler**\n\nBy default, Optuna uses the Tree-structured Parzen Estimator (TPE).\n- Logic: It divides previous results into \"good\" and \"bad\" groups. It then calculates the probability that a new set of parameters belongs to the \"good\" group.\n- Result: It focuses the search on promising areas of the map rather than wasting time on settings that consistently fail.\n\n**B. The Pruner**\n\nPruning is automated early stopping.\n- Logic: If a trial is halfway through training and its intermediate score (e.g., validation loss after 5 epochs) is significantly worse than the median of previous trials, Optuna kills the trial immediately.\n- Result: This saves massive amounts of GPU/CPU time, allowing you to run 5–10 times more experiments in the same window.","metadata":{}},{"cell_type":"markdown","source":"**Defined Fine-Tuned Hyperparameters**\n\n**1. Learning Rate Parameters**\n\nThese control how fast the \"brain\" of the model updates its knowledge.\n- `lr_backbone` ($1\\times10^{-6}$ to $1\\times10^{-4}$):\n    - Meaning: The learning rate for the main Transformer layers (the pre-trained part).\n    - Why it matters: We use a smaller rate here because the backbone already knows English. We don't want to \"overwrite\" its existing knowledge too aggressively.\n- `lr_classifier` ($1\\times10^{-5}$ to $1\\times10^{-3}$):\n    - Meaning: The learning rate for the final \"head\" that decides if text is extremist.\n    - Why it matters: This layer starts from scratch (randomly initialized), so it usually needs a higher learning rate than the backbone to learn the specific task quickly.\n\n**2. Training Dynamics & Stability**\n\nThese parameters manage the math and memory usage during training.\n- `batch_size` (2, 4, 8, 16):\n    - Meaning: How many rows of data the model looks at before calculating an error. Smaller batches (2, 4) are noisier but fit in GPU memory; larger batches (16) provide a smoother \"map\" for the model to follow.\n- `gradient_accumulation_steps` (1, 2, 4, 8):\n    - Meaning: This simulates a larger batch size. If your batch is 2 and accumulation is 8, the model acts as if it saw 16 rows before actually updating its weights. This is a \"memory hack.\"\n- `max_grad_norm` (0.1 to 5.0):\n    - Meaning: Also known as Gradient Clipping. If the math \"explodes\" and produces a massive update number, this hyperparameter \"clips\" it down to a maximum value.\n    - Why it matters: It prevents the model from \"breaking\" during training if it encounters a very confusing sentence.\n\n**3. Optimizer Configuration (`AdamW`)**\n\nThe model uses the AdamW optimizer, which has \"sub-parameters\" that control its \"memory.\"\n- `adam_epsilon` ($1\\times10^{-10}$ to $1\\times10^{-5}$):\n    - Meaning: A tiny number added to the denominator to prevent \"division by zero\" errors in the math.\n- `adam_beta1 & adam_beta2`:\n    - Meaning: These control the \"momentum.\" Beta1 (0.8–0.95) is how much the model remembers the direction of previous updates. Beta2 (0.98–0.99) is how much it remembers the volatility of those updates.\n- `warmup_ratio` (0.0 to 0.5):\n    - Meaning: The percentage of the first steps where the learning rate starts at zero and slowly climbs to the maximum.\n    - Why it matters: Like a car engine in winter, the model needs to \"warm up\" to avoid crashing (diverging) at the very start of training.\n\n**4. Generalization & Regularization**\n\nThese prevent the model from simply memorizing the training data (overfitting).\n- `dropout` (0.0 to 0.5):\n    - Meaning: Randomly \"turns off\" some neurons during training.\n    - Why it matters: It forces the model to find multiple patterns instead of relying on just one specific word.\n- `layerwise_decay` (0.85 to 1.0):\n    - Meaning: A sophisticated technique where lower layers (close to the input) learn slower than higher layers.\n    - The Logic: Layer 1 knows \"Grammar\" (don't change it much). Layer 12 knows \"Context/Extremism\" (change it more).\n- `label_smoothing` (0.0 to 0.3):\n    - Meaning: Instead of telling the model a post is \"100% Extremist,\" you tell it the post is \"90% Extremist.\"\n    - Why it matters: It prevents the model from becoming \"overconfident,\" which helps it perform better on new, unseen data.\n- `patience` (2 to 10):\n    - Meaning: How many epochs to wait for the score to improve before giving up (Early Stopping).","metadata":{}},{"cell_type":"markdown","source":"Instead of manually guessing settings, Optuna runs dozens of \"trials\" to find the best configuration for each model.\n\n- Parameters Tuned: Learning rate, batch size, weight decay, and dropout.\n- Fold CV: Each trial is tested twice on different slices of data to ensure the settings aren't just \"lucky.\"","metadata":{}},{"cell_type":"code","source":"# MAIN TUNING & TRAINING \nMODELS = [\n    \"martin-ha/toxic-comment-model\",\n    \"google/electra-base-discriminator\",\n    \"distilbert/distilbert-base-uncased\",\n    \"FacebookAI/roberta-base\",\n    \"microsoft/deberta-v3-base\"\n]\n\n# EXPANDED HYPERPARAM RANGES FOR BETTER ACCURACY (>0.97 target)\nBATCH_SIZE_CHOICES = [2, 4, 8, 16]  # Added 32\nGRAD_ACC_CHOICES = [1, 2, 4, 8]        # Added 8\nLAYERWISE_CHOICES = [0.85, 0.9, 0.95, 0.98, 1.0]  # Added 0.85\n\nval_predictions_for_meta = {}  # same length for all models\nbest_params = {}\nstudies = {}\nskf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)              # Increase number of validation split for more reliable result\n\ndef tune_and_train(model_name):\n    model_key = model_name.split(\"/\")[-1]\n    study_path = f\"{SAVE_DIR}/{model_key}_study.pkl\"\n\n    if os.path.exists(study_path):\n        study = joblib.load(study_path)\n        print(f\"Resumed study for {model_name} ({len(study.trials)} trials)\")\n    else:\n        study = optuna.create_study(direction='maximize')\n        print(f\"Started new study for {model_name}\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    def objective(trial):\n        params = {\n            'lr_backbone': trial.suggest_float('lr_backbone', 1e-6, 1e-4, log=True),  \n            'lr_classifier': trial.suggest_float('lr_classifier', 1e-5, 1e-3, log=True),  \n            'batch_size': trial.suggest_categorical('batch_size', BATCH_SIZE_CHOICES),\n            'epochs': trial.suggest_int('epochs', 5, 30),  \n            'max_grad_norm': trial.suggest_float('max_grad_norm', 0.1, 5.0),  \n            'gradient_accumulation_steps': trial.suggest_categorical('gradient_accumulation_steps', GRAD_ACC_CHOICES),\n            'warmup_ratio': trial.suggest_float('warmup_ratio', 0.0, 0.5),  \n            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 0.1, log=True),  \n            'adam_epsilon': trial.suggest_float('adam_epsilon', 1e-10, 1e-5, log=True),  \n            'adam_beta1': trial.suggest_float('adam_beta1', 0.8, 0.95),  \n            'adam_beta2': trial.suggest_float('adam_beta2', 0.98, 0.9999),  \n            'dropout': trial.suggest_float('dropout', 0.0, 0.5),  \n            'layerwise_decay': trial.suggest_categorical('layerwise_decay', LAYERWISE_CHOICES),\n            'label_smoothing': trial.suggest_float('label_smoothing', 0.0, 0.3),  \n            'patience': trial.suggest_int('patience', 2, 10)  \n        }\n\n        # Training loop\n        tr_idx, va_idx = next(skf.split(train_balanced, train_balanced['label']))\n        tr_fold = train_balanced.iloc[tr_idx].reset_index(drop=True)\n        va_fold = train_balanced.iloc[va_idx].reset_index(drop=True)\n\n        train_ds = ExtremismDataset(tr_fold, tokenizer)\n        val_ds = ExtremismDataset(va_fold, tokenizer)\n\n        train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True, collate_fn=data_collator)\n        val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=data_collator)\n\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n        # Apply dropout\n        if hasattr(model.config, 'hidden_dropout_prob'):\n            model.config.hidden_dropout_prob = params['dropout']\n        if hasattr(model.config, 'attention_probs_dropout_prob'):\n            model.config.attention_probs_dropout_prob = params['dropout']\n        if hasattr(model.config, 'classifier_dropout'):\n            model.config.classifier_dropout = params['dropout']\n\n        model.to(DEVICE)\n\n        # Optimizer\n        if params['layerwise_decay'] < 1.0:\n            param_groups = []\n            layer_idx = 0\n            num_layers = 12\n            for name, param in model.named_parameters():\n                if 'classifier' in name or 'pooler' in name:\n                    lr = params['lr_classifier']\n                else:\n                    decay_factor = params['layerwise_decay'] ** (num_layers - layer_idx)\n                    lr = params['lr_backbone'] * decay_factor\n                    if 'layer' in name and f\"layer.{layer_idx}\" in name:\n                        layer_idx += 1\n                param_groups.append({'params': param, 'lr': lr})\n            optimizer = AdamW(param_groups, weight_decay=params['weight_decay'],\n                              eps=params['adam_epsilon'], betas=(params['adam_beta1'], params['adam_beta2']))\n        else:\n            optimizer = AdamW([\n                {'params': model.base_model.parameters() if hasattr(model, 'base_model') else model.parameters(),\n                 'lr': params['lr_backbone']},\n                {'params': model.classifier.parameters(), 'lr': params['lr_classifier']}\n            ], weight_decay=params['weight_decay'], eps=params['adam_epsilon'],\n               betas=(params['adam_beta1'], params['adam_beta2']))\n\n        total_steps = len(train_loader) * params['epochs'] // params['gradient_accumulation_steps']\n        warmup_steps = int(total_steps * params['warmup_ratio'])  # Use ratio here\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\n        scaler = GradScaler('cuda')\n\n        best_acc = 0\n        wait = 0\n        for epoch in range(params['epochs']):\n            model.train()\n            optimizer.zero_grad()\n            accumulated = 0\n            for batch in train_loader:\n                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                with autocast('cuda'):\n                    outputs = model(**batch)\n                    loss = outputs.loss\n                    if params['label_smoothing'] > 0.0:\n                        log_probs = torch.log_softmax(outputs.logits, dim=-1)\n                        smooth_loss = -log_probs.mean()\n                        loss = (1 - params['label_smoothing']) * loss + params['label_smoothing'] * smooth_loss\n                scaler.scale(loss / params['gradient_accumulation_steps']).backward()\n                accumulated += 1\n                if accumulated % params['gradient_accumulation_steps'] == 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), params['max_grad_norm'])\n                    scaler.step(optimizer)\n                    scaler.update()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    accumulated = 0\n\n            # Validation\n            model.eval()\n            preds = []\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                    with autocast('cuda'):\n                        logits = model(**batch).logits\n                    preds.extend(torch.softmax(logits, dim=1)[:,1].cpu().numpy())\n            acc = accuracy_score(va_fold['label'], (np.array(preds)>0.5).astype(int))\n\n            if acc > best_acc:\n                best_acc = acc\n                wait = 0\n            else:\n                wait += 1\n                if wait >= params['patience']:\n                    break\n        return best_acc\n\n    study.optimize(objective, n_trials=3)                   # Increase number of trial for better accuracy score\n    # Save study\n    best_params[model_name] = study.best_params\n    studies[model_name] = study\n    joblib.dump(study, study_path)\n    print(f\"Saved: {model_name} tuning complete\")\n    print(f\"Best accuracy: {study.best_value:.5f}\")\n    print(f\"Best params: {study.best_params}\")\n\n    # Final training + collect predictions on ORIGINAL val_data\n    val_loader = DataLoader(ExtremismDataset(val_data, tokenizer), batch_size=2, shuffle=False, collate_fn=data_collator)\n\n    # Final model with best params\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n    if hasattr(model.config, 'hidden_dropout_prob'):\n        model.config.hidden_dropout_prob = best_params[model_name]['dropout']\n    if hasattr(model.config, 'attention_probs_dropout_prob'):\n        model.config.attention_probs_dropout_prob = best_params[model_name]['dropout']\n    model.to(DEVICE)\n\n    # Use warmup_ratio\n    dummy_loader = DataLoader(ExtremismDataset(train_balanced, tokenizer), batch_size=best_params[model_name]['batch_size'], shuffle=True, collate_fn=data_collator)\n    total_steps = len(dummy_loader) * best_params[model_name]['epochs'] // best_params[model_name]['gradient_accumulation_steps']\n    warmup_steps = int(total_steps * best_params[model_name]['warmup_ratio'])\n\n    optimizer = AdamW([\n        {'params': model.base_model.parameters() if hasattr(model, 'base_model') else model.parameters(),\n         'lr': best_params[model_name]['lr_backbone']},\n        {'params': model.classifier.parameters(), 'lr': best_params[model_name]['lr_classifier']}\n    ], weight_decay=best_params[model_name]['weight_decay'])\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n    scaler = GradScaler('cuda')\n\n    # Train on full train_balanced\n    for epoch in range(best_params[model_name]['epochs']):\n        model.train()\n        for batch in dummy_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            with autocast('cuda'):\n                outputs = model(**batch)\n                loss = outputs.loss\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), best_params[model_name]['max_grad_norm'])\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n\n    # Predict on val_data \n    model.eval()\n    val_preds = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            with autocast('cuda'):\n                logits = model(**batch).logits\n            val_preds.extend(torch.softmax(logits, dim=1)[:,1].cpu().numpy())\n    val_predictions_for_meta[model_name] = np.array(val_preds)\n\n# Run all\nfor model_name in MODELS:\n    tune_and_train(model_name)","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2026-01-10 20:00:59,866] A new study created in memory with name: no-name-0fb7cb93-834c-4656-882a-5318fac619ca\n"]},{"name":"stdout","output_type":"stream","text":["Started new study for martin-ha/toxic-comment-model\n"]},{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","[I 2026-01-10 20:04:16,258] Trial 0 finished with value: 0.7683752417794971 and parameters: {'lr_backbone': 1.59896870269066e-05, 'lr_classifier': 0.0003553107526483185, 'batch_size': 8, 'epochs': 13, 'max_grad_norm': 2.027937182498757, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.252973615198735, 'weight_decay': 0.0008570500681034111, 'adam_epsilon': 1.1067762943551042e-10, 'adam_beta1': 0.8824502834432306, 'adam_beta2': 0.9886947574333563, 'dropout': 0.3699323596768875, 'layerwise_decay': 0.95, 'label_smoothing': 0.15041277469644465, 'patience': 6}. Best is trial 0 with value: 0.7683752417794971.\n","[I 2026-01-10 20:06:36,759] Trial 1 finished with value: 0.6387814313346228 and parameters: {'lr_backbone': 4.264654675486179e-06, 'lr_classifier': 0.0006048748329429579, 'batch_size': 16, 'epochs': 15, 'max_grad_norm': 0.14734323172442587, 'gradient_accumulation_steps': 4, 'warmup_ratio': 0.24029469606169113, 'weight_decay': 0.07066918503464352, 'adam_epsilon': 1.6993393288134197e-10, 'adam_beta1': 0.8983065931490408, 'adam_beta2': 0.9829891396423313, 'dropout': 0.23839371051861064, 'layerwise_decay': 0.98, 'label_smoothing': 0.063758522226171, 'patience': 4}. Best is trial 0 with value: 0.7683752417794971.\n","[I 2026-01-10 20:08:32,379] Trial 2 finished with value: 0.5 and parameters: {'lr_backbone': 2.0761883224724402e-05, 'lr_classifier': 9.87187413819447e-05, 'batch_size': 2, 'epochs': 5, 'max_grad_norm': 1.5121741868302419, 'gradient_accumulation_steps': 8, 'warmup_ratio': 0.3021247280235801, 'weight_decay': 0.003987515410370118, 'adam_epsilon': 1.1674196567104158e-10, 'adam_beta1': 0.9186371474798035, 'adam_beta2': 0.9881824849224757, 'dropout': 0.4941675265565813, 'layerwise_decay': 1.0, 'label_smoothing': 0.09168989906313946, 'patience': 10}. Best is trial 0 with value: 0.7683752417794971.\n"]},{"name":"stdout","output_type":"stream","text":["Saved: martin-ha/toxic-comment-model tuning complete\n","Best accuracy: 0.76838\n","Best params: {'lr_backbone': 1.59896870269066e-05, 'lr_classifier': 0.0003553107526483185, 'batch_size': 8, 'epochs': 13, 'max_grad_norm': 2.027937182498757, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.252973615198735, 'weight_decay': 0.0008570500681034111, 'adam_epsilon': 1.1067762943551042e-10, 'adam_beta1': 0.8824502834432306, 'adam_beta2': 0.9886947574333563, 'dropout': 0.3699323596768875, 'layerwise_decay': 0.95, 'label_smoothing': 0.15041277469644465, 'patience': 6}\n"]},{"name":"stderr","output_type":"stream","text":["[I 2026-01-10 20:10:57,883] A new study created in memory with name: no-name-41a8ccb5-566d-4b63-a4f3-be20b1df3e99\n"]},{"name":"stdout","output_type":"stream","text":["Started new study for google/electra-base-discriminator\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","[I 2026-01-10 20:14:04,577] Trial 0 finished with value: 0.90715667311412 and parameters: {'lr_backbone': 1.3061038867920613e-05, 'lr_classifier': 2.410174274243116e-05, 'batch_size': 8, 'epochs': 7, 'max_grad_norm': 1.2528908938212762, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.05018311190961994, 'weight_decay': 0.04722807273266896, 'adam_epsilon': 8.337249440375688e-10, 'adam_beta1': 0.911707652040623, 'adam_beta2': 0.9987009538095121, 'dropout': 0.21316024816287132, 'layerwise_decay': 0.98, 'label_smoothing': 0.039982154988141956, 'patience': 2}. Best is trial 0 with value: 0.90715667311412.\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 20:17:39,766] Trial 1 finished with value: 0.7596711798839458 and parameters: {'lr_backbone': 1.0820048406741603e-06, 'lr_classifier': 2.7921668499076363e-05, 'batch_size': 8, 'epochs': 11, 'max_grad_norm': 2.9129245869614273, 'gradient_accumulation_steps': 8, 'warmup_ratio': 0.36939084468764705, 'weight_decay': 1.6254501620831956e-06, 'adam_epsilon': 2.995482229852248e-06, 'adam_beta1': 0.9407771802305156, 'adam_beta2': 0.984745483807251, 'dropout': 0.19390636576802156, 'layerwise_decay': 0.95, 'label_smoothing': 0.17109248067624447, 'patience': 3}. Best is trial 0 with value: 0.90715667311412.\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 20:20:46,914] Trial 2 finished with value: 0.9037717601547389 and parameters: {'lr_backbone': 7.868321134500167e-05, 'lr_classifier': 0.000105072430790657, 'batch_size': 8, 'epochs': 18, 'max_grad_norm': 4.69373329874764, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.021082384191990766, 'weight_decay': 0.00377186000925477, 'adam_epsilon': 2.8288824195836174e-10, 'adam_beta1': 0.8038773922665343, 'adam_beta2': 0.9859246048147302, 'dropout': 0.2292125963272898, 'layerwise_decay': 0.95, 'label_smoothing': 0.2676252893990425, 'patience': 2}. Best is trial 0 with value: 0.90715667311412.\n"]},{"name":"stdout","output_type":"stream","text":["Saved: google/electra-base-discriminator tuning complete\n","Best accuracy: 0.90716\n","Best params: {'lr_backbone': 1.3061038867920613e-05, 'lr_classifier': 2.410174274243116e-05, 'batch_size': 8, 'epochs': 7, 'max_grad_norm': 1.2528908938212762, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.05018311190961994, 'weight_decay': 0.04722807273266896, 'adam_epsilon': 8.337249440375688e-10, 'adam_beta1': 0.911707652040623, 'adam_beta2': 0.9987009538095121, 'dropout': 0.21316024816287132, 'layerwise_decay': 0.98, 'label_smoothing': 0.039982154988141956, 'patience': 2}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 20:24:29,946] A new study created in memory with name: no-name-24278e4c-2f9a-4c00-ac6e-5058b9c83403\n"]},{"name":"stdout","output_type":"stream","text":["Started new study for distilbert/distilbert-base-uncased\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","[I 2026-01-10 20:30:09,549] Trial 0 finished with value: 0.8655705996131529 and parameters: {'lr_backbone': 1.718154329847068e-06, 'lr_classifier': 4.8829238726036785e-05, 'batch_size': 4, 'epochs': 14, 'max_grad_norm': 2.8883196377787046, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.11692275817827491, 'weight_decay': 3.0444509945486562e-05, 'adam_epsilon': 7.257845758947511e-06, 'adam_beta1': 0.9408698574526806, 'adam_beta2': 0.9945190381217115, 'dropout': 0.1671143506359074, 'layerwise_decay': 0.95, 'label_smoothing': 0.23373391428607032, 'patience': 10}. Best is trial 0 with value: 0.8655705996131529.\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 20:31:24,727] Trial 1 finished with value: 0.8931334622823984 and parameters: {'lr_backbone': 3.951470242347571e-05, 'lr_classifier': 0.0006295043533964989, 'batch_size': 16, 'epochs': 8, 'max_grad_norm': 1.2087031969143767, 'gradient_accumulation_steps': 4, 'warmup_ratio': 0.17692978563624223, 'weight_decay': 0.0003824305986479276, 'adam_epsilon': 5.64923071355306e-08, 'adam_beta1': 0.8839395285597531, 'adam_beta2': 0.9808558820874684, 'dropout': 0.1841324994752428, 'layerwise_decay': 0.95, 'label_smoothing': 0.23047962141740042, 'patience': 8}. Best is trial 1 with value: 0.8931334622823984.\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 20:34:13,208] Trial 2 finished with value: 0.8191489361702128 and parameters: {'lr_backbone': 1.13247687097923e-06, 'lr_classifier': 0.0002718045842746991, 'batch_size': 4, 'epochs': 10, 'max_grad_norm': 4.0369777219738125, 'gradient_accumulation_steps': 4, 'warmup_ratio': 0.4044870442916494, 'weight_decay': 0.017257086406983084, 'adam_epsilon': 8.928527529178495e-10, 'adam_beta1': 0.9015822892170626, 'adam_beta2': 0.9887032513031365, 'dropout': 0.29856464799277627, 'layerwise_decay': 0.85, 'label_smoothing': 0.17652526420194686, 'patience': 8}. Best is trial 1 with value: 0.8931334622823984.\n"]},{"name":"stdout","output_type":"stream","text":["Saved: distilbert/distilbert-base-uncased tuning complete\n","Best accuracy: 0.89313\n","Best params: {'lr_backbone': 3.951470242347571e-05, 'lr_classifier': 0.0006295043533964989, 'batch_size': 16, 'epochs': 8, 'max_grad_norm': 1.2087031969143767, 'gradient_accumulation_steps': 4, 'warmup_ratio': 0.17692978563624223, 'weight_decay': 0.0003824305986479276, 'adam_epsilon': 5.64923071355306e-08, 'adam_beta1': 0.8839395285597531, 'adam_beta2': 0.9808558820874684, 'dropout': 0.1841324994752428, 'layerwise_decay': 0.95, 'label_smoothing': 0.23047962141740042, 'patience': 8}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 20:35:35,741] A new study created in memory with name: no-name-c44fe1f3-c814-4c2e-bd24-8fbc25e9bf9a\n"]},{"name":"stdout","output_type":"stream","text":["Started new study for FacebookAI/roberta-base\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","[I 2026-01-10 20:56:48,069] Trial 0 finished with value: 0.9081237911025145 and parameters: {'lr_backbone': 4.503597126597298e-06, 'lr_classifier': 0.00020998816014500615, 'batch_size': 2, 'epochs': 23, 'max_grad_norm': 2.6791006320124806, 'gradient_accumulation_steps': 2, 'warmup_ratio': 0.23369126450399175, 'weight_decay': 5.908659888525604e-06, 'adam_epsilon': 5.630417961345653e-10, 'adam_beta1': 0.824348995442171, 'adam_beta2': 0.982378341395409, 'dropout': 0.28128667778535066, 'layerwise_decay': 0.85, 'label_smoothing': 0.09815115485486645, 'patience': 7}. Best is trial 0 with value: 0.9081237911025145.\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 21:06:55,473] Trial 1 finished with value: 0.9206963249516441 and parameters: {'lr_backbone': 8.901975708567137e-05, 'lr_classifier': 0.00025789896217847557, 'batch_size': 16, 'epochs': 30, 'max_grad_norm': 3.3327717757057753, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.11060143650747228, 'weight_decay': 0.000146898092643942, 'adam_epsilon': 2.437829476675409e-10, 'adam_beta1': 0.8864108530900816, 'adam_beta2': 0.9802683800470975, 'dropout': 0.46433951303496224, 'layerwise_decay': 0.98, 'label_smoothing': 0.28239493723898756, 'patience': 10}. Best is trial 1 with value: 0.9206963249516441.\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 21:18:45,919] Trial 2 finished with value: 0.9115087040618955 and parameters: {'lr_backbone': 6.772320160568982e-06, 'lr_classifier': 0.00010992107256194691, 'batch_size': 2, 'epochs': 12, 'max_grad_norm': 2.0549581189209953, 'gradient_accumulation_steps': 2, 'warmup_ratio': 0.47176876625245334, 'weight_decay': 0.004399603717256812, 'adam_epsilon': 6.6597887890664295e-06, 'adam_beta1': 0.9436743779630061, 'adam_beta2': 0.9832427439037446, 'dropout': 0.06704248644718919, 'layerwise_decay': 1.0, 'label_smoothing': 0.01496458438057534, 'patience': 3}. Best is trial 1 with value: 0.9206963249516441.\n"]},{"name":"stdout","output_type":"stream","text":["Saved: FacebookAI/roberta-base tuning complete\n","Best accuracy: 0.92070\n","Best params: {'lr_backbone': 8.901975708567137e-05, 'lr_classifier': 0.00025789896217847557, 'batch_size': 16, 'epochs': 30, 'max_grad_norm': 3.3327717757057753, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.11060143650747228, 'weight_decay': 0.000146898092643942, 'adam_epsilon': 2.437829476675409e-10, 'adam_beta1': 0.8864108530900816, 'adam_beta2': 0.9802683800470975, 'dropout': 0.46433951303496224, 'layerwise_decay': 0.98, 'label_smoothing': 0.28239493723898756, 'patience': 10}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 21:28:21,135] A new study created in memory with name: no-name-b348efb8-136e-45ae-9733-0cbaec3f4aea\n"]},{"name":"stdout","output_type":"stream","text":["Started new study for microsoft/deberta-v3-base\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","[I 2026-01-10 21:38:17,379] Trial 0 finished with value: 0.8733075435203095 and parameters: {'lr_backbone': 3.959976506319883e-06, 'lr_classifier': 0.00034659571605606085, 'batch_size': 8, 'epochs': 13, 'max_grad_norm': 0.1555932167036509, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.44345135711477085, 'weight_decay': 0.003594511596372888, 'adam_epsilon': 1.7458498738842842e-10, 'adam_beta1': 0.8050535470882076, 'adam_beta2': 0.9992692272632591, 'dropout': 0.39935563816697794, 'layerwise_decay': 0.85, 'label_smoothing': 0.11003261790848885, 'patience': 5}. Best is trial 0 with value: 0.8733075435203095.\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 21:49:54,794] Trial 1 finished with value: 0.8771760154738878 and parameters: {'lr_backbone': 1.0917768427382281e-05, 'lr_classifier': 2.2848477363553595e-05, 'batch_size': 8, 'epochs': 20, 'max_grad_norm': 4.444830723642736, 'gradient_accumulation_steps': 8, 'warmup_ratio': 0.004335758500113307, 'weight_decay': 0.08439088300132051, 'adam_epsilon': 2.2359627460340451e-07, 'adam_beta1': 0.9458224791749428, 'adam_beta2': 0.9953379791797905, 'dropout': 0.033516985577914216, 'layerwise_decay': 0.85, 'label_smoothing': 0.17374719473309874, 'patience': 10}. Best is trial 1 with value: 0.8771760154738878.\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[I 2026-01-10 21:58:56,431] Trial 2 finished with value: 0.8249516441005803 and parameters: {'lr_backbone': 1.9285317601144543e-06, 'lr_classifier': 4.280833124952065e-05, 'batch_size': 2, 'epochs': 6, 'max_grad_norm': 4.620299891220947, 'gradient_accumulation_steps': 4, 'warmup_ratio': 0.42551532180718954, 'weight_decay': 8.720926888391582e-05, 'adam_epsilon': 3.869636269478821e-06, 'adam_beta1': 0.8261434572326766, 'adam_beta2': 0.9847681398886902, 'dropout': 0.12314204506572435, 'layerwise_decay': 0.95, 'label_smoothing': 0.1114285019117346, 'patience': 2}. Best is trial 1 with value: 0.8771760154738878.\n"]},{"name":"stdout","output_type":"stream","text":["Saved: microsoft/deberta-v3-base tuning complete\n","Best accuracy: 0.87718\n","Best params: {'lr_backbone': 1.0917768427382281e-05, 'lr_classifier': 2.2848477363553595e-05, 'batch_size': 8, 'epochs': 20, 'max_grad_norm': 4.444830723642736, 'gradient_accumulation_steps': 8, 'warmup_ratio': 0.004335758500113307, 'weight_decay': 0.08439088300132051, 'adam_epsilon': 2.2359627460340451e-07, 'adam_beta1': 0.9458224791749428, 'adam_beta2': 0.9953379791797905, 'dropout': 0.033516985577914216, 'layerwise_decay': 0.85, 'label_smoothing': 0.17374719473309874, 'patience': 10}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"execution_count":3},{"cell_type":"markdown","source":"# Final Training and Submission Creation","metadata":{}},{"cell_type":"markdown","source":"## Final Ensemble Training with Best Parameters\n\nOnce Optuna finds the best model settings, each model is re-trained one last time on the entire balanced dataset.\n\n- Mixed Precision (AMP): Uses 16-bit math instead of 32-bit to speed up training by 1.5x to 2x without losing accuracy.\n- Cosine Scheduling: Gradually lowers the learning rate in a wave-like pattern to help the model \"settle\" into the best possible logic.\n\nThe five models' predictions are averaged using Soft-Voting.\n\n- The Logic: If Model A is 90 percent sure a post is `EXTREMIST` and Model B is only 51 percent sure, the average (70.5 percent) is more reliable than just looking at one.\n- Final Threshold: Any average probability over 0.5 is labeled EXTREMIST.","metadata":{}},{"cell_type":"code","source":"# 5-MODEL LIST \nMODELS = [\n    \"FacebookAI/roberta-base\",\n    \"microsoft/deberta-v3-base\",\n    \"google/electra-base-discriminator\",\n    \"distilbert/distilbert-base-uncased\",\n    \"martin-ha/toxic-comment-model\"\n]\n\n# Containers for predictions\nval_predictions = {}  # For held-out validation\ntest_predictions = {} # For submission\n\nprint(\"Starting final training with best Optuna hyperparameters...\\n\")\n\nfor model_name in MODELS:\n    model_key = model_name.split(\"/\")[-1]\n    study_path = f\"{SAVE_DIR}/{model_key}_study.pkl\"\n\n    if not os.path.exists(study_path):\n        raise FileNotFoundError(f\"Study not found: {study_path}. Run tuning first!\")\n\n    # Load best params\n    study = joblib.load(study_path)\n    best_params = study.best_params\n    print(f\"Loaded best params for {model_name}\")\n    print(f\"  - Best CV accuracy: {study.best_value:.5f}\")\n    print(f\"  - Params: {best_params}\\n\")\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # Datasets\n    train_loader = DataLoader(\n        ExtremismDataset(train_balanced, tokenizer),\n        batch_size=best_params['batch_size'],\n        shuffle=True,\n        collate_fn=data_collator\n    )\n    val_loader = DataLoader(\n        ExtremismDataset(val_data, tokenizer),\n        batch_size=best_params.get('batch_size', 16),\n        shuffle=False,\n        collate_fn=data_collator\n    )\n    test_loader = DataLoader(\n        ExtremismDataset(test_df, tokenizer),\n        batch_size=best_params.get('batch_size', 16),\n        shuffle=False,\n        collate_fn=data_collator\n    )\n\n    # Model\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n    \n    # Apply dropout\n    if hasattr(model.config, 'hidden_dropout_prob'):\n        model.config.hidden_dropout_prob = best_params['dropout']\n    if hasattr(model.config, 'attention_probs_dropout_prob'):\n        model.config.attention_probs_dropout_prob = best_params['dropout']\n    if hasattr(model.config, 'classifier_dropout'):\n        model.config.classifier_dropout = best_params['dropout']\n\n    model.to(DEVICE)\n\n    # Optimizer with layerwise LR if needed\n    if best_params['layerwise_decay'] < 1.0:\n        param_groups = []\n        layer_idx = 0\n        num_layers = 12  # Adjust if needed\n        for name, param in model.named_parameters():\n            if 'classifier' in name or 'pooler' in name:\n                lr = best_params['lr_classifier']\n            else:\n                decay_factor = best_params['layerwise_decay'] ** (num_layers - layer_idx)\n                lr = best_params['lr_backbone'] * decay_factor\n                if 'layer' in name and f\"layer.{layer_idx}\" in name:\n                    layer_idx += 1\n            param_groups.append({'params': param, 'lr': lr})\n        optimizer = AdamW(\n            param_groups,\n            weight_decay=best_params['weight_decay'],\n            eps=best_params['adam_epsilon'],\n            betas=(best_params['adam_beta1'], best_params['adam_beta2'])\n        )\n    else:\n        optimizer = AdamW([\n            {'params': model.base_model.parameters() if hasattr(model, 'base_model') else model.parameters(),\n             'lr': best_params['lr_backbone']},\n            {'params': model.classifier.parameters(), 'lr': best_params['lr_classifier']}\n        ], weight_decay=best_params['weight_decay'])\n\n    # Scheduler\n    total_steps = len(train_loader) * best_params['epochs'] // best_params['gradient_accumulation_steps']\n    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n    scaler = GradScaler('cuda')\n\n    # Training loop\n    model.train()\n    accumulated = 0\n    for epoch in range(best_params['epochs']):\n        for batch in train_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            with autocast('cuda'):\n                outputs = model(**batch)\n                loss = outputs.loss\n            scaler.scale(loss / best_params['gradient_accumulation_steps']).backward()\n            accumulated += 1\n            if accumulated % best_params['gradient_accumulation_steps'] == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), best_params['max_grad_norm'])\n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad()\n                accumulated = 0\n\n    # Predictions\n    model.eval()\n\n    # Val predictions\n    val_preds = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            with autocast('cuda'):\n                logits = model(**batch).logits\n            val_preds.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n    val_predictions[model_name] = np.array(val_preds)\n\n    # Test predictions\n    test_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            with autocast('cuda'):\n                logits = model(**batch).logits\n            test_preds.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy())\n    test_predictions[model_name] = np.array(test_preds)\n\n    print(f\"{model_name} - Final training complete\\n\")\n\n# FINAL ENSEMBLE \n# Average ensemble on held-out val\nfinal_val_prob = np.mean(list(val_predictions.values()), axis=0)\nval_acc = accuracy_score(val_data['label'], (final_val_prob > 0.5).astype(int))\nprint(f\"\\nFINAL 5-MODEL ENSEMBLE HELD-OUT ACCURACY: {val_acc:.5f}\\n\")","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting final training with best Optuna hyperparameters...\n","\n","Loaded best params for FacebookAI/roberta-base\n","  - Best CV accuracy: 0.92070\n","  - Params: {'lr_backbone': 8.901975708567137e-05, 'lr_classifier': 0.00025789896217847557, 'batch_size': 16, 'epochs': 30, 'max_grad_norm': 3.3327717757057753, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.11060143650747228, 'weight_decay': 0.000146898092643942, 'adam_epsilon': 2.437829476675409e-10, 'adam_beta1': 0.8864108530900816, 'adam_beta2': 0.9802683800470975, 'dropout': 0.46433951303496224, 'layerwise_decay': 0.98, 'label_smoothing': 0.28239493723898756, 'patience': 10}\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["FacebookAI/roberta-base - Final training complete\n","\n","Loaded best params for microsoft/deberta-v3-base\n","  - Best CV accuracy: 0.87718\n","  - Params: {'lr_backbone': 1.0917768427382281e-05, 'lr_classifier': 2.2848477363553595e-05, 'batch_size': 8, 'epochs': 20, 'max_grad_norm': 4.444830723642736, 'gradient_accumulation_steps': 8, 'warmup_ratio': 0.004335758500113307, 'weight_decay': 0.08439088300132051, 'adam_epsilon': 2.2359627460340451e-07, 'adam_beta1': 0.9458224791749428, 'adam_beta2': 0.9953379791797905, 'dropout': 0.033516985577914216, 'layerwise_decay': 0.85, 'label_smoothing': 0.17374719473309874, 'patience': 10}\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["microsoft/deberta-v3-base - Final training complete\n","\n","Loaded best params for google/electra-base-discriminator\n","  - Best CV accuracy: 0.90716\n","  - Params: {'lr_backbone': 1.3061038867920613e-05, 'lr_classifier': 2.410174274243116e-05, 'batch_size': 8, 'epochs': 7, 'max_grad_norm': 1.2528908938212762, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.05018311190961994, 'weight_decay': 0.04722807273266896, 'adam_epsilon': 8.337249440375688e-10, 'adam_beta1': 0.911707652040623, 'adam_beta2': 0.9987009538095121, 'dropout': 0.21316024816287132, 'layerwise_decay': 0.98, 'label_smoothing': 0.039982154988141956, 'patience': 2}\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["google/electra-base-discriminator - Final training complete\n","\n","Loaded best params for distilbert/distilbert-base-uncased\n","  - Best CV accuracy: 0.89313\n","  - Params: {'lr_backbone': 3.951470242347571e-05, 'lr_classifier': 0.0006295043533964989, 'batch_size': 16, 'epochs': 8, 'max_grad_norm': 1.2087031969143767, 'gradient_accumulation_steps': 4, 'warmup_ratio': 0.17692978563624223, 'weight_decay': 0.0003824305986479276, 'adam_epsilon': 5.64923071355306e-08, 'adam_beta1': 0.8839395285597531, 'adam_beta2': 0.9808558820874684, 'dropout': 0.1841324994752428, 'layerwise_decay': 0.95, 'label_smoothing': 0.23047962141740042, 'patience': 8}\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["distilbert/distilbert-base-uncased - Final training complete\n","\n","Loaded best params for martin-ha/toxic-comment-model\n","  - Best CV accuracy: 0.76838\n","  - Params: {'lr_backbone': 1.59896870269066e-05, 'lr_classifier': 0.0003553107526483185, 'batch_size': 8, 'epochs': 13, 'max_grad_norm': 2.027937182498757, 'gradient_accumulation_steps': 1, 'warmup_ratio': 0.252973615198735, 'weight_decay': 0.0008570500681034111, 'adam_epsilon': 1.1067762943551042e-10, 'adam_beta1': 0.8824502834432306, 'adam_beta2': 0.9886947574333563, 'dropout': 0.3699323596768875, 'layerwise_decay': 0.95, 'label_smoothing': 0.15041277469644465, 'patience': 6}\n","\n"]},{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["martin-ha/toxic-comment-model - Final training complete\n","\n","\n","FINAL 5-MODEL ENSEMBLE HELD-OUT ACCURACY: 0.81778\n","\n"]}],"execution_count":6},{"cell_type":"markdown","source":"## Submission Creation","metadata":{}},{"cell_type":"code","source":"# Average ensemble on test\nfinal_test_prob = np.mean(list(test_predictions.values()), axis=0)\n\nsubmission = pd.DataFrame({\n    'ID': test_ids,\n    'Extremism_Label': ['EXTREMIST' if p > 0.5 else 'NON_EXTREMIST' for p in final_test_prob]\n})\nsubmission.to_csv('submission_old_pre.csv', index=False)\nprint(\"SUBMISSION SAVED\")\nprint(f\"Predictions shape: {final_test_prob.shape} | Test IDs: {len(test_ids)}\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["SUBMISSION SAVED\n","Predictions shape: (750,) | Test IDs: 750\n"]}],"execution_count":7},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"This notebook presents a complete, high-accuracy solution for the Social Media Extremism Detection Challenge. By combining careful data balancing, automated hyperparameter optimization via Optuna, and a robust 5-model ensemble, the pipeline achieves a superior held-out accuracy of approx. 81.8%.\n\nThis work contributes to the broader goal of creating safer digital environments by providing a reproducible, state-of-the-art framework for identifying harmful content. The final submission is optimized for both accuracy and robustness, making it a competitive entry for the final leaderboard.","metadata":{}},{"cell_type":"markdown","source":"# References","metadata":{}},{"cell_type":"markdown","source":"- [NLPAUG](https://nlpaug.readthedocs.io/en/latest/overview/overview.html)\n- [NLPAUG – A Python library to Augment Your Text Data](https://www.analyticsvidhya.com/blog/2021/08/nlpaug-a-python-library-to-augment-your-text-data/)\n- [Tokenization HuggingFace](https://huggingface.co/learn/llm-course/en/chapter2/4)\n- [Toward a Theory of Tokenization in LLMs](https://arxiv.org/abs/2404.08335)\n- [All you need to know about Tokenization in LLMs](https://medium.com/thedeephub/all-you-need-to-know-about-tokenization-in-llms-7a801302cf54)\n- [FacebookAI/roberta-base](https://huggingface.co/FacebookAI/roberta-base)\n- [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base)\n- [google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator)\n- [distilbert/distilbert-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased)\n- [martin-ha/toxic-comment-model](https://huggingface.co/martin-ha/toxic-comment-model)\n- [StratifiedKFold sk-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n- [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n- [Stratified K Fold Cross Validation](https://www.geeksforgeeks.org/machine-learning/stratified-k-fold-cross-validation/)\n- [VotingClassifier sk-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)\n- [Voting Classifier Greeksforgreeks](https://www.geeksforgeeks.org/machine-learning/voting-classifier/)","metadata":{}}]}