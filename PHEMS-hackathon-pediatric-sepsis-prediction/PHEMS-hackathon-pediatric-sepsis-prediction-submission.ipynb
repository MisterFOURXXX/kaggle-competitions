{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87997,"databundleVersionId":10854531,"sourceType":"competition"},{"sourceId":251066,"sourceType":"modelInstanceVersion","modelInstanceId":214622,"modelId":236303},{"sourceId":251067,"sourceType":"modelInstanceVersion","modelInstanceId":214623,"modelId":236304},{"sourceId":251069,"sourceType":"modelInstanceVersion","modelInstanceId":214625,"modelId":236307},{"sourceId":251073,"sourceType":"modelInstanceVersion","modelInstanceId":214629,"modelId":236311},{"sourceId":251089,"sourceType":"modelInstanceVersion","modelInstanceId":214645,"modelId":236328},{"sourceId":251124,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":214677,"modelId":236361}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition overview","metadata":{}},{"cell_type":"markdown","source":"**Competition Summary**\n\nThis is the PHEMS Hackathon: Pediatric Sepsis Prediction, a machine learning competition focused on developing an algorithm for the early detection of sepsis in pediatric intensive care unit (PICU) patients. Participants are tasked with creating a model that can predict the onset of sepsis up to 6 hours before a clinical diagnosis, using retrospective physiological and clinical time-series data.\n\nEarly detection of sepsis is a critical, life-saving intervention. The competition's goal is to create a data-driven solution that can enhance current clinical protocols and improve patient outcomes.\n\nThe primary evaluation metric is precision-recall AUC, which is particularly important for imbalanced medical datasets. Secondary metrics like accuracy and F1-score will also be considered. In case of a tie, model interpretability, execution time, and computational resource usage will serve as tiebreakers.\n\nThe competition runs from January 13, 2025, to February 5, 2025, and offers a total prize pool of 2,500€ for the top three winning teams.","metadata":{}},{"cell_type":"markdown","source":"### [Link to competition](https://www.kaggle.com/competitions/phems-hackathon-early-sepsis-prediction/data?select=SepsisLabel_sample_submission.csv)","metadata":{}},{"cell_type":"markdown","source":"# Dataset Overview","metadata":{}},{"cell_type":"markdown","source":"The dataset for the PHEMS Hackathon is a retrospective collection of pediatric patient data from the Pediatric Intensive Care Unit (PICU) of Hospital Sant Joan de Déu. The data is provided in a time-series format, with each row representing a measurement or event at a specific timestamp during a patient's stay. The dataset is split into training, testing, and a hidden private test set.\n\nThe data is organized into several standard tables, each providing a different category of patient information. The core objective is to use these predictor datasets to forecast the SepsisLabel for each patient at every time point in the test set. Importantly, all positive SepsisLabels in the training data have been adjusted to appear 6 hours earlier than the true onset of sepsis, aligning with the competition's goal of early prediction.","metadata":{}},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"markdown","source":"The following is a data dictionary for the provided files:","metadata":{}},{"cell_type":"markdown","source":"**`SepsisLabel_(train|test).csv`**\n\nThis file contains the target variable for the competition.","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">person_id</td>\n    <td class=\"tg-0lax\">A unique identifier for each patient.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">measurement_datetime</td>\n    <td class=\"tg-0lax\">The specific timestamp of the measurement.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">SepsisLabel</td>\n    <td class=\"tg-0lax\">The binary outcome variable: 1 for a positive sepsis assessment, 0 for a negative assessment.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"**`devices.csv`**\n\nThis table records the usage of medical devices.","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">visit_occurrence_id</td>\n    <td class=\"tg-0lax\">A unique ID for a specific PICU episode.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">device_datetime_hourly</td>\n    <td class=\"tg-0lax\">The timestamp (hourly granularity) when the device was used.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">person_id</td>\n    <td class=\"tg-0lax\">A unique patient identifier.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">device</td>\n    <td class=\"tg-0lax\">The type of medical device used.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"**`drugexposure.csv`**\n\nThis table contains data on drug administration.","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">visit_occurrence_id</td>\n    <td class=\"tg-0lax\">A unique ID for a specific PICU episode.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">person_id</td>\n    <td class=\"tg-0lax\">A unique patient identifier.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">drug_datetime_hourly</td>\n    <td class=\"tg-0lax\">The timestamp (hourly granularity) of drug administration.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">drug_concept_id</td>\n    <td class=\"tg-0lax\">An identifier for the administered drug.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">quantity</td>\n    <td class=\"tg-0lax\">The amount of the drug administered.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">dose_unit_source_value</td>\n    <td class=\"tg-0lax\">The unit of measurement for the drug dosage.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">route_concept_id</td>\n    <td class=\"tg-0lax\">The route of drug administration (e.g., intravenous).</td>\n  </tr>\n</tbody></table>","metadata":{}},{"cell_type":"markdown","source":"**`observation.csv`**\n\nThis table contains general clinical observations.","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">visit_occurrence_id</td>\n    <td class=\"tg-0lax\">A unique ID for a specific PICU episode.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">person_id</td>\n    <td class=\"tg-0lax\">A unique patient identifier.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">observation_datetime</td>\n    <td class=\"tg-0lax\">The timestamp when the observation was made.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">observation_concept_id</td>\n    <td class=\"tg-0lax\">A label describing the type of observation.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">valuefilled</td>\n    <td class=\"tg-0lax\">The value or description of the observation.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"**`person_demographics_episode.csv`**\n\nThis table contains patient demographic information.","metadata":{}},{"cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">visit_occurrence_id</td>\n    <td class=\"tg-0lax\">A unique ID for a specific PICU episode.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">person_id</td>\n    <td class=\"tg-0lax\">A unique patient identifier.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">age_in_months</td>\n    <td class=\"tg-0lax\">The patient's age in months.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">gender</td>\n    <td class=\"tg-0lax\">The patient's gender.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"cell_type":"markdown","source":"**`measurement_lab.csv, measurement_meds.csv, measurement_observation.csv`**\n\nThese files contain various lab measurements, medication administrations, and clinical observations. Specific column names are not provided in the description, but they would typically include `person_id, measurement_datetime`, and a series of columns for different lab tests, vital signs, or observations.","metadata":{}},{"cell_type":"markdown","source":"### [Link to dataset](https://www.kaggle.com/competitions/phems-hackathon-early-sepsis-prediction/data?select=SepsisLabel_sample_submission.csv)","metadata":{}},{"cell_type":"markdown","source":"# Submission Notebook Pipeline Overview","metadata":{}},{"cell_type":"markdown","source":"The script executes a sequential workflow to prepare the test data and generate predictions for a machine learning competition.\n\n- **Data Loading and Feature Engineering:** The process starts by loading multiple raw data files for the test set, including patient vitals, lab results, and medication data. Using pandas, the code merges these disparate datasets into a single, comprehensive dataframe. During this stage, it performs feature engineering, creating new features like patient stay duration (duration) and handling missing values by filling them with the mean or mode.\n\n- **Data Transformation:** This is a critical step where the test data is transformed to match the format of the training data used for the pre-trained model.\n\n  - Text to Embedding: It loads a pre-trained Word2Vec model to convert text-based features (e.g., device, procedure) into dense numerical vectors. This allows the model to understand the semantic meaning of these medical terms.\n  - Standardization: A pre-trained StandardScaler is loaded to normalize the numerical features. This is a crucial step for many machine learning models, as it ensures all features contribute equally during prediction.\n  - Dimensionality Reduction: A pre-trained PCA (Principal Component Analysis) model is loaded to reduce the number of features. This helps mitigate the curse of dimensionality and improves computational efficiency without losing critical information.\n  - Time Series Conversion: The create_time_series_dataset function reshapes the data into a 3D time-series format (`samples, time_steps, features`), which is the required input shape for the deep learning model. It does this by creating sequences of past data points for each sample.\n\n- **Prediction and Submission:** The script loads a pre-trained TensorFlow model from a saved file. The prepared time-series data is passed to this model to generate sepsis predictions as probabilities. These probabilities are then converted into binary labels (0 or 1) using a threshold of 0.5. Finally, the predictions are formatted into a submission file as required by the competition platform and saved as `submission.csv`.","metadata":{}},{"cell_type":"markdown","source":"# Review Model Architecture and Training Concepts","metadata":{}},{"cell_type":"markdown","source":"The model used in this notebook is a pre-trained deep learning model optimized for time-series classification. While the code doesn't define the model architecture, the file name (`optimized_time_series_modelV1.h5`) suggests it's a variant of a Temporal Convolutional Network (TCN) or an LSTM-based network, which are architectures designed to capture temporal patterns.\n\n**Core Concepts**\n\nTime Series Classification: The task is to predict a future event (sepsis) based on a sequence of past data points. The model needs to learn patterns and dependencies over time.\n\n**Feature Engineering:** \n\nThis process involves using domain knowledge to create new features from existing data. For this project, features like the duration of a patient's hospital stay and the average time a device was used are created, as they are likely strong indicators of a patient's condition.\n\n**Word Embeddings:** \n\nThis is a concept from Natural Language Processing (NLP) that is applied to the categorical text data. Word2Vec learns to represent words as numerical vectors, where words with similar meanings have similar vectors. By averaging these vectors for a given patient's history, the model can infer a patient's overall medical state.\n\n**Dimensionality Reduction:** \n\nPCA works by finding new, uncorrelated dimensions (called principal components) that capture the maximum variance in the data. This reduces the number of features while retaining most of the important information, which is essential when working with high-dimensional data like electronic health records.\n\n**Model Training:** \n\nThe pre-trained model was trained on a separate dataset (the training data) using a process that involved an optimizer (e.g., Adam) and a loss function (e.g., Binary Cross-Entropy) to minimize prediction error. Techniques like Early Stopping and Dropout were likely used to prevent overfitting, ensuring the model generalizes well to new data rather than just memorizing the training examples.","metadata":{}},{"cell_type":"markdown","source":"### [Model Training Notebook](https://www.kaggle.com/code/misterfour/phems-hackathon-model-training)","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nimport joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:10:50.155722Z","iopub.execute_input":"2025-09-02T22:10:50.156068Z","iopub.status.idle":"2025-09-02T22:11:48.084856Z","shell.execute_reply.started":"2025-09-02T22:10:50.156034Z","shell.execute_reply":"2025-09-02T22:11:48.083948Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/phems-hackathon-early-sepsis-prediction/SepsisLabel_sample_submission.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/drugsexposure_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/observation_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/devices_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/measurement_lab_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/proceduresoccurrences_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/person_demographics_episode_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/SepsisLabel_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/measurement_observation_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/training_data/measurement_meds_train.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/proceduresoccurrences_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/measurement_lab_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/observation_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/drugsexposure_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/measurement_observation_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/person_demographics_episode_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/devices_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/measurement_meds_test.csv\n/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv\n/kaggle/input/standard_scaler_v3/scikitlearn/default/1/standard_scaler.joblib\n/kaggle/input/text_to_embedding_v0/scikitlearn/default/1/text_to_embedding_transformer.pkl\n/kaggle/input/pca_model/scikitlearn/default/1/pca_model.joblib\n/kaggle/input/mild_model_v6/tensorflow2/default/1/optimized_time_series_modelV1.h5\n/kaggle/input/word2vec_v2/scikitlearn/default/1/word2vec.model\n/kaggle/input/tcn_model/scikitlearn/default/1/optimized_tcn_modelV3.h5\n","output_type":"stream"},{"name":"stderr","text":"2025-09-02 22:11:32.850786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756851093.143318      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756851093.228158      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"markdown","source":"This step is data preparation pipeline for a neural network model.It loads and merges various datasets related to patient records, medical devices, lab results, medications, and procedures. The core concept behind this step is some feature engineering, which involves using domain knowledge to create new features that can help a model better understand the data. This step transforms raw, fragmented data into a single, cohesive dataset.\n\n**Step-By-Step Breakdown**\n\n**1.Initial Data Loading and Device Information**\n\nThe pipeline begins by loading the main `SepsisLabel_test.csv` file, which contains core patient and timestamp information. It then loads `devices_test.csv` and merges it.\n\n- **Duration Calculation:** The code calculates the total time a person was monitored, in hours, by grouping data by `person_id` and finding the difference between the earliest and latest measurement timestamps.\n- **Device Aggregation:** It groups the data by `person_id` to create a list of all unique devices used for that person. This summarizes device information into a single feature for each patient.\n\n**2.Merging Lab and Medical Data**\n\nThis section loads and aggregates data from two more sources: `measurement_lab_test.csv` (for lab results) and `measurement_meds_test.csv` (for vital signs and medication data).\n\n- **Data Consolidation:** For both files, the code uses a `.groupby()` operation on `person_id` and `measurement_datetime`. It then applies `.sum()` to aggregate values for various lab tests (e.g., Bilirubin, Platelet count) and vital signs (e.g., Systolic blood pressure, Heart rate) at each measurement timestamp. This is a crucial step to combine multiple measurements taken at the same time into a single row.\n\n**3.Merging Observations and Procedures**\n\nThe pipeline continues to build the master dataset by merging data from two more files: `measurement_observation_test.csv` and `proceduresoccurrences_test.csv`.\n\n- **Observation Aggregation:** It follows the same pattern of grouping by `person_id` and `measurement_datetime` and then summing values for observations like Glasgow coma scale and pupil diameter.\n- **Procedure Aggregation:** It calculates the duration of a patient's procedures and aggregates the unique procedure names for each patient. This gives the model insight into the types of medical interventions a patient has undergone.\n\n**4.Merging Drug and General Observation Data**\n\nFinally, the pipeline incorporates information on drug exposure and general medical observations.\n\n- **Drug Exposure:** The `drugsexposure_test.csv` file is loaded. The code calculates the total duration of drug administration for each patient and aggregates unique drug concept IDs and routes. This provides a clear picture of the patient's pharmacological history.\n- **General Observations:** The `observation_test.csv` file is loaded and aggregated in the same manner, summarizing the duration of observations and listing unique observation concepts and values.\n\nThe result of this entire process is a single, broad DataFrame (`merged_test`) that contains all the relevant information from multiple sources, organized by `person_id` and `measurement_datetime`. This structured and enriched dataset is now ready to be used as input for a machine learning model to predict sepsis.","metadata":{}},{"cell_type":"code","source":"# Load test data\ntest_sepsislabel = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\ntest_sepsislabel['measurement_datetime'] = pd.to_datetime(test_sepsislabel['measurement_datetime'])\ntest_submission = test_sepsislabel.copy()\n\n# Convert 'measurement_datetime' to datetime\ntest_sepsislabel['measurement_datetime'] = pd.to_datetime(test_sepsislabel['measurement_datetime'])\n\n# Calculate duration for each person_id\ntest_sepsislabel['duration'] = test_sepsislabel.groupby('person_id')['measurement_datetime'].transform(lambda x: (x.max() - x.min()).total_seconds()/3600)\ntest_devices = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/devices_test.csv')\ntest_devices_grouped = test_devices.groupby('person_id')['device'].agg(lambda x: ', '.join(x.unique())).reset_index()\ntest_devices_grouped = pd.DataFrame(test_devices_grouped)\n\n# Drop specified columns\ntest_devices_hr = test_devices.copy()\ntest_devices_hr = test_devices_hr.drop(columns=['visit_occurrence_id', 'device'])\n\n# Convert 'device_datetime_hourly' to datetime objects\ntest_devices_hr['device_datetime_hourly'] = pd.to_datetime(test_devices_hr['device_datetime_hourly'])\n\n# Extract the hour from the datetime column\ntest_devices_hr['device_hour'] = test_devices_hr['device_datetime_hourly'].dt.hour\n\n# Group by 'person_id' and calculate the mean hour\nmean_hours = test_devices_hr.groupby('person_id')['device_hour'].mean().reset_index()\nmean_hours['device_mean_hour'] = mean_hours['device_hour']\n\n# Merge the mean hour back into the original DataFrame\ntest_devices_hr = pd.merge(test_devices_hr, mean_hours[['person_id', 'device_mean_hour']], on='person_id', how='left')\n\n# Remove duplicate 'person_id' rows (keeping the first occurrence)\ntest_devices_hr = test_devices_hr.drop_duplicates(subset='person_id')\ntest_devices_hr = test_devices_hr.drop(columns='device_datetime_hourly')\ntest_devices_merge = pd.merge(test_devices_grouped, test_devices_hr, on='person_id', how='left')\nmerged_test = pd.merge(test_sepsislabel, test_devices_merge, on='person_id', how='left')\n\n\ntest_lab=pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/measurement_lab_test.csv')\ntest_lab = test_lab.groupby(['person_id','measurement_datetime'])[['Base excess in Venous blood by calculation',\n       'Base excess in Arterial blood by calculation',\n       'Phosphate [Moles/volume] in Serum or Plasma',\n       'Potassium [Moles/volume] in Blood',\n       'Bilirubin.total [Moles/volume] in Serum or Plasma',\n       'Neutrophil Ab [Units/volume] in Serum',\n       'Bicarbonate [Moles/volume] in Arterial blood',\n       'Hematocrit [Volume Fraction] of Blood',\n       'Glucose [Moles/volume] in Serum or Plasma',\n       'Calcium [Moles/volume] in Serum or Plasma',\n       'Chloride [Moles/volume] in Blood',\n       'Sodium [Moles/volume] in Serum or Plasma',\n       'C reactive protein [Mass/volume] in Serum or Plasma',\n       'Carbon dioxide [Partial pressure] in Venous blood',\n       'Oxygen [Partial pressure] in Venous blood',\n       'Albumin [Mass/volume] in Serum or Plasma',\n       'Bicarbonate [Moles/volume] in Venous blood',\n       'Oxygen [Partial pressure] in Arterial blood',\n       'Carbon dioxide [Partial pressure] in Arterial blood',\n       'Interleukin 6 [Mass/volume] in Body fluid',\n       'Magnesium [Moles/volume] in Blood', 'Prothrombin time (PT)',\n       'Procalcitonin [Mass/volume] in Serum or Plasma',\n       'Lactate [Moles/volume] in Blood', 'Creatinine [Mass/volume] in Blood',\n       'Fibrinogen measurement', 'Bilirubin measurement',\n       'Partial thromboplastin time', ' activated', 'Total white blood count',\n       'Platelet count', 'White blood cell count', 'Blood venous pH',\n       'D-dimer level', 'Blood arterial pH',\n       'Hemoglobin [Moles/volume] in Blood', 'Ionised calcium measurement']].sum().reset_index()\n\n# Ensure 'measurement_datetime' is converted to datetime64[ns] in all DataFrames\ndef ensure_datetime(df, datetime_column):\n    df[datetime_column] = pd.to_datetime(df[datetime_column], errors='coerce')\n    return df\n\n# Convert 'measurement_datetime' to datetime in all relevant DataFrames\ntest_sepsislabel = ensure_datetime(test_sepsislabel, 'measurement_datetime')\ntest_lab = ensure_datetime(test_lab, 'measurement_datetime')\n\n# Merge the DataFrames\nmerged_test = pd.merge(merged_test, test_lab, on=['person_id', 'measurement_datetime'], how='left')\n\n\ntest_meds=pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/measurement_meds_test.csv')\ntest_meds = test_meds.groupby(['person_id','measurement_datetime'])[['Systolic blood pressure', 'Diastolic blood pressure',\n       'Body temperature', 'Respiratory rate', 'Heart rate',\n       'Measurement of oxygen saturation at periphery',\n       'Oxygen/Gas total [Pure volume fraction] Inhaled gas']].sum().reset_index()\n\n\n# Ensure 'measurement_datetime' is converted to datetime64[ns] in all DataFrames\ndef ensure_datetime(df, datetime_column):\n    df[datetime_column] = pd.to_datetime(df[datetime_column], errors='coerce')\n    return df\n\n# Convert 'measurement_datetime' to datetime in all relevant DataFrames\n\nmerged_test = ensure_datetime(merged_test, 'measurement_datetime')\ntest_meds = ensure_datetime(test_meds, 'measurement_datetime')\n\n# Merge the DataFrames\n\nmerged_test = pd.merge(merged_test, test_meds, on=['person_id', 'measurement_datetime'], how='left')\n\n\ntest_obs=pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/measurement_observation_test.csv')\n\ntest_obs = test_obs.groupby(['person_id','measurement_datetime'])[['Left pupil Diameter Auto', 'Right pupil Diameter Auto',\n       'Glasgow coma scale', 'Capillary refill [Time]', 'Pulse',\n       'Arterial pulse pressure', 'Right pupil Pupillary response',\n       'Left pupil Pupillary response']].sum().reset_index()\n\n\n# Ensure 'measurement_datetime' is converted to datetime64[ns] in all DataFrames\ndef ensure_datetime(df, datetime_column):\n    df[datetime_column] = pd.to_datetime(df[datetime_column], errors='coerce')\n    return df\n\nmerged_test = ensure_datetime(merged_test, 'measurement_datetime')\ntest_obs = ensure_datetime(test_obs, 'measurement_datetime')\nmerged_test = pd.merge(merged_test, test_obs, on=['person_id', 'measurement_datetime'], how='left')\n\n\ntest_procedure = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/proceduresoccurrences_test.csv')\ntest_procedure['procedure_datetime_hourly'] = pd.to_datetime(test_procedure['procedure_datetime_hourly'])\ntest_procedure['procedure_duration'] = test_procedure.groupby('person_id')['procedure_datetime_hourly'].transform(lambda x: (x.max() - x.min()).total_seconds() / 3600)\n\ntest_procedure_A = test_procedure[['person_id','procedure_duration']]\ntest_procedure_A = test_procedure_A.drop_duplicates(subset=['person_id'])\n\ntest_procedure_grouped = test_procedure.groupby('person_id')[['procedure']].agg(lambda x: ', '.join(x.unique())).reset_index()\nmerged_test = pd.merge(merged_test, test_procedure_grouped, on='person_id', how='left')\nmerged_test = pd.merge(merged_test, test_procedure_A, on='person_id', how='left')\nmerged_test = merged_test.sort_values(by='measurement_datetime') \n\n\ntest_drug=pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/drugsexposure_test.csv')\ntest_drug=test_drug[['person_id','drug_datetime_hourly','drug_concept_id','route_concept_id']]\ntest_drug['route_concept_id'] = test_drug['route_concept_id'].astype(str)\ntest_drug['drug_datetime_hourly'] = pd.to_datetime(test_drug['drug_datetime_hourly'])\ntest_drug['drug_duration'] = test_drug.groupby('person_id')['drug_datetime_hourly'].transform(lambda x: (x.max() - x.min()).total_seconds()/3600)\ntest_drug_A = test_drug[['person_id','drug_duration']]\ntest_drug_A = test_drug_A.drop_duplicates(subset=['person_id'])\ntest_drug_grouped = test_drug.groupby(['person_id'])[['drug_concept_id','route_concept_id']].agg(lambda x: ', '.join(x.unique())).reset_index()\ntest_drug_grouped = pd.DataFrame(test_drug_grouped)\nmerged_test = pd.merge(merged_test, test_drug_grouped, on='person_id', how='left')\nmerged_test = pd.merge(merged_test, test_drug_A, on='person_id', how='left')\n\n\n\ntest_observation=pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/observation_test.csv')\n# Convert 'drug_datetime_hourly' to datetime\ntest_observation['observation_datetime'] = pd.to_datetime(test_observation['observation_datetime'])\ntest_observation['observation_duration'] = test_observation.groupby('person_id')['observation_datetime'].transform(lambda x: (x.max() - x.min()).total_seconds()/3600)\ntest_observation_A = test_observation[['person_id','observation_duration']]\ntest_observation_A = test_observation_A.drop_duplicates(subset=['person_id'])\ntest_observation = test_observation.groupby('person_id')[['observation_concept_name','valuefilled']].agg(lambda x: ', '.join(x.unique())).reset_index()\nmerged_test = pd.merge(merged_test, test_observation_A, on='person_id', how='left')\nmerged_test = pd.merge(merged_test, test_observation, on='person_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:11:48.086294Z","iopub.execute_input":"2025-09-02T22:11:48.086949Z","iopub.status.idle":"2025-09-02T22:11:52.574937Z","shell.execute_reply.started":"2025-09-02T22:11:48.086923Z","shell.execute_reply":"2025-09-02T22:11:52.573873Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"This section of the code focuses on feature engineering and preparing the data for a machine learning model, specifically one that handles time series or sequential data. The goal of this step is to transform raw, messy test data into a clean, structured, and scaled format that a model can understand. This involves handling missing values, converting text data into numerical embeddings, scaling numerical features, and structuring the data into time-based sequences.","metadata":{}},{"cell_type":"markdown","source":"**Data Cleaning and Preparation**\n\nThis initial phase ensures the dataset is in a usable state.\n\n- **Sorting the Dataset:** The first action is to sort the DataFrame merged_test by the measurement_datetime column. For time series analysis, the order of observations is critical. By sorting the data chronologically, you ensure that the subsequent steps - like filling missing values and creating time-based sequences - are applied in the correct order. This prevents data leakage where information from the future is used to fill in a past value.\n- **Filling Missing Values (Imputation):** The code then fills in missing values (NaN) across different columns using various imputation techniques. Real-world datasets are rarely complete. Missing values can cause errors or poor model performance. Imputation is the process of replacing these missing values with a substituted value. The choice of method depends on the data type.\n\n**Step-By-Step Code Breakdown:**\n\n- **`mode()`:** Used for categorical data in extra_cols, which is a robust way to fill in gaps with the most frequently occurring value.\n- **`ffill() and bfill()`:** Used for `categorical_cols. ffill()` (forward fill) propagates the last valid observation forward, while `bfill()` (backward fill) propagates the next valid observation backward. This is common for time series data where you assume a value remains constant until the next measurement.\n- **`mean()`:** Used for `numerical_cols`. Filling missing numerical data with the mean is a simple yet effective technique, assuming the data is not heavily skewed.\n- **Dropping Unnecessary Columns:** Finally, a list of columns is dropped from the DataFrame. This is likely based on previous analysis of the training data, where these columns were found to be irrelevant for the model's predictions.","metadata":{}},{"cell_type":"markdown","source":"## Sort dataset","metadata":{}},{"cell_type":"code","source":"# Convert 'measurement_datetime' to datetime (if not already done)\nmerged_test['measurement_datetime'] = pd.to_datetime(merged_test['measurement_datetime'], errors='coerce')\n\n# Sort the DataFrame by 'measurement_datetime' from earliest to latest (ascending order)\nmerged_test = merged_test.sort_values(by='measurement_datetime')\n\n# Display the cleaned DataFrame\nx_test = merged_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:11:52.576389Z","iopub.execute_input":"2025-09-02T22:11:52.576740Z","iopub.status.idle":"2025-09-02T22:11:52.641843Z","shell.execute_reply.started":"2025-09-02T22:11:52.576712Z","shell.execute_reply":"2025-09-02T22:11:52.640927Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Fill missing values","metadata":{}},{"cell_type":"code","source":"date_time = ['measurement_datetime']\n\nnumerical_cols = ['person_id',\n 'duration',\n 'device_hour',\n 'device_mean_hour',\n 'Base excess in Venous blood by calculation',\n 'Base excess in Arterial blood by calculation',\n 'Phosphate [Moles/volume] in Serum or Plasma',\n 'Potassium [Moles/volume] in Blood',\n 'Bilirubin.total [Moles/volume] in Serum or Plasma',\n 'Neutrophil Ab [Units/volume] in Serum',\n 'Bicarbonate [Moles/volume] in Arterial blood',\n 'Hematocrit [Volume Fraction] of Blood',\n 'Glucose [Moles/volume] in Serum or Plasma',\n 'Calcium [Moles/volume] in Serum or Plasma',\n 'Chloride [Moles/volume] in Blood',\n 'Sodium [Moles/volume] in Serum or Plasma',\n 'C reactive protein [Mass/volume] in Serum or Plasma',\n 'Carbon dioxide [Partial pressure] in Venous blood',\n 'Oxygen [Partial pressure] in Venous blood',\n 'Albumin [Mass/volume] in Serum or Plasma',\n 'Bicarbonate [Moles/volume] in Venous blood',\n 'Oxygen [Partial pressure] in Arterial blood',\n 'Carbon dioxide [Partial pressure] in Arterial blood',\n 'Interleukin 6 [Mass/volume] in Body fluid',\n 'Magnesium [Moles/volume] in Blood',\n 'Prothrombin time (PT)',\n 'Procalcitonin [Mass/volume] in Serum or Plasma',\n 'Lactate [Moles/volume] in Blood',\n 'Creatinine [Mass/volume] in Blood',\n 'Fibrinogen measurement',\n 'Bilirubin measurement',\n 'Partial thromboplastin time',\n ' activated',\n 'Total white blood count',\n 'Platelet count',\n 'White blood cell count',\n 'Blood venous pH',\n 'D-dimer level',\n 'Blood arterial pH',\n 'Hemoglobin [Moles/volume] in Blood',\n 'Ionised calcium measurement',\n 'Systolic blood pressure',\n 'Diastolic blood pressure',\n 'Body temperature',\n 'Respiratory rate',\n 'Heart rate',\n 'Measurement of oxygen saturation at periphery',\n 'Oxygen/Gas total [Pure volume fraction] Inhaled gas',\n 'Left pupil Diameter Auto',\n 'Right pupil Diameter Auto',\n 'Glasgow coma scale',\n 'procedure_duration',\n 'drug_duration',\n 'observation_duration']\n\ncategorical_cols = ['device',\n 'Capillary refill [Time]',\n 'Pulse',\n 'Arterial pulse pressure',\n 'Right pupil Pupillary response',\n 'Left pupil Pupillary response',\n 'procedure',\n 'drug_concept_id',\n 'route_concept_id',\n 'observation_concept_name',\n 'valuefilled']\n\nextra_cols = [\n 'Right pupil Pupillary response',\n 'Left pupil Pupillary response']\n\nx_test.loc[:, extra_cols] = x_test[extra_cols].apply(lambda col: col.fillna(col.mode()[0]), axis=0)\nx_test.loc[:, categorical_cols] = x_test[categorical_cols].ffill().bfill()\nx_test.loc[:, numerical_cols] = x_test[numerical_cols].fillna(x_test[numerical_cols].mean())\nx_test.loc[:, date_time] = x_test[date_time].fillna(x_test[date_time].mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:11:52.644297Z","iopub.execute_input":"2025-09-02T22:11:52.644590Z","iopub.status.idle":"2025-09-02T22:11:53.435046Z","shell.execute_reply.started":"2025-09-02T22:11:52.644565Z","shell.execute_reply":"2025-09-02T22:11:53.434044Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Drop unecessary columns","metadata":{}},{"cell_type":"code","source":"x_test.drop('measurement_datetime', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:11:53.435859Z","iopub.execute_input":"2025-09-02T22:11:53.436102Z","iopub.status.idle":"2025-09-02T22:11:53.476133Z","shell.execute_reply.started":"2025-09-02T22:11:53.436083Z","shell.execute_reply":"2025-09-02T22:11:53.474770Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"columns_to_drop = [\n    'person_id',\n    \"Calcium [Moles/volume] in Serum or Plasma\",\n    \"Left pupil Pupillary response\",\n    \"Capillary refill [Time]\",\n    \"Arterial pulse pressure\",\n    \"Left pupil Diameter Auto\",\n    \"Right pupil Pupillary response\",\n    \"observation_concept_name\",\n    \"Neutrophil Ab [Units/volume] in Serum\",\n    \"Heart rate\",\n    \"Interleukin 6 [Mass/volume] in Body fluid\",\n    \"Glucose [Moles/volume] in Serum or Plasma\",\n    \"C reactive protein [Mass/volume] in Serum or Plasma\",\n    \"Oxygen [Partial pressure] in Venous blood\",\n    \"Albumin [Mass/volume] in Serum or Plasma\",\n    \"Oxygen [Partial pressure] in Arterial blood\",\n    \"Carbon dioxide [Partial pressure] in Arterial blood\",\n    \"Lactate [Moles/volume] in Blood\",\n    \"D-dimer level\",\n    \"Bilirubin measurement\",\n    \" activated\",  # Note: There might be a typo here (extra space at the beginning)\n    \"Total white blood count\",\n    \"Platelet count\",\n    \"White blood cell count\",\n    \"Bicarbonate [Moles/volume] in Arterial blood\",\n    \"Blood venous pH\"\n]\n\n# Drop columns from x_test\nx_test = x_test.drop(columns=columns_to_drop)\n\nprint(\"x_test shape after dropping columns:\", x_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:11:53.477206Z","iopub.execute_input":"2025-09-02T22:11:53.477526Z","iopub.status.idle":"2025-09-02T22:11:53.502407Z","shell.execute_reply.started":"2025-09-02T22:11:53.477494Z","shell.execute_reply":"2025-09-02T22:11:53.501386Z"}},"outputs":[{"name":"stdout","text":"x_test shape after dropping columns: (130483, 39)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Text representation, Data scaling, and Dimensionality reduction","metadata":{}},{"cell_type":"markdown","source":"This is the core feature engineering part of the pipeline, where raw data is converted into a format suitable for a neural network.\n\n- **Text Representation (Word2Vec):** Categorical text columns (e.g., 'device', 'procedure') are converted into numerical vectors. Machine learning models can only process numerical data. Word2Vec is a technique that turns words (or in this case, categories) into dense numerical vectors (embeddings). The key idea is that words with similar meanings have similar vector representations. This allows the model to learn relationships between different devices or procedures. The code loads a pre-trained Word2Vec model. It then defines a `TextToEmbeddingTransformer` class to apply this model. For each text column, it splits the text into words, looks up the vector for each word, and then takes the average of all word vectors to create a single, fixed-size vector for each observation.\n\n- **Data Scaling (StandardScaler):** The numerical features are scaled to have a mean of 0 and a standard deviation of 1. Many machine learning algorithms, especially those that use gradient descent (like neural networks), perform better when features are on a similar scale. Standardization prevents features with larger numerical values from dominating the learning process. The code loads a pre-trained `StandardScaler` to ensure the test data is scaled consistently with how the training data was processed.\n\n- **Dimensionality Reduction (PCA):** The final step in this section is to reduce the number of features using Principal Component Analysis. After creating embeddings, the dataset's dimensionality (number of columns) can become very large, which can lead to computational inefficiency and the curse of dimensionality. PCA is a technique that finds a smaller set of new features (principal components) that capture most of the variance from the original data. This reduces noise and improves model performance. The code loads a pre-trained `PCA` model to apply this transformation.","metadata":{}},{"cell_type":"code","source":"# Load the saved transformer object\ntext_to_embedding_tuple = joblib.load(\"/kaggle/input/text_to_embedding_v0/scikitlearn/default/1/text_to_embedding_transformer.pkl\")\n\n# Debugging: Check what's inside\nprint(\"Loaded object:\", text_to_embedding_tuple)\nprint(\"Type of loaded object:\", type(text_to_embedding_tuple))\nprint(\"Length of loaded object:\", len(text_to_embedding_tuple))\n\n# Extract text_columns correctly\n#text_columns = text_to_embedding_tuple[0]  \ntext_columns = ['device', 'Pulse', 'procedure', 'drug_concept_id', 'route_concept_id', 'valuefilled']\n# Load Word2Vec model separately\nword2vec_model = Word2Vec.load(\"/kaggle/input/word2vec_v2/scikitlearn/default/1/word2vec.model\")\n\n# Define tokenizer\ndef tokenize_text(text):\n    if pd.isna(text):\n        return []\n    return str(text).split()\n\n# Define transformer class\nclass TextToEmbeddingTransformer:\n    def __init__(self, word2vec_model, text_columns):\n        self.word2vec_model = word2vec_model\n        self.text_columns = text_columns\n\n    def transform_text_to_embedding(self, text):\n        tokens = tokenize_text(text)\n        if not tokens:\n            return np.zeros(self.word2vec_model.vector_size)\n        vectors = [self.word2vec_model.wv[word] for word in tokens if word in self.word2vec_model.wv]\n        if not vectors:\n            return np.zeros(self.word2vec_model.vector_size)\n        return np.mean(vectors, axis=0)\n\n    def transform(self, X):\n        X = X.copy()\n        for col in self.text_columns:\n            if col in X.columns:  # Ensure column exists before transformation\n                X[col] = X[col].apply(self.transform_text_to_embedding)\n            else:\n                print(f\"Warning: Column '{col}' not found in input DataFrame.\")\n        return X\n\n# Recreate the TextToEmbeddingTransformer object\ntext_to_embedding = TextToEmbeddingTransformer(word2vec_model, text_columns)\n\n# Apply text-to-embedding transformation\nx_test_transformed = text_to_embedding.transform(x_test)\n\n# Verify the transformation\n#print(\"Sample transformed row:\")\n#print(x_test_transformed.iloc[0])\n\n# Convert transformed embeddings to a structured NumPy array\nembedding_dim = word2vec_model.vector_size\nx_test_embedded = np.array([np.hstack(row[text_columns].values) for _, row in x_test_transformed.iterrows()])\n\n# Load the saved scaler\nscaler = joblib.load(\"/kaggle/input/standard_scaler_v3/scikitlearn/default/1/standard_scaler.joblib\")\n\n# Standardize the test data\nx_test_scaled = scaler.transform(x_test_embedded)\n\n# Load the saved PCA model\npca = joblib.load(\"/kaggle/input/pca_model/scikitlearn/default/1/pca_model.joblib\")\n\n# Apply PCA transformation\nx_test_pca = pca.transform(x_test_scaled)\n\n# Output the transformed test data\nprint(\"Transformed x_test shape:\", x_test_pca.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:11:53.503486Z","iopub.execute_input":"2025-09-02T22:11:53.503803Z","iopub.status.idle":"2025-09-02T22:13:00.415608Z","shell.execute_reply.started":"2025-09-02T22:11:53.503766Z","shell.execute_reply":"2025-09-02T22:13:00.414789Z"}},"outputs":[{"name":"stdout","text":"Loaded object: ['device', 'Pulse', 'procedure', 'drug_concept_id', 'route_concept_id', 'valuefilled']\nType of loaded object: <class 'list'>\nLength of loaded object: 6\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 1.6.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Transformed x_test shape: (130483, 23)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Prepared time series dataset","metadata":{}},{"cell_type":"markdown","source":"**Time Series Data Structuring**\n\nThe final step reshapes the cleaned and transformed data into a 3D tensor suitable for a sequence model. Models like LSTMs or Transformers require input data in a specific 3D format: (`n_samples, n_timesteps, n_features`). This structure allows the model to analyze sequences of data over time rather than just single, isolated observations. The `create_time_series_dataset` function transforms the 2D data into this 3D shape.\n\nThe function iterates through the dataset and for each row, it creates a sequence of the previous `time_steps` observations. This creates a \"sliding window\" over the data, which is a common technique for preparing time series data for deep learning. If there aren't enough previous steps (at the beginning of the dataset), it pads the sequence with zeros.\n\nAfter these steps, the `x_test_time_series` variable contains the fully prepared, time-sequenced data, ready to be fed into the final machine learning model for inference.","metadata":{}},{"cell_type":"code","source":"def create_time_series_dataset(data, time_steps):\n    \"\"\"\n    Create time series dataset while retaining the same shape as input data.\n    \n    Args:\n        data (np.ndarray): Input data of shape (n_samples, n_features).\n        time_steps (int): Number of time steps to use for creating sequences.\n    \n    Returns:\n        np.ndarray: Time series data of shape (n_samples, time_steps, n_features).\n    \"\"\"\n    n_samples, n_features = data.shape\n    Xs = np.zeros((n_samples, time_steps, n_features))  # Initialize output array\n    \n    for i in range(n_samples):\n        if i < time_steps:\n            # Pad the beginning with zeros if there aren't enough previous time steps\n            Xs[i, :i+1, :] = data[:i+1, :]\n        else:\n            # Slice time series data\n            Xs[i, :, :] = data[i-time_steps+1:i+1, :]\n    \n    return Xs\n\ntime_steps = 100  # Number of time steps\n\n# Create time series dataset\nx_test_time_series = create_time_series_dataset(x_test_pca, time_steps)\n\nprint(\"Shape of time series X_test:\", x_test_time_series.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:13:00.416594Z","iopub.execute_input":"2025-09-02T22:13:00.417414Z","iopub.status.idle":"2025-09-02T22:13:02.901065Z","shell.execute_reply.started":"2025-09-02T22:13:00.417384Z","shell.execute_reply":"2025-09-02T22:13:02.900138Z"}},"outputs":[{"name":"stdout","text":"Shape of time series X_test: (130483, 100, 23)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Model Prediction","metadata":{}},{"cell_type":"markdown","source":"**Step-By-Step Breakdown**\n\n**Loading the Trained Model**\n\nAfter a model is trained on a large dataset, its architecture and learned parameters (weights and biases) are saved to a file. This process is called serialization. Loading a Trained model means you don't need to retrain it, which saves a significant amount of time and computational resources. The `.h5` file format is a common way to save Keras/TensorFlow models, as it stores the model's architecture, weights, and optimizer state in a single file. The code first loads a saved TensorFlow model from a file. `tf.keras.models.load_model(...)` function is the standard way to load a saved Keras model. It reads the model's structure and weights from the specified file path, recreating the exact model that was saved.\n\n**Making Predictions on New Data**\n\nOnce the model is loaded, the code uses it to generate predictions on the prepared test data. Inference is the process of using a trained model to make predictions or decisions on new data. The model takes the `x_test_time_series` dataset, which has been pre-processed and shaped into the correct format, and runs it through the neural network. The output, predictions, will be the model's forecast for each input sample. `model.predict(x_test_time_series)` is the core command that performs the inference. The model processes the input data in a forward pass, layer by layer, until it produces the final output. The `x_test_time_series` data is likely a 3D array with the shape (`number_of_samples, time_steps, number_of_features`), a format commonly used for time series models like LSTMs or Transformers, which would have been prepared in an earlier step. The output predictions will be a NumPy array containing the model's predictions.","metadata":{}},{"cell_type":"code","source":"# Load the TensorFlow model\nmodel = tf.keras.models.load_model(\"/kaggle/input/mild_model_v6/tensorflow2/default/1/optimized_time_series_modelV1.h5\")\n\n# Make predictions\npredictions = model.predict(x_test_time_series)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:13:02.902030Z","iopub.execute_input":"2025-09-02T22:13:02.902318Z","iopub.status.idle":"2025-09-02T22:23:02.172111Z","shell.execute_reply.started":"2025-09-02T22:13:02.902288Z","shell.execute_reply":"2025-09-02T22:23:02.170825Z"}},"outputs":[{"name":"stderr","text":"2025-09-02 22:13:03.016800: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4078/4078\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 145ms/step\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Submission File Generation","metadata":{}},{"cell_type":"markdown","source":"**Step-By-Step Breakdown**\n\n**Adding Predictions to the Main DataFrame**\n\nThe first action is to add the generated predictions to the merged_test DataFrame.\n\n- `merged_test['SepsisLabel'] = predictions` : A new column (`SepsisLabel`) is created in the `merged_test` DataFrame. The values from the predictions array, which are the model's output, are assigned to this new column. This effectively links the final prediction to the corresponding patient and timestamp.\n\n**Merging Predictions with the Submission Template**\n\nThis part of the code merges the predictions back into the original test_submission DataFrame, which likely serves as the template for the final output.\n\n- `pd.merge(...)`: The `pd.merge` function is used to join `test_submission` with the `merged_test` DataFrame.\n- `on='person_id'`: The merge is performed on the `person_id` column. This is crucial for ensuring that the correct prediction is associated with the correct patient.\n- `how='left'`: A left merge is used. This means that all rows from the `test_submission` DataFrame will be kept, and the corresponding `SepsisLabel` from `merged_test` will be added to each row.\n\n**Creating a Unique Identifier**\n\nA new column is created to serve as a unique identifier for each prediction. This is a common requirement for submission files in data science competitions.\n\n- `test_submission['person_id_datetime'] = (...)`: A new column is created by concatenating the `person_id` and the `measurement_datetime` with an underscore separator. This creates a unique key for each prediction instance, as each patient at each specific time point has a distinct record.\n\n**Finalizing the Submission File**\n\nThe final steps clean up the DataFrame and save it to a CSV file.\n\n- `test_submission = test_submission[['person_id_datetime', 'SepsisLabel']]`: This command selects only the two required columns for the final submission: the unique identifier (`person_id_datetime`) and the predicted value (`SepsisLabel`). All other columns are dropped.\n- `test_submission = test_submission.drop_duplicates(...)`: This ensures that each unique `person_id_datetime` has only one entry. While the merge should have created unique rows, this is a good practice to handle any potential data duplication issues.\n- `test_submission.to_csv('submission.csv', index=False)`: The final DataFrame is saved as a CSV file named `submission.csv`. The `index=False` argument prevents pandas from writing the DataFrame's index as an extra column in the CSV file, which is typically not desired in a submission file.\n- `print(\"Submission successfully created.\")`: A message is printed to the console to confirm that the process is complete.","metadata":{}},{"cell_type":"code","source":"# Ensure 'measurement_datetime' is a string for concatenation\ntest_submission['measurement_datetime'] = test_submission['measurement_datetime'].astype(str)\n\n# Add predictions to the merged_test dataset\nmerged_test['SepsisLabel'] = predictions\n\n# Merge observations data into merged_test using a memory-efficient approach\n# Instead of loading everything into memory, process chunks if the dataset is large\ntest_submission = pd.merge(\n    test_submission,\n    merged_test[['person_id', 'SepsisLabel']],  # Only select necessary columns\n    on='person_id',\n    how='left'\n)\n\n# Concatenate 'person_id' and 'measurement_datetime' with '_' as separator\ntest_submission['person_id_datetime'] = (\n    test_submission['person_id'].astype(str) + '_' + test_submission['measurement_datetime']\n)\n\n# Select only the required columns\ntest_submission = test_submission[['person_id_datetime', 'SepsisLabel']]\n\n# Remove duplicates based on 'person_id_datetime'\ntest_submission = test_submission.drop_duplicates(subset=['person_id_datetime'])\n\n# Save the submission file\ntest_submission.to_csv('submission.csv', index=False)\n\nprint(\"Submission successfully created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T22:23:02.176397Z","iopub.execute_input":"2025-09-02T22:23:02.177099Z","iopub.status.idle":"2025-09-02T22:23:59.323412Z","shell.execute_reply.started":"2025-09-02T22:23:02.177055Z","shell.execute_reply":"2025-09-02T22:23:59.320875Z"}},"outputs":[{"name":"stdout","text":"Submission successfully created.\n","output_type":"stream"}],"execution_count":10}]}