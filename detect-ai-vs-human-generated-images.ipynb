{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91198,"databundleVersionId":10884264,"sourceType":"competition"},{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":3719.960153,"end_time":"2025-01-22T10:42:14.878393","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-22T09:40:14.918240","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"090a8067da624edfb869285316994dec":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e5ddd82c34a419b9bcdf273a1bf48cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c86e5f5ed4b422ca94d90051209f342":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_c3e653ec074047b898d04bea8d2651c2","placeholder":"​","style":"IPY_MODEL_30a5a12b8ac146df96f47b459ebf084e","tabbable":null,"tooltip":null,"value":" 86.5M/86.5M [00:00&lt;00:00, 230MB/s]"}},"30a5a12b8ac146df96f47b459ebf084e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"352ce852af0a46cbbebb57bf14e67410":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8b45f6792f634b23ac69046e9dbc8cfa":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f802a96b2244d9aa1d2b473e2fa4b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_bc7598ab7fca4a7fb550fc79977976c2","max":86523256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e5ddd82c34a419b9bcdf273a1bf48cb","tabbable":null,"tooltip":null,"value":86523256}},"bc7598ab7fca4a7fb550fc79977976c2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3e653ec074047b898d04bea8d2651c2":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce512ee981ab4a7e9cbc840f8e63678c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e653f4335c0b4d1b82f184ed42b251d8","IPY_MODEL_93f802a96b2244d9aa1d2b473e2fa4b6","IPY_MODEL_1c86e5f5ed4b422ca94d90051209f342"],"layout":"IPY_MODEL_090a8067da624edfb869285316994dec","tabbable":null,"tooltip":null}},"e653f4335c0b4d1b82f184ed42b251d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8b45f6792f634b23ac69046e9dbc8cfa","placeholder":"​","style":"IPY_MODEL_352ce852af0a46cbbebb57bf14e67410","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3b218463-2afe-4a9d-806b-b6d41cdf51ac","cell_type":"markdown","source":"# Competition Overview","metadata":{}},{"id":"1bcd7f33-2624-4116-be0e-705dce0de149","cell_type":"markdown","source":"**Competition Overview: AI vs. Human-Generated Images**\n\nThis competition challenges participants to develop machine learning models that can accurately classify images as either AI-generated or human-created. The challenge is a response to the growing sophistication of generative AI and the need for robust deepfake detection systems to combat misinformation and maintain media trust.","metadata":{}},{"id":"59bdf2b6-42c7-41e9-a6ae-4abd8ea60760","cell_type":"markdown","source":"**Challenge, Objective, and Process**\n\nThe core objective is to design a model that performs well on a diverse dataset of paired real and AI-generated images. The dataset, a collaborative effort between Shutterstock and DeepMedia, includes authentic images with a mix of human subjects and their AI-generated counterparts, which are created using state-of-the-art generative models. This pairing allows for a direct comparison that enhances model training. The primary evaluation metric for submissions is the F1-Score.\n\nParticipation is open to everyone, and team formation is encouraged, with a maximum of four members per team. The competition promotes diversity and inclusivity, with special prizes for teams that demonstrate diversity in their composition.","metadata":{}},{"id":"f8763b8f-594d-43c0-8a96-7d66b4a01632","cell_type":"markdown","source":"**Submission and Evaluation**\n\nParticipants must submit their predictions as a CSV file with two columns: id and label. The label column should contain the model's prediction, with 0 for human-created and 1 for AI-generated images.\n\nThe primary evaluation metric is the F1-Score, calculated at a 0.5 threshold. This metric is well-suited for this task because it provides a balance between the model’s precision (avoiding false positives) and its recall (avoiding false negatives), which are both critical for effective image authenticity detection. Submitted notebooks must be clear, well-documented, and reproducible to be considered for final evaluation.","metadata":{}},{"id":"f32d9eb7-c098-48f3-92e1-debfeae3cd9f","cell_type":"markdown","source":"**Timeline and Prizes**\n\nThe competition runs from January 15, 2025, to March 7, 2025. The total prize pool is $10,000, which will be awarded to the top five teams based on two key criteria: Leaderboard Ranking and Innovation and Diversity. Prizes are specifically allocated to the Best Innovative Team, Best Student Team, Best Diverse Team, Best Female Team, and Best Global South Team, reinforcing the competition's commitment to inclusivity in the AI community.\n\n**What participants will get...**\n\n- Real-World Impact: Tackle the challenge of deepfake detection, a critical issue for media authenticity and security.\n- Skill Development: Gain hands-on experience with dataset from industry leaders.\n- Global Recognition: Compete with data scientists worldwide and showcase your work.\n- Inclusivity: Be part of a challenge that promotes diversity and fairness in AI.","metadata":{}},{"id":"3e393a1e","cell_type":"markdown","source":"### [Competition Link](https://www.kaggle.com/competitions/detect-ai-vs-human-generated-images)","metadata":{}},{"id":"edd782f0-6bda-4efc-ba9e-1ec096a3c47c","cell_type":"markdown","source":"# Dataset Description","metadata":{}},{"id":"182aba4d-c43d-40c7-b76f-9b0ddd9b6f80","cell_type":"markdown","source":"This dataset is designed for the \"AI vs. Human-Generated Images\" challenge, providing a robust foundation for building image authenticity detection systems. It is a carefully curated collection of images that directly compares authentic human-created content with their AI-generated equivalents.\n\nThe dataset is structured to facilitate the training and evaluation of machine learning models for deepfake detection. It includes a training set and a test set, with images stored in separate folders (train_data and test_data) and organized with unique IDs. The training and test CSV files provide the link between the image IDs and their corresponding labels.\n\nThe images are sourced from two key partners:\n\n- **Authentic images:** Provided by Shutterstock, a global leader in creative content. These images include a balanced selection of categories, with one-third featuring human subjects.\n- **AI-generated images:** Contributed by DeepMedia, an expert in AI-generated content. These images are the AI-created counterparts to the authentic Shutterstock images, ensuring a direct comparison for model training.","metadata":{}},{"id":"cd117fa3-ca5f-4b28-91f9-84018e816e92","cell_type":"markdown","source":"**Data Structure**","metadata":{}},{"id":"8538c103-1f05-431b-9515-249d66e88331","cell_type":"markdown","source":"<pre>\n/data\n    /train data\n        /images/ # A folder containing all the images for the training set, identified by their unique id, containing 79,950 images \n                   with their corresponding IDs and labels.    \n    /test data\n        /images/ # A folder containing all the images for the test set, identified by their unique id,containing 19,986 images for \n                   which participants must predict the labels.\n    \n    /test.csv    # The index of test dataset in csv file format\n    \n    /train.csv   # The index of train dataset in csv file format\n</pre>","metadata":{}},{"id":"85407d77-fd11-4372-a034-5eb2630c21c0","cell_type":"markdown","source":"**Data Dictionary**","metadata":{}},{"id":"f72fb2da-7770-4063-a640-951b4402ce9e","cell_type":"markdown","source":"**Train.csv**","metadata":{}},{"id":"43484b7d-fbfe-4ab0-8bad-37505dbb32d6","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">id</td>\n    <td class=\"tg-0lax\">A unique identifier for each image. <br>This ID links the image file (e.g., a6dcb93f596a43249135678dfcfc17ea.jpg) to its entry in the CSV file.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">label</td>\n    <td class=\"tg-0lax\">The target variable for the classification task. <br>This column indicates whether an image is AI-generated (1) or human-created (0). <br>This column is present in the train.csv file but must be predicted in the test.csv file for submission.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"id":"aef14296-f418-4a0d-90b9-6a056c15738b","cell_type":"markdown","source":"**Test.csv**","metadata":{}},{"id":"e08ddb6f-a272-45bc-8206-79aba3e25413","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Column</th>\n    <th class=\"tg-7zrl\">Description</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">id</td>\n    <td class=\"tg-0lax\">A unique identifier for each image. <br>This ID links the image file (e.g., a6dcb93f596a43249135678dfcfc17ea.jpg) to its entry in the CSV file.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"id":"eecd5fb9","cell_type":"markdown","source":"### [Dataset Link](https://www.kaggle.com/datasets/97c52dbae208af8189b94d549ab0f2dfd90c3f5b14700d8506ba358d1a2d5c6f/data)","metadata":{}},{"id":"cafeaa3b-aa1a-4f68-a3ae-35528208674c","cell_type":"markdown","source":"# Approach","metadata":{}},{"id":"29a8a30d-baec-4c23-85fe-67ed376cec09","cell_type":"markdown","source":"**Approach - Vanilla classification baseline with timm and albumentations**\n\nIn machine learning and computer vision projects, a vanilla classification baseline is a foundational model used as a point of reference. Its purpose is to establish a minimum performance benchmark against which the results of more complex or novel methods can be measured, such as a simple logistic regression or a shallow neural network with one hidden layer, without advanced features like complex architectural designs or intricate regularization techniques then continue improve model against the baseline.\n\n**1. Using Pre-trained Models:**\n\nThe core of this approach is leveraging pre-trained models from the timm (PyTorch Image Models) library. Instead of training a model from scratch, which requires a massive amount of data and computational power, the code uses powerful, pre-trained architectures (DINOv2, Hiera, and EfficientNet V2). It then fine-tunes these models on the specific dataset.\n\n**2. Standard Data Augmentation:**\n\nTo improve the model's ability to generalize and prevent overfitting, the code uses a wide range of standard image transformations from the albumentations library. This includes common techniques such as:\n\n- **Geometric transformations:** horizontal/vertical flips, rotation, scaling, and affine transformations.\n- **Color-based transformations:** random brightness/contrast, hue/saturation/value shifts.\n- **Noise and blur:** Gaussian noise and blur.\n- **Normalization:** The standard normalization using ImageNet means and standard deviations.\n\n**3. Ensemble Learning:**\n\nThe approach goes a step beyond a single model by using an ensemble of three different models (DINOv2, Hiera, and EfficientNet V2). Each model is trained independently on its own augmented version of the input data, and their predictions are averaged in the forward pass to produce a final output. This simple ensemble method is a common and effective technique for boosting performance without requiring complex new architecture designs.\n\n**4. Standard Training Loop:**\n\nThe train_phase function implements a classic training loop.\n\n- It uses the AdamW optimizer, a popular choice for fine-tuning deep learning models.\n- It employs a learning rate scheduler (LambdaLR) to gradually decrease the learning rate over epochs, which is a standard practice for stable training.\n- The loss function is nn.CrossEntropyLoss, the most common choice for multi-class classification problems.","metadata":{}},{"id":"67f68405-bd84-4516-a788-7af403cd7500","cell_type":"markdown","source":"# Pipeline overview","metadata":{}},{"id":"9e01d716-b3be-477f-b09f-ba6cbcfaffeb","cell_type":"markdown","source":"**An Ensemble Model for Image Classification**\n\nThis Python code outlines a sophisticated pipeline for fine-tuning an ensemble of pre-trained deep learning models for image classification. The primary objective is to leverage powerful, state-of-the-art architectures from the timm library and combine their predictive power to achieve high accuracy. The pipeline is designed to be robust and efficient, incorporating advanced techniques like aggressive data augmentation, mixed-precision training, and a dynamic learning rate schedule.\n\n**1. Data Preparation and Augmentation**\n\nThe pipeline begins by setting up the data. A custom MultiSizeDataset class is created to handle images for multiple models with different input size requirements (e.g., DINOv2, Hiera, and EfficientNet). This class loads images and applies specific transformations to them on the fly.\n\n- **Training Transformations:** The get_train_transform function uses the albumentations library to apply a wide range of random augmentations to the training images. This is a critical step for preventing overfitting and increasing the model's ability to generalize to new, unseen images. The transformations include resizing, padding, flips, rotations, and various color and noise distortions.\n- **Validation Transformations:** The get_val_transform function applies a simpler set of transformations to the validation data, primarily focusing on resizing and normalization to ensure consistent input size and format.\n\n**2. Ensemble Model Architecture**\n\nThe core of the pipeline is the EnsembleModel class, a neural network module that combines three different pre-trained vision models. This ensemble approach is based on the theory that combining the predictions of multiple, diverse models often leads to better and more robust performance than a single model alone.\n\n- **Weak Learner Models:** The ensemble consists of:\n\n  - **DINOv2:** A powerful self-supervised vision transformer known for its strong feature representations.\n  - **Hiera:** A hierarchical vision transformer.\n  - **EfficientNetV2:** A family of convolutional neural networks optimized for both speed and accuracy.\n\n- **Parallel Processing:** During the forward pass, each image is processed by all three models simultaneously, and the outputs are combined to make a final prediction.\n\n**3. Fine-Tuning and Optimization**\n\nThe train_phase function manages the training loop, where the ensemble model's weights are fine-tuned on the new dataset. This process is optimized for efficiency and stability.\n\n- **Learning Rate Schedule:** An adaptive learning rate schedule is implemented to adjust the learning rate during training. This strategy helps the model to converge quickly in the early epochs and fine-tune more precisely in later epochs.\n- **Mixed-Precision Training:** The code leverages torch.amp.autocast, which performs computations in both float16 and float32 precision. This significantly speeds up training and reduces memory usage on CUDA-enabled GPUs without sacrificing model accuracy.\n- **Gradient Clipping:** The torch.nn.utils.clip_grad_norm_ function is used to prevent the vanishing or exploding gradient problem, which can cause training to become unstable or fail. This technique limits the magnitude of the gradients during backpropagation.\n- **Model Checkpointing:** The best-performing version of the model on the validation set is automatically saved. This ensures that the final output is the one with the highest accuracy, even if the model's performance fluctuates during the final epochs.","metadata":{}},{"id":"aea3ebb3-a225-4af9-a2ab-8909b02f7921","cell_type":"markdown","source":"# Related Notebooks","metadata":{}},{"id":"41eedf64-b994-4db0-961a-3a5cb57420fa","cell_type":"markdown","source":"### [Reference](https://arxiv.org/pdf/2305.12972)\n### [Reference](https://medium.com/the-modern-scientist/mastering-the-essentials-of-machine-learning-with-a-vanilla-example-1ee5d090eae6)\n### [Inspiration Notebook](https://www.kaggle.com/code/vyacheslavshen/99-97-lb-baseline-with-timm)","metadata":{}},{"id":"65ea663e-c050-4ac6-aa53-039e1a3be83f","cell_type":"markdown","source":"# Install Packages and Import Libraries","metadata":{}},{"id":"30dcd4d5-c2f0-42ff-a20a-86d5a507470c","cell_type":"code","source":"!pip install albumentations\n!pip install --upgrade albumentations\n!pip install timm\n!pip install --upgrade timm\n!pip install kaggle\n!pip install kagglehub","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n","Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.2.6)\n","Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.16.2)\n","Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations) (5.4.1)\n","Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n","Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\n","Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (4.0.7)\n","Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n","Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.2.6)\n","Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.16.2)\n","Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations) (5.4.1)\n","Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n","Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.12.0.88)\n","Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (4.0.7)\n","Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.1.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.16.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from timm) (5.4.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.66.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.15.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.2.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (2.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (3.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (2020.6.20)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->timm) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.1.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.16.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from timm) (5.4.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.66.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.15.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.2.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (2.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (3.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (2020.6.20)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->timm) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"execution_count":9},{"id":"817a4e8e-0610-4c47-9e78-0b37a0874cf1","cell_type":"code","source":"!pip install pandas","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.0)\n","Collecting numpy<2,>=1.23.2 (from pandas)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.2.6\n","    Uninstalling numpy-2.2.6:\n","      Successfully uninstalled numpy-2.2.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","deepspeed 0.10.3 requires pydantic<2.0.0, but you have pydantic 2.11.7 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"execution_count":10},{"id":"d5c12201-aa01-48b3-a66f-83560f056691","cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom albumentations.pytorch import ToTensorV2\nimport timm\nimport albumentations as albu\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom torch.utils.data._utils.collate import default_collate\nimport kagglehub","metadata":{"tags":[]},"outputs":[],"execution_count":12},{"id":"a453b2b4-f6f2-438b-811b-c72cfd088c89","cell_type":"markdown","source":"# Download dataset using API from Kaggle","metadata":{}},{"id":"8ea371ed-1c45-4cab-a2b4-94d3b91eb480","cell_type":"markdown","source":"Due to this model is train on cloud computer, so it needs to use API to download dataset from Kaggle.com.","metadata":{}},{"id":"3083857f-196b-4c36-a70e-2e62f80b7cab","cell_type":"code","source":"# configuring the path of Kaggle.json file\n!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n# Remark that have to import Kaggle API key in the same path with notebook\n\n# Path to competition dataset\npath = kagglehub.dataset_download(\"alessandrasala79/ai-vs-human-generated-dataset\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b6e9f988-8219-4457-95b2-746158eb5b9f","cell_type":"markdown","source":"# Set Configurations","metadata":{}},{"id":"6ecf55d4-655a-48f0-a54e-211410533f7b","cell_type":"code","source":"# Model Constants\nBATCH_SIZE = 8\nSEED = 42\nDINO_SIZE = 518\nEFFNET_SIZE = 384\nHIERA_SIZE = 224\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"tags":[]},"outputs":[],"execution_count":13},{"id":"985eeb8e-62c7-463a-bc17-b6dd695ceea8","cell_type":"markdown","source":"# Check file path","metadata":{}},{"id":"725a5fe5-0643-4d06-b861-b91daa87f7ff","cell_type":"markdown","source":"The Python script below is a basic but essential tool for file system management within project. It's designed to programmatically inspect and display the contents of a specified directory, which is crucial for verifying data integrity and understanding the project's file structure.","metadata":{}},{"id":"d667ef35-6fac-4529-bccb-0c47cbfef372","cell_type":"code","source":"# Define the path to the directory you want to inspect\ntarget_path = \"/root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4\"\n\ndef list_directory_contents(path):\n    \"\"\"\n    Lists all files and subdirectories in the specified path.\n    \"\"\"\n    print(f\"--- Contents of: {path} ---\")\n    try:\n        # Get a list of all entries in the directory\n        contents = os.listdir(path)\n        if not contents:\n            print(\"The directory is empty.\")\n        else:\n            for item in contents:\n                item_path = os.path.join(path, item)\n                if os.path.isdir(item_path):\n                    print(f\"DIRECTORY: {item}\")\n                else:\n                    print(f\"FILE:      {item}\")\n    except FileNotFoundError:\n        print(f\"Error: The directory was not found at {path}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n# Run the function with the target path\nlist_directory_contents(target_path)","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Contents of: /root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4 ---\n","FILE:      test.csv\n","DIRECTORY: test_data_v2\n","DIRECTORY: train_data\n","FILE:      train.csv\n"]}],"execution_count":14},{"id":"30edd551-082f-4a0c-990e-3da2e16c068e","cell_type":"markdown","source":"# Data Preparation and Augmentation","metadata":{}},{"id":"8603f007-bfea-4f12-8a0f-cf1abb048a8b","cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"id":"fd6a53a8-79da-4259-85ff-adb055dd01ca","cell_type":"markdown","source":"The primary objective is to sysnthetics images to increase the size and diversity of the training dataset. By applying a variety of randomized transformations - including geometric, photometric, and noise-based changes - to the input images, the model is exposed to a broader range of visual variations. This process helps to regularize the model, significantly reducing the risk of overfitting to the specific training data and thereby improving its ability to generalize to new, unseen images.\n\nThe pipeline is implemented using the albumentations library, which is highly optimized for performance and provides a wide array of image transformations, making it a standard tool in modern computer vision research.","metadata":{}},{"id":"bee26799-02b9-44ce-8976-cdf500117772","cell_type":"markdown","source":"**Albumentations library**\n\nAlbumentations is an optimized Python library for data augmentation in computer vision. It's designed to efficiently create a diverse set of transformed training images to improve model performance.\n\n**How it Works**\n\nAt its core, Albumentations uses a declarative pipeline. User have to define a sequence of transformations, each with a specific probability of being applied, using the Compose class. When an image is passed through this pipeline during training, it is randomly modified on-the-fly. This ensures the model never sees the exact same image twice, which is a key strategy for effective learning.\n\n**Theories and concepts**\n\nThis augmentation pipeline is grounded in several key theoretical concepts in machine learning and computer vision:\n\n- **Regularization:** Data augmentation acts as a powerful form of regularization. By preventing the model from seeing the exact same image twice during training, it discourages the model from \"memorizing\" the training data. This combats overfitting and forces the model to learn meaningful, high-level features that are representative of the underlying data distribution, not just the specific training samples.\n- **Imbalance dataset:** The goal of data augmentation is to teach the model to be invariant to a wide range of transformations. For a classification model, invariance means that the output class should not change even if the image is rotated, scaled, or has its lighting altered. By exposing the model to these augmented variations, it learns to extract features that are robust to these changes, leading to better generalization.\n- **Learning with Limited Data:** In many real-world scenarios, the amount of available data is a major constraint. Data augmentation is an effective strategy to combat this challenge, as it significantly expands the effective size of the training set without requiring the acquisition of new data.\n\n**Transformation Categories**\n\nAlbumentations provides a wide array of transformations, which can be grouped into four main categories:\n\n- **Geometric:** These alter the spatial arrangement of pixels (e.g., flips, rotations, scaling, and distortions).\n- **Photometric:** These modify the color and lighting characteristics of the image (e.g., brightness, contrast, hue, and saturation).\n- **Noise and Blur:** These simulate real-world imperfections and sensor noise (e.g., Gaussian noise and blur).\n- **Normalization:** A crucial step that standardizes pixel values to a mean of 0 and a standard deviation of 1, which helps stabilize model training.","metadata":{}},{"id":"4c056f31-6611-4cc4-821c-bbafecf7d256","cell_type":"markdown","source":"**Step-By-Step Breakdown**\n\nThis step defines two key functions, `get_train_transform(size)` and `get_val_transform(size)`.\n\n- **`get_train_transform(size)`:** This function creates a composite pipeline of transformations using albu.Compose. Each time a training image is requested by the data loader, this sequence of operations is applied on-the-fly, generating a unique version of the image for each epoch.\n  - **Resizing and Padding:** The pipeline begins with `albu.LongestMaxSize(size)` and `albu.PadIfNeeded(size, size)`. These ensure all images are resized to a uniform, square dimension, a prerequisite for batch processing in a neural network.\n  - **Geometric Transformations:** These include `HorizontalFlip, VerticalFlip, Rotate, and Affine` (which combines translation, scaling, rotation, and shearing). These operations modify the image's spatial orientation.\n  - **Photometric Transformations:** These augmentations, such as `RandomBrightnessContrast, HueSaturationValue, RandomGamma, and CLAHE`, alter the image's color, brightness, and contrast properties.\n  - **Noise and Blur:** `GaussNoise and GaussianBlur` add simulated noise and blur to the image, teaching the model to be robust to sensor noise and out-of-focus artifacts.\n  - **Distortion:** The `albu.OneOf` block randomly selects and applies one of three complex warping transformations: `OpticalDistortion, GridDistortion, or ElasticTransform`. This introduces non-linear deformations.\n  - **Normalization and Conversion:** The final steps are crucial for model training. `albu.Normalize` standardizes the pixel values, a common practice to ensure stable training. `ToTensorV2` converts the image from a NumPy array to a PyTorch tensor, the required format for the model.\n- **`get_val_transform(size)`:** This function defines a simpler pipeline for the validation set. It includes only resizing, padding, and normalization, along with minor noise and blur. No random geometric or color transformations are applied. This is a crucial distinction, as validation data must be evaluated in a deterministic and non-randomized manner to obtain a consistent and reliable measure of the model's performance on a fixed set of data.","metadata":{}},{"id":"53adcecf-ce48-4e9b-a2ea-96eca9875e4f","cell_type":"code","source":"# Model-specific augmentations\ndef get_train_transform(size):\n    return albu.Compose([\n        albu.LongestMaxSize(size),\n        albu.PadIfNeeded(size, size, border_mode=0),\n        albu.HorizontalFlip(p=0.7),  # Increased from 0.5 to 0.7\n        albu.VerticalFlip(p=0.7),    # Increased from 0.5 to 0.7\n        albu.Rotate(limit=180, p=0.9),  # Widened from 90 to 180, p from 0.7 to 0.9\n        albu.Affine(\n            translate_percent=(-0.2, 0.2),  # Increased translation range\n            scale=(0.6, 1.4),                 # Wider scaling range\n            rotate=(-60, 60),                 # Increased rotation range\n            shear=(-15, 15),                  # Added shear for additional distortion\n            p=0.9\n        ),\n        albu.RandomBrightnessContrast(\n            brightness_limit=(-0.4, 0.4),  # Increased from 0.3 to 0.4\n            contrast_limit=(-0.4, 0.4),    # Increased from 0.3 to 0.4\n            p=0.9                          # Increased from 0.8 to 0.9\n        ),\n        albu.HueSaturationValue(\n            hue_shift_limit=30,             # Increased from 20 to 30\n            sat_shift_limit=40,             # Increased from 30 to 40\n            val_shift_limit=30,             # Increased from 20 to 30\n            p=0.9                          # Increased from 0.7 to 0.9\n        ),\n        albu.GaussNoise(\n            std_range=(0.1, 0.5),         # Stronger noise\n            p=0.9\n        ),\n        albu.GaussianBlur(\n            blur_limit=(3, 9),             # Increased from (3, 7) to (3, 9)\n            p=0.8                          # Increased from 0.6 to 0.8\n        ),\n        albu.RandomGamma(\n            gamma_limit=(60, 140),         # Expanded from (80, 120) to (60, 140)\n            p=0.7                          # Increased from 0.5 to 0.7\n        ),\n        albu.CLAHE(\n            clip_limit=(2.0, 6.0),         # Expanded from 4.0 to (2.0, 6.0) for variability\n            tile_grid_size=(8, 8),\n            p=0.7                          # Increased from 0.5 to 0.7\n        ),\n        albu.OneOf([\n            albu.OpticalDistortion(distort_limit=0.1, p=1.0),            # Increased from 0.05 to 0.1\n            albu.GridDistortion(num_steps=5, distort_limit=0.1, p=1.0),  # Increased from 0.05 to 0.1\n            albu.ElasticTransform()                                      # Added elastic transform\n        ], p=0.5),                                                       # Increased from 0.3 to 0.5\n        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        albu.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.7),\n        albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.9),\n        ToTensorV2(),\n    ])\n\ndef get_val_transform(size):\n    return albu.Compose([\n        albu.LongestMaxSize(size),\n        albu.PadIfNeeded(size, size, border_mode=0),\n        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        albu.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.7),\n        albu.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.9),\n        ToTensorV2(),\n    ])\n","metadata":{"tags":[]},"outputs":[],"execution_count":15},{"id":"16d95cca-1c0a-46da-90ca-55ab03d7e307","cell_type":"markdown","source":"**Albumentation Functions and its Hyperparameters Description**","metadata":{}},{"id":"d07d7f83-bc9a-4b1b-8db8-35044ef40d74","cell_type":"markdown","source":"These transformations are categorized into geometric, color, and noise-related groups to enhance the diversity and robustness of a training dataset. The following tables detail each function's purpose, hyperparameters, and the resulting image transformation.","metadata":{}},{"id":"063b9f0d-3675-48e5-83db-9cf4360a29c8","cell_type":"markdown","source":"**Resizing and Padding**","metadata":{}},{"id":"cd549a08-86ac-421c-84ec-644b04c3acea","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Function</th>\n    <th class=\"tg-7zrl\">Transformation</th>\n    <th class=\"tg-7zrl\">Hyperparameters &amp; Setup</th>\n    <th class=\"tg-7zrl\">Result</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">LongestMaxSize</td>\n    <td class=\"tg-7zrl\">Resizes the input image so that its longest side matches a specified size, maintaining the aspect ratio. This is the first step to standardize image dimensions.</td>\n    <td class=\"tg-7zrl\">- size: The target size for the longest side.</td>\n    <td class=\"tg-0lax\">An image scaled down (or up) to a maximum side length of size, with its original aspect ratio intact.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">PadIfNeeded</td>\n    <td class=\"tg-7zrl\">Pads the image with a specified border color if its dimensions are less than the required size x size.</td>\n    <td class=\"tg-7zrl\">- size: The target height and width. <br>- border_mode=0: Specifies a constant border value, in this case, a black border.</td>\n    <td class=\"tg-0lax\">A square image of size x size pixels, with the original image centered and padded with black space.</td>\n  </tr>\n</tbody></table>","metadata":{}},{"id":"766602e0-144c-4550-a9cd-32c189785ef9","cell_type":"markdown","source":"**Geometric Transformations**","metadata":{}},{"id":"cc600e07-7bce-49e4-8e23-7dc4b73c326e","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Function</th>\n    <th class=\"tg-7zrl\">Transformation</th>\n    <th class=\"tg-7zrl\">Hyperparameters &amp; Setup</th>\n    <th class=\"tg-7zrl\">Result</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">HorizontalFlip</td>\n    <td class=\"tg-7zrl\">Flips the image horizontally.</td>\n    <td class=\"tg-7zrl\">- p=0.7: Has a 70% probability of being applied.</td>\n    <td class=\"tg-7zrl\">A mirror image along the vertical axis.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">VerticalFlip</td>\n    <td class=\"tg-7zrl\">Flips the image vertically.</td>\n    <td class=\"tg-7zrl\">- p=0.7: Has a 70% probability of being applied.</td>\n    <td class=\"tg-7zrl\">An image flipped upside down.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Rotate</td>\n    <td class=\"tg-7zrl\">Rotates the image by a random angle within a given range.</td>\n    <td class=\"tg-7zrl\">- limit=180: The image is rotated randomly between -180 and +180 degrees. <br>- p=0.9: A 90% chance of rotation.</td>\n    <td class=\"tg-7zrl\">An image rotated to a new orientation.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">Affine</td>\n    <td class=\"tg-7zrl\">Applies a range of geometric distortions including translation, scaling, rotation, and shear in a single, efficient operation.</td>\n    <td class=\"tg-7zrl\">- translate_percent=(-0.2, 0.2): Randomly shifts the image up to 20% of its height/width. <br>- scale=(0.6, 1.4): Zooms in/out randomly from 60% to 140% of the original size. <br>- rotate=(-60, 60): Rotates the image randomly between -60 and +60 degrees.<br>- shear=(-15, 15): Skews the image randomly between -15 and +15 degrees.<br>- p=0.9: A 90% chance of applying the transformation.</td>\n    <td class=\"tg-7zrl\">A geometrically distorted image that simulates changes in viewpoint and camera angle.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">OpticalDistortion</td>\n    <td class=\"tg-7zrl\">Simulates lens distortions, such as barrel or pincushion effects. Used within OneOf.</td>\n    <td class=\"tg-7zrl\">- distort_limit=0.1: The range of the distortion coefficient. Higher values create more pronounced effects. <br>- p=1.0: Always applied if OpticalDistortion is selected by the parent OneOf.</td>\n    <td class=\"tg-7zrl\">An image with a warped, lens-like appearance.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">GridDistortion</td>\n    <td class=\"tg-7zrl\">Applies non-linear deformations by moving the grid points of a virtual grid placed over the image. Used within OneOf.</td>\n    <td class=\"tg-7zrl\">- num_steps=5: Number of grid cells on each side. <br>- distort_limit=0.1: The range of distortion. <br>- p=1.0: Always applied if selected by the parent OneOf.</td>\n    <td class=\"tg-7zrl\">A randomly warped or rippled image.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ElasticTransform</td>\n    <td class=\"tg-7zrl\">Creates smooth, elastic deformations by displacing pixels. Used within OneOf.</td>\n    <td class=\"tg-7zrl\">- it uses the defaults which apply a Gaussian filter to a random displacement field.</td>\n    <td class=\"tg-7zrl\">An image that appears stretched or warped like a rubber sheet.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ShiftScaleRotate</td>\n    <td class=\"tg-7zrl\">Combines shifting, scaling, and rotation into one operation. A more simplified version of Affine.</td>\n    <td class=\"tg-7zrl\">- shift_limit=0.1: The maximum fractional shift. <br>- scale_limit=0.1: The maximum fractional scale change. <br>- rotate_limit=45: The maximum rotation angle. <br>- p=0.9: A 90% chance of being applied.</td>\n    <td class=\"tg-7zrl\">A translated, scaled, and rotated image.</td>\n  </tr>\n</tbody></table>","metadata":{}},{"id":"7fc62fa6-638e-4b48-8be1-a608f50efd4f","cell_type":"markdown","source":"**Photometric Transformations (Color and Lighting Transformations)**","metadata":{}},{"id":"ce5b5abc-a34b-4b66-9511-a60aa26fc2d8","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Function</th>\n    <th class=\"tg-7zrl\">Transformation</th>\n    <th class=\"tg-7zrl\">Hyperparameters &amp; Setup</th>\n    <th class=\"tg-7zrl\">Result</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">RandomBrightnessContrast</td>\n    <td class=\"tg-7zrl\">Randomly adjusts the brightness and contrast of the image.</td>\n    <td class=\"tg-7zrl\">- brightness_limit=(-0.4, 0.4): Randomly adjusts brightness by a factor in this range. <br>- contrast_limit=(-0.4, 0.4): Randomly adjusts contrast by a factor in this range. <br>- p=0.9: A 90% chance of being applied.</td>\n    <td class=\"tg-7zrl\">An image that is either darker/lighter or has more/less contrast.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">HueSaturationValue</td>\n    <td class=\"tg-7zrl\">Randomly changes the hue, saturation, and value of the image's colors.</td>\n    <td class=\"tg-7zrl\">- hue_shift_limit=30: Max hue shift. <br>- sat_shift_limit=40: Max saturation shift. <br>- val_shift_limit=30: Max value (lightness) shift. <br>- p=0.9: A 90% chance of being applied.</td>\n    <td class=\"tg-7zrl\">An image with different color tones and vibrancy, simulating various lighting conditions or camera color settings.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">RandomGamma</td>\n    <td class=\"tg-7zrl\">Applies gamma correction, which non-linearly adjusts image intensity and brightness.</td>\n    <td class=\"tg-7zrl\">- gamma_limit=(60, 140): The range for gamma values. <br>- p=0.7: A 70% chance of being applied.</td>\n    <td class=\"tg-7zrl\">An image with a different overall tone curve, affecting its perceived brightness and shadows.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">CLAHE</td>\n    <td class=\"tg-7zrl\">Contrast Limited Adaptive Histogram Equalization. Enhances local contrast, particularly in areas that are either very dark or very bright.</td>\n    <td class=\"tg-7zrl\">- clip_limit=(2.0, 6.0): The threshold for contrast limiting. <br>- tile_grid_size=(8, 8): The size of the grid over which the equalization is performed. <br>- p=0.7: A 70% chance of being applied.</td>\n    <td class=\"tg-7zrl\">An image with improved local contrast, revealing details in underexposed or overexposed regions.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">RGBShift</td>\n    <td class=\"tg-7zrl\">Randomly shifts the values of the red, green, and blue channels independently.</td>\n    <td class=\"tg-7zrl\">- r_shift_limit=20, g_shift_limit=20, b_shift_limit=20: The maximum shift for each channel. <br>- p=0.7: A 70% chance of being applied.</td>\n    <td class=\"tg-7zrl\">An image with a slight color cast or a shift in its overall color balance.</td>\n  </tr>\n</tbody></table>","metadata":{}},{"id":"9f08cd60-3e11-4f28-a71e-be486720df25","cell_type":"markdown","source":"**Noise and Blurring**","metadata":{}},{"id":"4249d796-6a09-43a2-964e-7dc8c353ba9d","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Function</th>\n    <th class=\"tg-7zrl\">Transformation</th>\n    <th class=\"tg-7zrl\">Hyperparameters &amp; Setup</th>\n    <th class=\"tg-7zrl\">Result</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">GaussNoise</td>\n    <td class=\"tg-7zrl\">Adds random Gaussian noise to the image, simulating sensor noise.</td>\n    <td class=\"tg-7zrl\">- std_range=(0.1, 0.5): The standard deviation range for the noise. <br>- p=0.9: A 90% chance of being applied.</td>\n    <td class=\"tg-0lax\">A grainy, noisy image.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">GaussianBlur</td>\n    <td class=\"tg-7zrl\">Blurs the image using a Gaussian kernel, simulating an out-of-focus effect.</td>\n    <td class=\"tg-7zrl\">- blur_limit=(3, 9): The kernel size for blurring. A larger value results in more blur. <br>- p=0.8: An 80% chance of being applied.</td>\n    <td class=\"tg-0lax\">A softer, less-focused image.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"id":"6b9b164f-7f9a-4ec3-b737-4ea7d9b3e963","cell_type":"markdown","source":"**Distortion**","metadata":{}},{"id":"1fad3d66-df09-4f7b-bf3c-1d85fe34489d","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Function</th>\n    <th class=\"tg-7zrl\">Transformation</th>\n    <th class=\"tg-7zrl\">Hyperparameters &amp; Setup</th>\n    <th class=\"tg-7zrl\">Result</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">OneOf</td>\n    <td class=\"tg-7zrl\">Selects and applies exactly one transformation from a list of possibilities.</td>\n    <td class=\"tg-7zrl\">- p=0.5: A 50% chance of applying one of the contained transforms.</td>\n    <td class=\"tg-0lax\">Exactly one of the three geometric distortions (OpticalDistortion, GridDistortion, or ElasticTransform) is applied.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"id":"b76d272b-7531-4a16-b566-87492ddadc74","cell_type":"markdown","source":"**Normalization**","metadata":{}},{"id":"409f2c12-d49b-46a5-b4e2-58e7411e205a","cell_type":"markdown","source":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-k9u1{border-color:inherit;color:#1B1C1D;font-size:100%;text-align:left;vertical-align:bottom}\n.tg .tg-7zrl{text-align:left;vertical-align:bottom}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-k9u1\">Function</th>\n    <th class=\"tg-7zrl\">Transformation</th>\n    <th class=\"tg-7zrl\">Hyperparameters &amp; Setup</th>\n    <th class=\"tg-7zrl\">Result</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-7zrl\">Normalize</td>\n    <td class=\"tg-7zrl\">Normalizes the image's pixel values to a standard range, typically a mean of 0 and a standard deviation of 1.</td>\n    <td class=\"tg-7zrl\">- mean=(0.485, 0.456, 0.406): Mean values for each channel (R, G, B) from the ImageNet dataset. <br>- std=(0.229, 0.224, 0.225): Standard deviation for each channel from the ImageNet dataset.</td>\n    <td class=\"tg-0lax\">An image with pixel values converted to a standardized floating-point representation, which helps deep learning models converge faster.</td>\n  </tr>\n</tbody>\n</table>","metadata":{}},{"id":"cca3c924-20a9-4bea-b5fe-0114b6c84f25","cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"id":"b9e8898a-07fb-48c4-a9bd-2e3f4753c56f","cell_type":"markdown","source":"The `MultiSizeDataset` is a custom PyTorch Dataset designed to efficiently handle data for multiple deep learning models, particularly for ensemble learning. It creates a dictionary of image transformation pipelines in its constructor, dynamically selecting between randomized data augmentation for training and deterministic transformations for validation. At runtime, it loads a single source image and applies each of the specified transformations, returning a dictionary of preprocessed image tensors along with the corresponding label. This approach streamlines the data pipeline for multi-model projects, leveraging key concepts like transfer learning and regularization to enhance model performance and robustness.","metadata":{}},{"id":"8ad6aba7-40ff-4774-a0b0-1de59595bf94","cell_type":"markdown","source":"**Theories and Concepts**\n\nThe design of the `MultiSizeDataset` class is based on several key principles in modern deep learning:\n\n- **Data Preparation for Transfer Learning and Ensemble Modeling:** This step is structured to support a workflow that leverages pre-trained models (like `DINOv2 and Hiera`). These models often have specific input size requirements, which this class accommodates. By training and combining the predictions of multiple diverse models, a practice known as ensemble modeling, higher accuracy and better generalization can be achieved than with a single model.\n- **Data Augmentation as Regularization:** This step's use of distinct training and validation transformation pipelines is a practical application of regularization. Randomized augmentations during training prevent the model from overfitting by forcing it to learn features that are invariant to common visual changes. The deterministic validation transformations, in contrast, ensure a fair and consistent evaluation of the model's performance on a fixed set of data.\n- **Modularity:** Implementing this as a custom Dataset class adheres to the software engineering principle of modularity. It cleanly separates the data loading and preprocessing logic from the model training loop, making the code more organized, reusable, and easier to debug.","metadata":{}},{"id":"accb2706-776e-4011-940e-1b7f6888c075","cell_type":"markdown","source":"**How the Step Works**\n\nThis step defines a custom Dataset class, a core component of PyTorch. The class's functionality is split across three main methods:\n\n- **`__init__`:** The constructor sets up the dataset. It takes a DataFrame (likely containing image file names and labels), the data directory, and a flag indicating whether the dataset is for training or validation. Crucially, it initializes a dictionary called `self.transforms`. This dictionary maps model names (dinov2, hiera, etc.) to their specific image transformation pipelines, which are chosen based on the `is_train` flag.\n- **`__len__`:** This simple method returns the total number of images in the dataset, which is used by PyTorch's DataLoader to know when to stop iterating.\n- **`__getitem__`:** This is the heart of the class. It takes an index, loads the corresponding image from disk, and then applies each of the transformation pipelines stored in `self.transforms`. The output is a dictionary where each key is a model name and the value is the fully preprocessed image tensor for that model. This allows a single data loader to supply correctly formatted data to multiple models simultaneously.","metadata":{}},{"id":"325c88b0-2b5d-455b-81b9-ab25a76928ff","cell_type":"code","source":"class MultiSizeDataset(Dataset):\n    def __init__(self, df, data_dir, is_train=True):\n        self.df = df\n        self.data_dir = data_dir\n        self.is_train = is_train\n        self.transforms = {\n            'dinov2': get_train_transform(DINO_SIZE) if is_train else get_val_transform(DINO_SIZE),\n            'hiera': get_train_transform(HIERA_SIZE) if is_train else get_val_transform(HIERA_SIZE),\n            'effnet': get_train_transform(EFFNET_SIZE) if is_train else get_val_transform(EFFNET_SIZE),\n        }\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = self.df.iloc[idx]['file_name']\n        label = self.df.iloc[idx]['label']\n        img_path = os.path.join(self.data_dir, filename)\n        \n        try:\n            image = Image.open(img_path).convert('RGB')\n            image = np.array(image)\n        except Exception as e:\n            print(f\"Warning: Failed to load image {img_path}: {e}\")\n            max_size = max(DINO_SIZE, EFFNET_SIZE, HIERA_SIZE)\n            image = np.zeros((max_size, max_size, 3), dtype=np.uint8)\n\n        transformed = {\n            model: transform(image=image)['image']\n            for model, transform in self.transforms.items()\n        }\n        return transformed, label\n\n# Data preparation\n# Data preparation\ntrain_csv_path = \"/root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4/train.csv\"\ntrain_df = pd.read_csv(train_csv_path)\ntrain_data_dir = \"/root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4\"\n# train_df = train_df.sample(frac=0.05)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=SEED, stratify=train_df['label'])\n\ntrain_dataset = MultiSizeDataset(train_df, train_data_dir, is_train=True)\nval_dataset = MultiSizeDataset(val_df, train_data_dir, is_train=False)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n","  original_init(self, **validated_kwargs)\n"]}],"execution_count":16},{"id":"6e1f1b96-bbc5-473e-9588-d242d3f51812","cell_type":"markdown","source":"**Hyperparameters Description**\n\nThis step hasthree main hyperparameters that are critical to its function:\n\n- **`DINO_SIZE, HIERA_SIZE, EFFNET_SIZE`:** These are the specific integer dimensions (e.g., 224, 384, 518) that each model architecture requires as input. These values are crucial hyperparameters for pre-trained models and are essential for ensuring the images are correctly scaled for each network. They represent the target square size for the input image tensors.","metadata":{}},{"id":"678fcf0b-40e9-4c4f-b534-9b1ba2667ac9","cell_type":"markdown","source":"# Model Training","metadata":{}},{"id":"e58a1a1f-2ca0-48c7-8522-5f39fd19eddf","cell_type":"markdown","source":"In this step, it defines and implements a complete fine-tuning and training pipeline for a deep learning model ensemble. The primary goal is to combine the predictive power of multiple pre-trained models, in this case `DINOv2, Hiera, and EfficientNet`, to achieve superior performance on a specific task. This approach streamlines the process of loading, training, and evaluating a complex multi-model system.","metadata":{}},{"id":"00fb21a8-5d2a-490c-98bf-55acd553e128","cell_type":"markdown","source":"**Theories and concepts**\n\nThe architecture of this code is based on several key theoretical concepts in deep learning:\n\n- **Ensemble Modeling:** The fundamental idea is that a group of diverse models, each with a different architecture, can collectively outperform a single model. The EnsembleModel class facilitates this by combining the outputs of three distinct networks (`ViT-based DINOv2, hierarchical Hiera, and EfficientNet`) for a single final prediction. The diversity of these models (e.g., Transformer vs. CNN-based architectures) helps them learn complementary features, leading to higher accuracy and better generalization.\n\n- **Transfer Learning:** Instead of training the models from scratch, which would be computationally expensive and require massive datasets, the code leverages transfer learning. It uses pre-trained models from timm that have learned powerful features from large-scale image datasets. The training process then becomes a fine-tuning task, where the models adapt their learned knowledge to the new, specific dataset. This significantly reduces training time and improves performance, especially on smaller datasets.\n\n- **Optimization and Regularization:** The code incorporates several techniques to ensure stable and efficient training:\n\n  - **Gradient Clipping:** This is a regularization technique that helps prevent the \"exploding gradient\" problem, where gradients become too large and cause the model's weights to become unstable.\n  - **Mixed Precision Training:** By performing calculations in half-precision (16-bit) floating-point format where possible, this technique reduces the training time and memory footprint on modern GPUs, making it possible to use larger batch sizes.\n  - **Learning Rate Schedulers:** The `LambdaLR` scheduler adjusts the learning rate over time, allowing for a higher learning rate at the beginning of training to explore the parameter space quickly and a lower learning rate later on to fine-tune the weights more carefully.","metadata":{}},{"id":"a43fb891-d694-4b3b-8aa2-8c287630d807","cell_type":"markdown","source":"**Models and Hyperparameters**\n\nThe code uses three powerful and distinct deep learning models, each with specific hyperparameters:\n\n- **`DINOv2`:** A Vision Transformer (ViT) model, specifically `'vit_small_patch14_dinov2.lvd142m'`. It is trained using a self-supervised learning method, meaning it learns robust representations from unlabeled data.\n  - **`num_classes=2`:** This hyperparameter sets the size of the final classification layer, adapting the model for a binary classification task.\n  - **`img_size=DINO_SIZE`:** The required input image dimensions for this specific model variant.\n- **`Hiera`:** A Hierarchical Vision Transformer, identified as `'hiera_small_224'`. It combines the local feature-learning of CNNs with the global context-awareness of Vision Transformers, resulting in an efficient and high-performing architecture.\n  - **`num_classes=2`:** Configures the model for a binary classification task.\n- **`EfficientNetV2`:** A state-of-the-art Convolutional Neural Network (CNN) known for its excellent balance of accuracy and efficiency. The model `'tf_efficientnetv2_s.in21k_ft_in1k'` is a \"small\" variant that was pre-trained on two different large-scale ImageNet datasets.\n  - **`num_classes=2`:** Sets the model's output layer for binary classification.","metadata":{}},{"id":"cf3d4a5e-00b0-443e-929f-a0644eca82f6","cell_type":"markdown","source":"**Overall Step-By-Step breakdown**\n\nThis step consists of three main parts: an `EnsembleModel class`, a `train_phase` function, and several utility functions.\n\n- **The Train Phase:** This function orchestrates the entire fine-tuning process. It runs for a specified number of epochs, with each epoch consisting of a training loop and a validation loop.\n\n  - **Training Loop:** The model is set to training mode (`model.train()`), and gradients are zeroed for each batch. The forward pass is executed to get predictions from all three models. A combined loss is calculated by averaging the loss from each model. The backward pass computes gradients, which are then used to update the model weights using the configured optimizers. This phase also incorporates mixed-precision training using `torch.amp.GradScaler` for faster computation and lower memory usage, and gradient clipping to prevent exploding gradients.\n  - **Validation Loop:** The model is switched to evaluation mode (`model.eval()`) to disable layers like dropout. The code then runs a forward pass on the validation dataset without computing gradients (`with torch.no_grad()`). It calculates the validation loss and accuracy, and a check is included to skip batches with a NaN loss, which enhances stability.\n  - **Checkpointing:** After each epoch, the function compares the current validation accuracy to the best accuracy seen so far. If a new best is found, it saves the state of the entire ensemble model, as well as each individual model, to a file. This ensures the best-performing model is preserved.\n\n\n- **Utility Functions:** The `load_checkpoint` function is a helper that loads a saved model state, and the `visualize_history` function uses matplotlib to plot the training/validation loss and accuracy curves, providing a clear visual representation of the fine-tuning process.\n\n- **The Ensemble Model:** This class is a PyTorch `nn.Module` that acts as a container for the individual deep learning models. In its constructor (`__init__`) it initializes and loads three models from the timm library, each with a pre-trained checkpoint if available. The `create_model` helper function is responsible for this instantiation and for handling different checkpoint formats. The forward method is the heart of the class, taking a dictionary of inputs (where each key corresponds to a model name) and returning a list of outputs, one from each model.","metadata":{}},{"id":"0670b484-cbdc-4d46-af18-30709a8a1f09","cell_type":"code","source":"# Model Constants\nFINE_TUNE_EPOCHS = 5                                             # Example for illustration, actually I saved model then loaded model and continue fined-tune around 80 Epochs\n\n# Learning rate schedule\nFINE_TUNE_LRS = [5e-5, 5e-5, 5e-5, 1e-5, 1e-5, 1e-5, 5e-6, 5e-6] # Fine-tuning schedule\n\ndef lrfn_factory(lr_list):\n    \"\"\"Factory function to create learning rate schedulers\"\"\"\n    def lr_fn(epoch):\n        return lr_list[epoch] if epoch < len(lr_list) else lr_list[-1]\n    return lr_fn","metadata":{"tags":[]},"outputs":[],"execution_count":17},{"id":"5655418f-6286-497e-b877-731d9a6151bb","cell_type":"markdown","source":"## Weak learner models","metadata":{}},{"id":"e3bccad6-16ee-4039-98ca-3072333593bd","cell_type":"markdown","source":"The function operates over a specified number of epochs, with each epoch consisting of a training loop and a validation loop:\n\n**Training Loop:**\n\n- The model is switched to training mode (`model.train()`) to enable features like dropout.\n- The code iterates through mini-batches of data from the training data loader.\n- For each batch, it performs a forward pass to get predictions, calculates the loss, and then performs a backward pass to compute gradients.\n- It applies gradient clipping to the gradients to prevent them from becoming too large and unstable.\n- The optimizers update the model's weights based on the computed gradients.\n- It also utilizes mixed-precision training if a compatible GPU is available, which speeds up computation and reduces memory usage.\n\n**Validation Loop:**\n\n- The model is set to evaluation mode (`model.eval()`) to disable training-specific layers.\n- The code iterates through the validation data without computing gradients, which saves time.\n- It calculates the validation loss and accuracy to evaluate the model's performance on unseen data.\n- A check for NaN (Not a Number) loss is included to ensure numerical stability.\n\n**Checkpointing:**\n\n- At the end of each epoch, the function compares the current validation accuracy to the best accuracy achieved so far.\n- If the current accuracy is better, it saves the model's state (its weights) to disk. This is a critical step to ensure that the best-performing model is preserved, even if subsequent epochs result in overfitting.","metadata":{}},{"id":"16cb4b14-0eab-454c-84d0-5b13283a8470","cell_type":"code","source":"# Modified train_phase with gradient clipping, stability checks, and early stopping\ndef train_phase(model, train_loader, val_loader, optimizers, schedulers, \n               loss_fn, device, total_epochs=FINE_TUNE_EPOCHS, checkpoint_path='best_model.pth',\n               dinov2_path='best_dinov2.pth', hiera_path='best_hiera.pth', effnet_path='best_effnet.pth',\n               patience=10):\n    model.to(device)\n    best_acc = 0.0\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n    max_grad_norm = 1.0  # Gradient clipping threshold\n    epochs_no_improve = 0\n\n    for epoch in range(total_epochs):\n        print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n        print(\"-\" * 50)\n        \n        # Training\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch in tqdm(train_loader, desc=\"Training\"):\n            inputs, labels = batch\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            labels = labels.to(device)\n            \n            # Zero gradients\n            for optimizer in optimizers.values():\n                optimizer.zero_grad()\n            \n            # Forward pass with mixed precision\n            with torch.cuda.amp.autocast():\n                outputs = model(inputs)\n                loss = sum(loss_fn(out, labels) for out in outputs) / 3.0\n            \n            # Backward pass with gradient clipping\n            scaler.scale(loss).backward()\n            for optimizer in optimizers.values():\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # Metrics\n            train_loss += loss.item()\n            with torch.no_grad():\n                _, predicted = torch.max(torch.mean(torch.stack(outputs), dim=0), 1)\n                correct += (predicted == labels).sum().item()\n                total += labels.size(0)\n        \n        # Step schedulers after epoch\n        for scheduler in schedulers.values():\n            scheduler.step()\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validating\"):\n                inputs, labels = batch\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                labels = labels.to(device)\n                \n                outputs = model(inputs)\n                loss = sum(loss_fn(out, labels) for out in outputs) / 3.0\n                \n                # Check for NaN\n                if torch.isnan(loss):\n                    print(\"Warning: NaN loss detected, skipping batch\")\n                    continue\n                \n                val_loss += loss.item()\n                _, predicted = torch.max(torch.mean(torch.stack(outputs), dim=0), 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n        \n        # Calculate metrics\n        train_loss = train_loss / len(train_loader)\n        train_acc = correct / total\n        val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else float('nan')\n        val_acc = val_correct / val_total if val_total > 0 else 0.0\n        \n        # Update history\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n        \n        # Save best model\n        if not np.isnan(val_acc) and val_acc > best_acc:\n            best_acc = val_acc\n            epochs_no_improve = 0\n            torch.save({\n                'model_state': model.state_dict(),\n                'optimizers': {k: v.state_dict() for k, v in optimizers.items()},\n                'best_acc': best_acc,\n                'epoch': epoch\n            }, checkpoint_path)\n            # Save individual models\n            torch.save(model.dinov2.state_dict(), dinov2_path)\n            torch.save(model.hiera.state_dict(), hiera_path)\n            torch.save(model.effnet.state_dict(), effnet_path)\n            print(f\"Saved models with accuracy: {best_acc:.4f}\")\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping at epoch {epoch+1}: no improvement for {patience} epochs.\")\n                break\n    \n    return history","metadata":{"tags":[]},"outputs":[],"execution_count":18},{"id":"6992064a-f5a8-4742-8729-364438ecccdf","cell_type":"markdown","source":"## Ensemble Model","metadata":{}},{"id":"06224953-5d28-41b4-a8e0-5c83fdf19b23","cell_type":"markdown","source":"The `EnsembleModel class` is a custom PyTorch `nn.Module` designed for ensemble learning. It acts as a container for multiple pre-trained deep learning models, allowing them to be managed and trained as a single unit. This class is crucial for leveraging the diverse strengths of different model architectures to achieve superior performance and robustness on a given task.","metadata":{}},{"id":"cf749e38-90f4-4764-a39c-86adcd452709","cell_type":"markdown","source":"The process involves three primary methods:\n\n- **Constructor (`__init__`):** This method initializes the three constituent models. It calls a helper function, `create_model`, for each of the pre-selected architectures: DINOv2, Hiera, and EfficientNet. This step sets up each model as a sub-module of the EnsembleModel.\n  \n- **Helper Function (`create_model`):** This utility handles the creation and loading of each individual model. It uses the timm library, a popular resource for computer vision models, to instantiate a model by its name. It checks if a pre-existing checkpoint path is provided. If not, it loads the standard pre-trained weights for the model. If a checkpoint is given, it loads the saved state, enabling continuous training or inference from a previously saved point.\n  \n- **Forward Pass (`forward`):** This method defines the data flow. It accepts a dictionary of input tensors, one for each model, and passes each tensor through its corresponding model. The outputs of all three models are then returned as a list, ready for further processing, such as combining their predictions for a final classification.","metadata":{}},{"id":"bb1b4504-20cc-4a32-93e9-95a48a0407cd","cell_type":"code","source":"def load_checkpoint(model, checkpoint_path, device):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    if 'model_state' in checkpoint:\n        model.load_state_dict(checkpoint['model_state'], strict=False)\n    else:\n        # Handle case where checkpoint is direct state dict\n        model.load_state_dict(checkpoint, strict=False)\n    model.to(device)\n    print(f\"Loaded checkpoint from {checkpoint_path}\")\n    return model\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, dinov2_path=None, hiera_path=None, effnet_path=None):\n        super().__init__()\n        # Use valid model names for timm.create_model()\n        self.dinov2 = self._create_model(\n            'vit_small_patch14_dinov2.lvd142m',  # Correct model name\n            dinov2_path,\n            num_classes=2,\n            img_size=DINO_SIZE\n        )\n        self.hiera = self._create_model(\n            'hiera_small_224',  # Correct model name\n            hiera_path,\n            num_classes=2\n        )\n        self.effnet = self._create_model(\n            'tf_efficientnetv2_s.in21k_ft_in1k',  # Correct model name\n            effnet_path,\n            num_classes=2\n        )\n\n    def _create_model(self, model_name, checkpoint_path, num_classes, **kwargs):\n        # Create the model using timm\n        model = timm.create_model(\n            model_name,\n            pretrained=checkpoint_path is None,  # Use pretrained weights if no checkpoint\n            num_classes=num_classes,\n            **kwargs\n        )\n        \n        # Load checkpoint if provided\n        if checkpoint_path:\n            if not os.path.exists(checkpoint_path):\n                raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n            state_dict = torch.load(checkpoint_path, map_location=DEVICE)\n            \n            # Handle different checkpoint formats\n            if 'model_state' in state_dict:\n                state_dict = state_dict['model_state']\n            elif 'state_dict' in state_dict:\n                state_dict = state_dict['state_dict']\n            \n            # Load state dict\n            model.load_state_dict(state_dict, strict=False)\n            print(f\"Loaded {model_name} from {checkpoint_path}\")\n        \n        return model\n\n    def forward(self, x_dict):\n        return [\n            self.dinov2(x_dict['dinov2']),\n            self.hiera(x_dict['hiera']),\n            self.effnet(x_dict['effnet'])\n        ]","metadata":{"tags":[]},"outputs":[],"execution_count":19},{"id":"943d930b-4a10-4203-b123-33f2875bf17a","cell_type":"markdown","source":"## Fine-tuning weak learner models and ensemble model","metadata":{}},{"id":"db7c3c74-ed38-49b9-a3fd-2a08e23a82c8","cell_type":"markdown","source":"This code block is a complete fine-tuning pipeline for a deep learning model ensemble. It sets up the entire training and validation process, from loading the pre-trained models to saving the best performing checkpoint. The step is essential for adapting the powerful, general features learned by the models on massive datasets to a specific, more focused task.\n\n- **Model Loading:** The code begins by creating an instance of the `EnsembleModel class`. By passing None to the `dinov2_path, hiera_path, and effnet_path` arguments, the model will automatically load the official pre-trained weights from the timm library. This is a crucial step in transfer learning, as it provides a strong starting point for the fine-tuning process.\n\n- **Optimizer and Scheduler Setup:**\n  - **Optimizers:** The optimizers dictionary is created, assigning an `AdamW` optimizer to the parameters of each individual model within the ensemble. `AdamW` is a popular optimizer known for its effectiveness and for separating weight decay from the L2 regularization in the loss function.\n  - **Schedulers:** A `LambdaLR` scheduler is created for each optimizer. This object is responsible for dynamically adjusting the learning rate during training, typically by decreasing it over time.\n\n- **Loss Function:** The `loss_fn` is set to `nn.CrossEntropyLoss()`, which is a standard and effective loss function for classification problems.\n\n- **Fine-Tuning Execution:** The `train_phase` function is then called with all the prepared components (model, dataloaders, optimizers, etc.). This function executes the main training loop, where the model's weights are iteratively updated to minimize the loss and improve accuracy on the training data.\n\n- **Performance Visualization:** After training is complete, the `visualize_history` function is called to generate and display plots of the training and validation loss and accuracy over time. This provides a clear visual summary of the model's learning progress and helps identify issues like overfitting.","metadata":{}},{"id":"06c85e71-d3d9-4ed1-a629-a27ffc25fbb8","cell_type":"code","source":"# Load pre-trained model\nprint(\"Loading pre-trained model...\")\nmodel = EnsembleModel(\n    dinov2_path=None,  # Path to DINOv2 checkpoint\n    hiera_path=None,    # Path to Hiera checkpoint\n    effnet_path=None   # Path to EfficientNet checkpoint\n)\n# If you have a saved checkpoint and want to resume training,\n# uncomment the two lines below and comment out the 'model = EnsembleModel(...)' line above.\n# checkpoint_path = 'fine_tuned_ensemble.pth'\n# model = load_checkpoint(model, checkpoint_path, DEVICE)\n\n# Set up fine-tuning components\noptimizers = {\n    'dinov2': AdamW(model.dinov2.parameters(), lr=FINE_TUNE_LRS[0], weight_decay=1e-4),\n    'hiera': AdamW(model.hiera.parameters(), lr=FINE_TUNE_LRS[0], weight_decay=1e-4),\n    'effnet': AdamW(model.effnet.parameters(), lr=FINE_TUNE_LRS[0], weight_decay=1e-4)\n}\n\nschedulers = {\n    name: LambdaLR(optimizer, lr_lambda=lrfn_factory(FINE_TUNE_LRS))\n    for name, optimizer in optimizers.items()\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\n# Start fine-tuning\nprint(\"Starting fine-tuning...\")\nhistory = train_phase(\n    model=model,\n    train_loader=train_dataloader,\n    val_loader=val_dataloader,\n    optimizers=optimizers,\n    schedulers=schedulers,\n    loss_fn=loss_fn,\n    device=DEVICE,\n    total_epochs=FINE_TUNE_EPOCHS,\n    checkpoint_path='fine_tuned_ensemble.pth',\n    dinov2_path='best_dinov2.pth',  # Path to save DINOv2\n    hiera_path='best_hiera.pth',    # Path to save Hiera\n    effnet_path='best_effnet.pth',   # Path to save EfficientNet\n    patience=3,  # Stop if validation loss doesn't improve for 10 epochs\n)\n\nprint(\"Fine-tuning complete.\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-trained model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4a59ed6d5f549b89e983ea52f13db25","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3b5b557183a4fcd888ce66cc0656599","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/140M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9b054c7f17541f98f05223b28f24898","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Starting fine-tuning...\n","\n","Epoch 1/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [31:49<00:00,  4.19it/s]\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","Validating: 100%|██████████| 1999/1999 [06:55<00:00,  4.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8916 | Acc: 0.5327\n","Val Loss: 1.6335 | Acc: 0.5503\n","Saved models with accuracy: 0.5503\n","\n","Epoch 2/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [31:51<00:00,  4.18it/s]\n","Validating: 100%|██████████| 1999/1999 [06:56<00:00,  4.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8960 | Acc: 0.5332\n","Val Loss: 1.6190 | Acc: 0.5506\n","Saved models with accuracy: 0.5506\n","\n","Epoch 3/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [31:48<00:00,  4.19it/s]\n","Validating: 100%|██████████| 1999/1999 [06:55<00:00,  4.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8922 | Acc: 0.5348\n","Val Loss: 1.6059 | Acc: 0.5634\n","Saved models with accuracy: 0.5634\n","\n","Epoch 4/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [31:50<00:00,  4.19it/s]\n","Validating: 100%|██████████| 1999/1999 [06:55<00:00,  4.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8840 | Acc: 0.5324\n","Val Loss: 1.6507 | Acc: 0.5493\n","\n","Epoch 5/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [31:42<00:00,  4.20it/s]\n","Validating: 100%|██████████| 1999/1999 [06:53<00:00,  4.84it/s]"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8808 | Acc: 0.5357\n","Val Loss: 1.6464 | Acc: 0.5523\n","Fine-tuning complete.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"execution_count":20},{"id":"600e80ae-8aae-460f-9c18-627e23e1ad87","cell_type":"code","source":"# Load pre-trained model\nprint(\"Loading pre-trained model...\")\nmodel = EnsembleModel(\n    dinov2_path='best_dinov2.pth', # Path to DINOv2 checkpoint\n    hiera_path='best_hiera.pth',   # Path to Hiera checkpoint\n    effnet_path='best_effnet.pth'  # Path to EfficientNet checkpoin\n)\n\n# Load ensemble checkpoint if it exists\ncheckpoint_path = 'fine_tuned_ensemble.pth'\nmodel = load_checkpoint(model, checkpoint_path, DEVICE)\n\n# Set up fine-tuning components\noptimizers = {\n    'dinov2': AdamW(model.dinov2.parameters(), lr=FINE_TUNE_LRS[0], weight_decay=1e-4),\n    'hiera': AdamW(model.hiera.parameters(), lr=FINE_TUNE_LRS[0], weight_decay=1e-4),\n    'effnet': AdamW(model.effnet.parameters(), lr=FINE_TUNE_LRS[0], weight_decay=1e-4)\n}\n\nschedulers = {\n    name: LambdaLR(optimizer, lr_lambda=lrfn_factory(FINE_TUNE_LRS))\n    for name, optimizer in optimizers.items()\n}\n\nloss_fn = nn.CrossEntropyLoss()\n\n# Start fine-tuning\nprint(\"Starting fine-tuning...\")\nhistory = train_phase(\n    model=model,\n    train_loader=train_dataloader,\n    val_loader=val_dataloader,\n    optimizers=optimizers,\n    schedulers=schedulers,\n    loss_fn=loss_fn,\n    device=DEVICE,\n    total_epochs=FINE_TUNE_EPOCHS,\n    checkpoint_path='fine_tuned_ensemble.pth',\n    dinov2_path='best_dinov2.pth',  # Path to save DINOv2\n    hiera_path='best_hiera.pth',    # Path to save Hiera\n    effnet_path='best_effnet.pth',   # Path to save EfficientNet\n    patience=3,  # Stop if validation loss doesn't improve for 10 epochs\n)\n\nprint(\"Fine-tuning complete.\")","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pre-trained model...\n","Loaded vit_small_patch14_dinov2.lvd142m from best_dinov2.pth\n","Loaded hiera_small_224 from best_hiera.pth\n","Loaded tf_efficientnetv2_s.in21k_ft_in1k from best_effnet.pth\n","Loaded checkpoint from fine_tuned_ensemble.pth\n","Starting fine-tuning...\n","\n","Epoch 1/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [33:39<00:00,  3.96it/s]\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","Validating: 100%|██████████| 1999/1999 [07:18<00:00,  4.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8770 | Acc: 0.5345\n","Val Loss: 1.5536 | Acc: 0.5734\n","Saved models with accuracy: 0.5734\n","\n","Epoch 2/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [34:02<00:00,  3.92it/s]\n","Validating: 100%|██████████| 1999/1999 [07:20<00:00,  4.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8815 | Acc: 0.5320\n","Val Loss: 1.5552 | Acc: 0.5687\n","\n","Epoch 3/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [33:50<00:00,  3.94it/s]\n","Validating: 100%|██████████| 1999/1999 [07:22<00:00,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8702 | Acc: 0.5333\n","Val Loss: 1.4945 | Acc: 0.5792\n","Saved models with accuracy: 0.5792\n","\n","Epoch 4/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [33:47<00:00,  3.94it/s]\n","Validating: 100%|██████████| 1999/1999 [07:23<00:00,  4.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8540 | Acc: 0.5400\n","Val Loss: 1.5796 | Acc: 0.5694\n","\n","Epoch 5/5\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 7995/7995 [34:25<00:00,  3.87it/s]\n","Validating: 100%|██████████| 1999/1999 [07:41<00:00,  4.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 1.8599 | Acc: 0.5380\n","Val Loss: 1.5014 | Acc: 0.5851\n","Saved models with accuracy: 0.5851\n","Fine-tuning complete.\n"]}],"execution_count":20},{"id":"e6919147-08e4-48ea-b5fb-8778a0f22988","cell_type":"markdown","source":"## Visualize Model Training","metadata":{}},{"id":"74dc7343-0c32-4d1d-9c8e-3b19d13cb790","cell_type":"code","source":"def visualize_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n    plt.title(\"Training/Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \nvisualize_history(history)","metadata":{"tags":[]},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"execution_count":21},{"id":"d6e3f159-c15c-4035-a7a2-3798f51b76ad","cell_type":"markdown","source":"# Prediction","metadata":{}},{"id":"e369ee79-f77f-43d3-9f36-c40e0da67554","cell_type":"markdown","source":"The final step is the inference phase of a deep learning project. It outlines the process of using a previously trained model to make predictions on a new, unseen test dataset and prepare the results for submission.","metadata":{}},{"id":"3b3fbe14-0bfc-4dbc-83d5-5994d0a3fa70","cell_type":"markdown","source":"- **Data Loading:** The code first loads the `test.csv` file using the pandas library, which contains the unique identifiers (id) for each test image. It then uses these identifiers to construct the full file paths to where the images are stored on disk.\n\n- **Inference Loop:** The code iterates through each row of the test data. For each image:\n  - **Preprocessing:** The image is opened, converted to RGB format, and then transformed using a `val_transform`. This step is crucial as it ensures the image is correctly resized, normalized, and converted into a tensor format that the model can understand. The `.unsqueeze(0)` call adds a batch dimension to the tensor, making it compatible with the model's expected input shape.\n  - **Prediction:** The preprocessed image tensor is passed through the trained model. The with `torch.no_grad()` context manager is used to disable gradient calculations, which speeds up inference and saves memory, as no model updates are needed. The output, which typically represents the raw prediction scores (logits), is then converted into a final class label using `torch.argmax()`.\n\n- **Output Generation:** The image identifier and its predicted label are appended to a list. After all images have been processed, this list is converted into a pandas DataFrame.\n\n- **Submission:** Finally, the DataFrame is saved to a `submission.csv` file. This file contains the id and the predicted label for each image, formatted for submission to a competition or for further analysis.","metadata":{}},{"id":"7ea09c51-72bc-4977-aaf3-5896e5ac6743","cell_type":"markdown","source":"**Theories and concepts**\n\n- **Inference:** This step is a practical application of inference in machine learning, which is the process of using a trained model to make predictions on new data. Unlike the training phase where model weights are updated, inference is a forward-only operation.\n- **Data Preprocessing:** The application of a consistent `val_transform` is a core principle. A model's performance is highly dependent on the quality and consistency of its input data. The transformations applied (e.g., `resizing, normalization`) ensure the test data is in the same format as the training data, which is critical for accurate predictions.\n- **Memory and Performance Optimization:** The use of `torch.no_grad()` is an optimization technique. During training, deep learning frameworks track operations to compute gradients for backpropagation. For inference, this tracking is unnecessary. By disabling it, the code significantly reduces memory consumption and speeds up the entire prediction process.","metadata":{}},{"id":"ed8375dd-b031-4af4-b992-a87627025848","cell_type":"markdown","source":"## Test data loading","metadata":{}},{"id":"54623a60-66df-4f9a-bbab-340b3884c262","cell_type":"code","source":"# Load the test CSV for predictions\ntest_csv_path = \"/root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4/test.csv\"\ntest_df = pd.read_csv(test_csv_path)\n\n# Define the base directory where test images are stored\ntest_data_dir = \"/root/.cache/kagglehub/datasets/alessandrasala79/ai-vs-human-generated-dataset/versions/4/test_data_v2\"\n\n# Update image paths in the test dataframe\n# The 'id' column contains paths like 'test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg'\n# Extract the filename and join with test_data_dir\ntest_df['file_name'] = test_df['id'].apply(lambda x: os.path.join(test_data_dir, os.path.basename(x)))","metadata":{"tags":[]},"outputs":[],"execution_count":22},{"id":"26113650-5235-4748-bee6-b619155a5716","cell_type":"code","source":"def get_val_transform(size):\n    return albu.Compose([\n        albu.LongestMaxSize(size),\n        albu.PadIfNeeded(size, size, border_mode=0),\n        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ])\n\n# Update MultiSizeDataset to handle test data without labels\nclass MultiSizeDataset(Dataset):\n    def __init__(self, df, data_dir, is_train=True):\n        self.df = df\n        self.data_dir = data_dir\n        self.is_train = is_train\n        self.transforms = {\n            'dinov2': get_train_transform(DINO_SIZE) if is_train else get_val_transform(DINO_SIZE),\n            'hiera': get_train_transform(HIERA_SIZE) if is_train else get_val_transform(HIERA_SIZE),\n            'effnet': get_train_transform(EFFNET_SIZE) if is_train else get_val_transform(EFFNET_SIZE),\n        }\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = self.df.iloc[idx]['file_name']\n        label = self.df.iloc[idx].get('label', -1)  # Use .get() for safety\n        img_path = filename  # Use the pre-computed full path directly\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image = np.array(image)\n        except Exception as e:\n            print(f\"Warning: Failed to load image {img_path}: {e}\")\n            max_size = max(DINO_SIZE, EFFNET_SIZE, HIERA_SIZE)\n            image = np.zeros((max_size, max_size, 3), dtype=np.uint8)\n\n        transformed = {\n            model: transform(image=image)['image']\n            for model, transform in self.transforms.items()\n        }\n        return transformed, label\n\n# Custom collate function to handle None labels\ndef custom_collate(batch):\n    # Unzip the batch into inputs and labels\n    inputs = [item[0] for item in batch]  # List of transformed dictionaries\n    labels = [item[1] for item in batch]  # List of labels (may contain None for test)\n    \n    # Collate only the inputs (dictionaries of tensors)\n    collated_inputs = {}\n    for key in inputs[0].keys():  # Assume all inputs have the same keys\n        collated_inputs[key] = default_collate([d[key] for d in inputs])\n    \n    return collated_inputs, labels  # Return collated inputs and list of labels\n\n# Create test dataset using updated MultiSizeDataset\ntest_dataset = MultiSizeDataset(test_df, test_data_dir, is_train=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, collate_fn=custom_collate)","metadata":{"tags":[]},"outputs":[],"execution_count":23},{"id":"5756ee7c-fba0-4a58-a58f-3e37c9b0b512","cell_type":"markdown","source":"## Inference loops and Output generation","metadata":{}},{"id":"54b04108-5bfe-459e-8c20-db42ae227817","cell_type":"code","source":"# Load the fine-tuned ensemble model\nmodel = EnsembleModel(\n    dinov2_path='best_dinov2.pth',\n    hiera_path='best_hiera.pth',\n    effnet_path='best_effnet.pth'\n)\nmodel = load_checkpoint(model, 'fine_tuned_ensemble.pth', DEVICE)\nmodel.eval()\n\n# Perform inference on test images\npredictions = []\nmodel.to(DEVICE)\n\n# Modified prediction loop\nwith torch.no_grad():\n    for i, batch in enumerate(tqdm(test_dataloader, desc=\"Predicting\")):\n        inputs, labels = batch # Labels are -1 in this case\n        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n        \n        try:\n            outputs = model(inputs)\n            ensemble_output = torch.mean(torch.stack(outputs), dim=0)\n            predicted_labels = torch.argmax(ensemble_output, dim=-1).cpu().numpy()\n            \n            # Get corresponding IDs for the batch\n            start_idx = i * BATCH_SIZE\n            end_idx = start_idx + len(predicted_labels)\n            batch_ids = test_df.loc[start_idx:end_idx-1, 'id'].values\n            \n            for id_, label in zip(batch_ids, predicted_labels):\n                predictions.append((id_, label))\n                \n        except Exception as e:\n            print(f\"Error processing batch {i}: {e}\")\n            # Fallback for failed batch\n            start_idx = i * BATCH_SIZE\n            end_idx = start_idx + BATCH_SIZE\n            batch_ids = test_df.loc[start_idx:end_idx-1, 'id'].values\n            for id_ in batch_ids:\n                predictions.append((id_, 0))","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded vit_small_patch14_dinov2.lvd142m from best_dinov2.pth\n","Loaded hiera_small_224 from best_hiera.pth\n","Loaded tf_efficientnetv2_s.in21k_ft_in1k from best_effnet.pth\n","Loaded checkpoint from fine_tuned_ensemble.pth\n"]},{"name":"stderr","output_type":"stream","text":["Predicting: 100%|██████████| 693/693 [02:23<00:00,  4.83it/s]\n"]}],"execution_count":24},{"id":"e5a834c3-8221-46e5-90ba-79d1a8c785f8","cell_type":"markdown","source":"## Submission","metadata":{}},{"id":"d577fe9e-9311-42b8-ae1a-32f40a56f7b7","cell_type":"code","source":"# Create a DataFrame for submission\nsubmission_df = pd.DataFrame(predictions, columns=[\"id\", \"label\"])\n\n# Ensure all test IDs are included (handle missing predictions)\nmissing_ids = set(test_df['id']) - set(submission_df['id'])\nif missing_ids:\n    print(f\"Warning: {len(missing_ids)} images failed to process. Assigning default label 0.\")\n    missing_predictions = [(id_, 0) for id_ in missing_ids]\n    missing_df = pd.DataFrame(missing_predictions, columns=[\"id\", \"label\"])\n    submission_df = pd.concat([submission_df, missing_df], ignore_index=True)\n\n# Sort by ID to match original order\nsubmission_df = submission_df.sort_values(by='id').reset_index(drop=True)\n\n# Save to CSV for submission\nsubmission_csv_path = \"submission.csv\"\nsubmission_df.to_csv(submission_csv_path, index=False)\nprint(f\"Submission file saved at {submission_csv_path}\")\n\n# Display label distribution and sample\nprint(\"\\nLabel distribution:\")\nprint(submission_df['label'].value_counts())\nprint(\"\\nSubmission sample:\")\nprint(submission_df.head())","metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Submission file saved at submission.csv\n","\n","Label distribution:\n","label\n","1    2906\n","0    2634\n","Name: count, dtype: int64\n","\n","Submission sample:\n","                                                  id  label\n","0  test_data_v2/0016e1d72d404fe68074cc87cb30aa37.jpg      1\n","1  test_data_v2/002cbbdc87f0484db60ed0c261c53e7b.jpg      0\n","2  test_data_v2/0030624722ae447b98cd36e9c734fce4.jpg      1\n","3  test_data_v2/0065d47a13fc4608a55be23d02a0f523.jpg      0\n","4  test_data_v2/007b141831394b01bfe01fde0a250f6d.jpg      0\n"]}],"execution_count":25}]}